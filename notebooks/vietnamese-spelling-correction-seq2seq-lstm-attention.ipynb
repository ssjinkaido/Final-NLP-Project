{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport unidecode\nimport itertools\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport time\nfrom tqdm.notebook import tqdm\nimport nltk\nfrom nltk.tokenize import word_tokenize\nimport pandas as pd\nimport pickle\nimport string\nimport random\nfrom tqdm.notebook import tqdm\nfrom nltk.tokenize.treebank import TreebankWordDetokenizer\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nfrom torch.jit import script, trace\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\nimport matplotlib.ticker as ticker\nimport math\nimport gc\n# nltk.download('punkt')\n# sentence_tokenizer  =  nltk.data.load('tokenizers/punkt/english.pickle')","metadata":{"papermill":{"duration":2.863742,"end_time":"2022-01-02T08:20:36.826787","exception":false,"start_time":"2022-01-02T08:20:33.963045","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-16T09:30:36.414429Z","iopub.execute_input":"2022-01-16T09:30:36.415369Z","iopub.status.idle":"2022-01-16T09:30:36.423447Z","shell.execute_reply.started":"2022-01-16T09:30:36.415320Z","shell.execute_reply":"2022-01-16T09:30:36.422433Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nset_seed(seed = 1)","metadata":{"papermill":{"duration":0.042634,"end_time":"2022-01-02T08:20:36.905131","exception":false,"start_time":"2022-01-02T08:20:36.862497","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-16T09:30:36.425650Z","iopub.execute_input":"2022-01-16T09:30:36.426071Z","iopub.status.idle":"2022-01-16T09:30:36.433652Z","shell.execute_reply.started":"2022-01-16T09:30:36.426036Z","shell.execute_reply":"2022-01-16T09:30:36.432907Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"#borrow from https://github.com/hisiter97/Spelling_Correction_Vietnamese/blob/master/dataset/add_noise.py\nclass SynthesizeData(object):\n    \"\"\"\n    Uitils class to create artificial miss-spelled words\n    Args:\n        vocab_path: path to vocab file. Vocab file is expected to be a set of words, separate by ' ', no newline charactor.\n    \"\"\"\n\n    def __init__(self, vocab_path=\"\"):\n\n        # self.vocab = open(vocab_path, 'r', encoding = 'utf-8').read().split()\n        self.tokenizer = word_tokenize\n        self.word_couples = [['sương', 'xương'], ['sĩ', 'sỹ'], ['sẽ', 'sẻ'], ['sã', 'sả'], ['sả', 'xả'], ['sẽ', 'sẻ'],\n                             ['mùi', 'muồi'],\n                             ['chỉnh', 'chỉn'], ['sữa', 'sửa'], ['chuẩn', 'chẩn'], ['lẻ', 'lẽ'], ['chẳng', 'chẵng'],\n                             ['cổ', 'cỗ'],\n                             ['sát', 'xát'], ['cập', 'cặp'], ['truyện', 'chuyện'], ['xá', 'sá'], ['giả', 'dả'],\n                             ['đỡ', 'đở'],\n                             ['giữ', 'dữ'], ['giã', 'dã'], ['xảo', 'sảo'], ['kiểm', 'kiễm'], ['cuộc', 'cục'],\n                             ['dạng', 'dạn'],\n                             ['tản', 'tảng'], ['ngành', 'nghành'], ['nghề', 'ngề'], ['nổ', 'nỗ'], ['rảnh', 'rãnh'],\n                             ['sẵn', 'sẳn'],\n                             ['sáng', 'xán'], ['xuất', 'suất'], ['suôn', 'suông'], ['sử', 'xử'], ['sắc', 'xắc'],\n                             ['chữa', 'chửa'],\n                             ['thắn', 'thắng'], ['dỡ', 'dở'], ['trải', 'trãi'], ['trao', 'trau'], ['trung', 'chung'],\n                             ['thăm', 'tham'],\n                             ['sét', 'xét'], ['dục', 'giục'], ['tả', 'tã'], ['sông', 'xông'], ['sáo', 'xáo'],\n                             ['sang', 'xang'],\n                             ['ngã', 'ngả'], ['xuống', 'suống'], ['xuồng', 'suồng']]\n\n        self.vn_alphabet = ['a', 'ă', 'â', 'b', 'c', 'd', 'đ', 'e', 'ê', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'ô',\n                            'ơ', 'p', 'q', 'r', 's', 't', 'u', 'ư', 'v', 'x', 'y']\n        self.alphabet_len = len(self.vn_alphabet)\n        self.char_couples = [['i', 'y'], ['s', 'x'], ['gi', 'd'],\n                             ['ă', 'â'], ['ch', 'tr'], ['ng', 'n'],\n                             ['nh', 'n'], ['ngh', 'ng'], ['ục', 'uộc'], ['o', 'u'],\n                             ['ă', 'a'], ['o', 'ô'], ['ả', 'ã'], ['ổ', 'ỗ'], ['ủ', 'ũ'], ['ễ', 'ể'],\n                             ['e', 'ê'], ['à', 'ờ'], ['ằ', 'à'], ['ẩn', 'uẩn'], ['ẽ', 'ẻ'], ['ùi', 'uồi'], ['ă', 'â'],\n                             ['ở', 'ỡ'], ['ỹ', 'ỷ'], ['ỉ', 'ĩ'], ['ị', 'ỵ'],\n                             ['ấ', 'á'], ['n', 'l'], ['qu', 'w'], ['ph', 'f'], ['d', 'z'], ['c', 'k'], ['qu', 'q'],\n                             ['i', 'j'], ['gi', 'j'],\n                             ]\n\n        self.teencode_dict = {'mình': ['mk', 'mik', 'mjk'], 'vô': ['zô', 'zo', 'vo'], 'vậy': ['zậy', 'z', 'zay', 'za'],\n                              'phải': ['fải', 'fai', ], 'biết': ['bit', 'biet'],\n                              'rồi': ['rùi', 'ròi', 'r'], 'bây': ['bi', 'bay'], 'giờ': ['h', ],\n                              'không': ['k', 'ko', 'khong', 'hk', 'hong', 'hông', '0', 'kg', 'kh', ],\n                              'đi': ['di', 'dj', ], 'gì': ['j', ], 'em': ['e', ], 'được': ['dc', 'đc', ], 'tao': ['t'],\n                              'tôi': ['t'], 'chồng': ['ck'], 'vợ': ['vk']\n\n                              }\n\n        # self.typo={\"ă\":\"aw\",\"â\":\"aa\",\"á\":\"as\",\"à\":\"af\",\"ả\":\"ar\",\"ã\":\"ax\",\"ạ\":\"aj\",\"ắ\":\"aws\",\"ổ\":\"oor\",\"ỗ\":\"oox\",\"ộ\":\"ooj\",\"ơ\":\"ow\",\n        #           \"ằ\":\"awf\",\"ẳ\":\"awr\",\"ẵ\":\"awx\",\"ặ\":\"awj\",\"ó\":\"os\",\"ò\":\"of\",\"ỏ\":\"or\",\"õ\":\"ox\",\"ọ\":\"oj\",\"ô\":\"oo\",\"ố\":\"oos\",\"ồ\":\"oof\",\n        #           \"ớ\":\"ows\",\"ờ\":\"owf\",\"ở\":\"owr\",\"ỡ\":\"owx\",\"ợ\":\"owj\",\"é\":\"es\",\"è\":\"ef\",\"ẻ\":\"er\",\"ẽ\":\"ex\",\"ẹ\":\"ej\",\"ê\":\"ee\",\"ế\":\"ees\",\"ề\":\"eef\",\n        #           \"ể\":\"eer\",\"ễ\":\"eex\",\"ệ\":\"eej\",\"ú\":\"us\",\"ù\":\"uf\",\"ủ\":\"ur\",\"ũ\":\"ux\",\"ụ\":\"uj\",\"ư\":\"uw\",\"ứ\":\"uws\",\"ừ\":\"uwf\",\"ử\":\"uwr\",\"ữ\":\"uwx\",\n        #           \"ự\":\"uwj\",\"í\":\"is\",\"ì\":\"if\",\"ỉ\":\"ir\",\"ị\":\"ij\",\"ĩ\":\"ix\",\"ý\":\"ys\",\"ỳ\":\"yf\",\"ỷ\":\"yr\",\"ỵ\":\"yj\",\"đ\":\"dd\",\n        #           \"Ă\":\"Aw\",\"Â\":\"Aa\",\"Á\":\"As\",\"À\":\"Af\",\"Ả\":\"Ar\",\"Ã\":\"Ax\",\"Ạ\":\"Aj\",\"Ắ\":\"Aws\",\"Ổ\":\"Oor\",\"Ỗ\":\"Oox\",\"Ộ\":\"Ooj\",\"Ơ\":\"Ow\",\n        #           \"Ằ\":\"AWF\",\"Ẳ\":\"Awr\",\"Ẵ\":\"Awx\",\"Ặ\":\"Awj\",\"Ó\":\"Os\",\"Ò\":\"Of\",\"Ỏ\":\"Or\",\"Õ\":\"Ox\",\"Ọ\":\"Oj\",\"Ô\":\"Oo\",\"Ố\":\"Oos\",\"Ồ\":\"Oof\",\n        #           \"Ớ\":\"Ows\",\"Ờ\":\"Owf\",\"Ở\":\"Owr\",\"Ỡ\":\"Owx\",\"Ợ\":\"Owj\",\"É\":\"Es\",\"È\":\"Ef\",\"Ẻ\":\"Er\",\"Ẽ\":\"Ex\",\"Ẹ\":\"Ej\",\"Ê\":\"Ee\",\"Ế\":\"Ees\",\"Ề\":\"Eef\",\n        #           \"Ể\":\"Eer\",\"Ễ\":\"Eex\",\"Ệ\":\"Eej\",\"Ú\":\"Us\",\"Ù\":\"Uf\",\"Ủ\":\"Ur\",\"Ũ\":\"Ux\",\"Ụ\":\"Uj\",\"Ư\":\"Uw\",\"Ứ\":\"Uws\",\"Ừ\":\"Uwf\",\"Ử\":\"Uwr\",\"Ữ\":\"Uwx\",\n        #           \"Ự\":\"Uwj\",\"Í\":\"Is\",\"Ì\":\"If\",\"Ỉ\":\"Ir\",\"Ị\":\"Ij\",\"Ĩ\":\"Ix\",\"Ý\":\"Ys\",\"Ỳ\":\"Yf\",\"Ỷ\":\"Yr\",\"Ỵ\":\"Yj\",\"Đ\":\"Dd\"}\n        self.typo = {\"ă\": [\"aw\", \"a8\"], \"â\": [\"aa\", \"a6\"], \"á\": [\"as\", \"a1\"], \"à\": [\"af\", \"a2\"], \"ả\": [\"ar\", \"a3\"],\n                     \"ã\": [\"ax\", \"a4\"], \"ạ\": [\"aj\", \"a5\"], \"ắ\": [\"aws\", \"ă1\"], \"ổ\": [\"oor\", \"ô3\"], \"ỗ\": [\"oox\", \"ô4\"],\n                     \"ộ\": [\"ooj\", \"ô5\"], \"ơ\": [\"ow\", \"o7\"],\n                     \"ằ\": [\"awf\", \"ă2\"], \"ẳ\": [\"awr\", \"ă3\"], \"ẵ\": [\"awx\", \"ă4\"], \"ặ\": [\"awj\", \"ă5\"], \"ó\": [\"os\", \"o1\"],\n                     \"ò\": [\"of\", \"o2\"], \"ỏ\": [\"or\", \"o3\"], \"õ\": [\"ox\", \"o4\"], \"ọ\": [\"oj\", \"o5\"], \"ô\": [\"oo\", \"o6\"],\n                     \"ố\": [\"oos\", \"ô1\"], \"ồ\": [\"oof\", \"ô2\"],\n                     \"ớ\": [\"ows\", \"ơ1\"], \"ờ\": [\"owf\", \"ơ2\"], \"ở\": [\"owr\", \"ơ2\"], \"ỡ\": [\"owx\", \"ơ4\"], \"ợ\": [\"owj\", \"ơ5\"],\n                     \"é\": [\"es\", \"e1\"], \"è\": [\"ef\", \"e2\"], \"ẻ\": [\"er\", \"e3\"], \"ẽ\": [\"ex\", \"e4\"], \"ẹ\": [\"ej\", \"e5\"],\n                     \"ê\": [\"ee\", \"e6\"], \"ế\": [\"ees\", \"ê1\"], \"ề\": [\"eef\", \"ê2\"],\n                     \"ể\": [\"eer\", \"ê3\"], \"ễ\": [\"eex\", \"ê3\"], \"ệ\": [\"eej\", \"ê5\"], \"ú\": [\"us\", \"u1\"], \"ù\": [\"uf\", \"u2\"],\n                     \"ủ\": [\"ur\", \"u3\"], \"ũ\": [\"ux\", \"u4\"], \"ụ\": [\"uj\", \"u5\"], \"ư\": [\"uw\", \"u7\"], \"ứ\": [\"uws\", \"ư1\"],\n                     \"ừ\": [\"uwf\", \"ư2\"], \"ử\": [\"uwr\", \"ư3\"], \"ữ\": [\"uwx\", \"ư4\"],\n                     \"ự\": [\"uwj\", \"ư5\"], \"í\": [\"is\", \"i1\"], \"ì\": [\"if\", \"i2\"], \"ỉ\": [\"ir\", \"i3\"], \"ị\": [\"ij\", \"i5\"],\n                     \"ĩ\": [\"ix\", \"i4\"], \"ý\": [\"ys\", \"y1\"], \"ỳ\": [\"yf\", \"y2\"], \"ỷ\": [\"yr\", \"y3\"], \"ỵ\": [\"yj\", \"y5\"],\n                     \"đ\": [\"dd\", \"d9\"],\n                     \"Ă\": [\"Aw\", \"A8\"], \"Â\": [\"Aa\", \"A6\"], \"Á\": [\"As\", \"A1\"], \"À\": [\"Af\", \"A2\"], \"Ả\": [\"Ar\", \"A3\"],\n                     \"Ã\": [\"Ax\", \"A4\"], \"Ạ\": [\"Aj\", \"A5\"], \"Ắ\": [\"Aws\", \"Ă1\"], \"Ổ\": [\"Oor\", \"Ô3\"], \"Ỗ\": [\"Oox\", \"Ô4\"],\n                     \"Ộ\": [\"Ooj\", \"Ô5\"], \"Ơ\": [\"Ow\", \"O7\"],\n                     \"Ằ\": [\"AWF\", \"Ă2\"], \"Ẳ\": [\"Awr\", \"Ă3\"], \"Ẵ\": [\"Awx\", \"Ă4\"], \"Ặ\": [\"Awj\", \"Ă5\"], \"Ó\": [\"Os\", \"O1\"],\n                     \"Ò\": [\"Of\", \"O2\"], \"Ỏ\": [\"Or\", \"O3\"], \"Õ\": [\"Ox\", \"O4\"], \"Ọ\": [\"Oj\", \"O5\"], \"Ô\": [\"Oo\", \"O6\"],\n                     \"Ố\": [\"Oos\", \"Ô1\"], \"Ồ\": [\"Oof\", \"Ô2\"],\n                     \"Ớ\": [\"Ows\", \"Ơ1\"], \"Ờ\": [\"Owf\", \"Ơ2\"], \"Ở\": [\"Owr\", \"Ơ3\"], \"Ỡ\": [\"Owx\", \"Ơ4\"], \"Ợ\": [\"Owj\", \"Ơ5\"],\n                     \"É\": [\"Es\", \"E1\"], \"È\": [\"Ef\", \"E2\"], \"Ẻ\": [\"Er\", \"E3\"], \"Ẽ\": [\"Ex\", \"E4\"], \"Ẹ\": [\"Ej\", \"E5\"],\n                     \"Ê\": [\"Ee\", \"E6\"], \"Ế\": [\"Ees\", \"Ê1\"], \"Ề\": [\"Eef\", \"Ê2\"],\n                     \"Ể\": [\"Eer\", \"Ê3\"], \"Ễ\": [\"Eex\", \"Ê4\"], \"Ệ\": [\"Eej\", \"Ê5\"], \"Ú\": [\"Us\", \"U1\"], \"Ù\": [\"Uf\", \"U2\"],\n                     \"Ủ\": [\"Ur\", \"U3\"], \"Ũ\": [\"Ux\", \"U4\"], \"Ụ\": [\"Uj\", \"U5\"], \"Ư\": [\"Uw\", \"U7\"], \"Ứ\": [\"Uws\", \"Ư1\"],\n                     \"Ừ\": [\"Uwf\", \"Ư2\"], \"Ử\": [\"Uwr\", \"Ư3\"], \"Ữ\": [\"Uwx\", \"Ư4\"],\n                     \"Ự\": [\"Uwj\", \"Ư5\"], \"Í\": [\"Is\", \"I1\"], \"Ì\": [\"If\", \"I2\"], \"Ỉ\": [\"Ir\", \"I3\"], \"Ị\": [\"Ij\", \"I5\"],\n                     \"Ĩ\": [\"Ix\", \"I4\"], \"Ý\": [\"Ys\", \"Y1\"], \"Ỳ\": [\"Yf\", \"Y2\"], \"Ỷ\": [\"Yr\", \"Y3\"], \"Ỵ\": [\"Yj\", \"Y5\"],\n                     \"Đ\": [\"Dd\", \"D9\"]}\n        self.all_word_candidates = self.get_all_word_candidates(self.word_couples)\n        self.string_all_word_candidates = ' '.join(self.all_word_candidates)\n        self.all_char_candidates = self.get_all_char_candidates()\n        self.keyboardNeighbors = self.getKeyboardNeighbors()\n\n    def replace_teencode(self, word):\n        candidates = self.teencode_dict.get(word, None)\n        if candidates is not None:\n            chosen_one = 0\n            if len(candidates) > 1:\n                chosen_one = np.random.randint(0, len(candidates))\n            return candidates[chosen_one]\n\n    def getKeyboardNeighbors(self):\n        keyboardNeighbors = {}\n        keyboardNeighbors['a'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ă'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['â'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['á'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['à'] = \"aáàảãăắằẳẵâấầẩẫ\"\n        keyboardNeighbors['ả'] = \"aảã\"\n        keyboardNeighbors['ã'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ạ'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ắ'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ằ'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ẳ'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ặ'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ẵ'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ấ'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ầ'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ẩ'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ẫ'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ậ'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['b'] = \"bh\"\n        keyboardNeighbors['c'] = \"cgn\"\n        keyboardNeighbors['d'] = \"đctơở\"\n        keyboardNeighbors['đ'] = \"d\"\n        keyboardNeighbors['e'] = \"eéèẻẽẹêếềểễệbpg\"\n        keyboardNeighbors['é'] = \"eéèẻẽẹêếềểễệ\"\n        keyboardNeighbors['è'] = \"eéèẻẽẹêếềểễệ\"\n        keyboardNeighbors['ẻ'] = \"eéèẻẽẹêếềểễệ\"\n        keyboardNeighbors['ẽ'] = \"eéèẻẽẹêếềểễệ\"\n        keyboardNeighbors['ẹ'] = \"eéèẻẽẹêếềểễệ\"\n        keyboardNeighbors['ê'] = \"eéèẻẽẹêếềểễệá\"\n        keyboardNeighbors['ế'] = \"eéèẻẽẹêếềểễệố\"\n        keyboardNeighbors['ề'] = \"eéèẻẽẹêếềểễệ\"\n        keyboardNeighbors['ể'] = \"eéèẻẽẹêếềểễệôốồổỗộ\"\n        keyboardNeighbors['ễ'] = \"eéèẻẽẹêếềểễệ\"\n        keyboardNeighbors['ệ'] = \"eéèẻẽẹêếềểễệ\"\n        keyboardNeighbors['g'] = \"qgộ\"\n        keyboardNeighbors['h'] = \"h\"\n        keyboardNeighbors['i'] = \"iíìỉĩịat\"\n        keyboardNeighbors['í'] = \"iíìỉĩị\"\n        keyboardNeighbors['ì'] = \"iíìỉĩị\"\n        keyboardNeighbors['ỉ'] = \"iíìỉĩị\"\n        keyboardNeighbors['ĩ'] = \"iíìỉĩị\"\n        keyboardNeighbors['ị'] = \"iíìỉĩịhự\"\n        keyboardNeighbors['k'] = \"klh\"\n        keyboardNeighbors['l'] = \"ljidđ\"\n        keyboardNeighbors['m'] = \"mn\"\n        keyboardNeighbors['n'] = \"mnedư\"\n        keyboardNeighbors['o'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ó'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ò'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ỏ'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['õ'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ọ'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ô'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ố'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ồ'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ổ'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ộ'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ỗ'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ơ'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ớ'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ờ'] = \"oóòỏọõôốồổỗộơớờởợỡà\"\n        keyboardNeighbors['ở'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ợ'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ỡ'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        # keyboardNeighbors['p'] = \"op\"\n        # keyboardNeighbors['q'] = \"qọ\"\n        # keyboardNeighbors['r'] = \"rht\"\n        # keyboardNeighbors['s'] = \"s\"\n        # keyboardNeighbors['t'] = \"tp\"\n        keyboardNeighbors['u'] = \"uúùủũụưứừữửựhiaạt\"\n        keyboardNeighbors['ú'] = \"uúùủũụưứừữửự\"\n        keyboardNeighbors['ù'] = \"uúùủũụưứừữửự\"\n        keyboardNeighbors['ủ'] = \"uúùủũụưứừữửự\"\n        keyboardNeighbors['ũ'] = \"uúùủũụưứừữửự\"\n        keyboardNeighbors['ụ'] = \"uúùủũụưứừữửự\"\n        keyboardNeighbors['ư'] = \"uúùủũụưứừữửựg\"\n        keyboardNeighbors['ứ'] = \"uúùủũụưứừữửự\"\n        keyboardNeighbors['ừ'] = \"uúùủũụưứừữửự\"\n        keyboardNeighbors['ử'] = \"uúùủũụưứừữửự\"\n        keyboardNeighbors['ữ'] = \"uúùủũụưứừữửự\"\n        keyboardNeighbors['ự'] = \"uúùủũụưứừữửựg\"\n        keyboardNeighbors['v'] = \"v\"\n        keyboardNeighbors['x'] = \"x\"\n        keyboardNeighbors['y'] = \"yýỳỷỵỹụ\"\n        keyboardNeighbors['ý'] = \"yýỳỷỵỹ\"\n        keyboardNeighbors['ỳ'] = \"yýỳỷỵỹ\"\n        keyboardNeighbors['ỷ'] = \"yýỳỷỵỹ\"\n        keyboardNeighbors['ỵ'] = \"yýỳỷỵỹ\"\n        keyboardNeighbors['ỹ'] = \"yýỳỷỵỹ\"\n        # keyboardNeighbors['w'] = \"wv\"\n        # keyboardNeighbors['j'] = \"jli\"\n        # keyboardNeighbors['z'] = \"zs\"\n        # keyboardNeighbors['f'] = \"ft\"\n\n        return keyboardNeighbors\n\n    def replace_char_noaccent(self, text, onehot_label):\n\n        # find index noise\n        idx = np.random.randint(0, len(onehot_label))\n        prevent_loop = 0\n        while onehot_label[idx] == 1 or text[idx].isnumeric() or text[idx] in string.punctuation:\n            idx = np.random.randint(0, len(onehot_label))\n            prevent_loop += 1\n            if prevent_loop > 10:\n                return False, text, onehot_label\n\n        index_noise = idx\n        onehot_label[index_noise] = 1\n        word_noise = text[index_noise]\n        for id in range(0, len(word_noise)):\n            char = word_noise[id]\n\n            if char in self.keyboardNeighbors:\n                neighbors = self.keyboardNeighbors[char]\n                idx_neigh = np.random.randint(0, len(neighbors))\n                replaced = neighbors[idx_neigh]\n                word_noise = word_noise[: id] + replaced + word_noise[id + 1:]\n                text[index_noise] = word_noise\n                return True, text, onehot_label\n\n        return False, text, onehot_label\n\n    def replace_word_candidate(self, word):\n        \"\"\"\n        Return a homophone word of the input word.\n        \"\"\"\n        capital_flag = word[0].isupper()\n        word = word.lower()\n        if capital_flag and word in self.teencode_dict:\n            return self.replace_teencode(word).capitalize()\n        elif word in self.teencode_dict:\n            return self.replace_teencode(word)\n\n        for couple in self.word_couples:\n            for i in range(2):\n                if couple[i] == word:\n                    if i == 0:\n                        if capital_flag:\n                            return couple[1].capitalize()\n                        else:\n                            return couple[1]\n                    else:\n                        if capital_flag:\n                            return couple[0].capitalize()\n                        else:\n                            return couple[0]\n\n    def replace_char_candidate(self, char):\n        \"\"\"\n        return a homophone char/subword of the input char.\n        \"\"\"\n        for couple in self.char_couples:\n            for i in range(2):\n                if couple[i] == char:\n                    if i == 0:\n                        return couple[1]\n                    else:\n                        return couple[0]\n\n    def replace_char_candidate_typo(self, char):\n        \"\"\"\n        return a homophone char/subword of the input char.\n        \"\"\"\n        i = np.random.randint(0, 2)\n\n        return self.typo[char][i]\n\n    def get_all_char_candidates(self, ):\n\n        all_char_candidates = []\n        for couple in self.char_couples:\n            all_char_candidates.extend(couple)\n        return all_char_candidates\n\n    def get_all_word_candidates(self, word_couples):\n\n        all_word_candidates = []\n        for couple in self.word_couples:\n            all_word_candidates.extend(couple)\n        return all_word_candidates\n\n    def remove_diacritics(self, text, onehot_label):\n        \"\"\"\n        Replace word which has diacritics with the same word without diacritics\n        Args:\n            text: a list of word tokens\n            onehot_label: onehot array indicate position of word that has already modify, so this\n            function only choose the word that do not has onehot label == 1.\n        return: a list of word tokens has one word that its diacritics was removed,\n                a list of onehot label indicate the position of words that has been modified.\n        \"\"\"\n        idx = np.random.randint(0, len(onehot_label))\n        prevent_loop = 0\n        while onehot_label[idx] == 1 or text[idx] == unidecode.unidecode(text[idx]) or text[idx] in string.punctuation:\n            idx = np.random.randint(0, len(onehot_label))\n            prevent_loop += 1\n            if prevent_loop > 10:\n                return False, text, onehot_label\n\n        onehot_label[idx] = 1\n        text[idx] = unidecode.unidecode(text[idx])\n        return True, text, onehot_label\n\n    def replace_with_random_letter(self, text, onehot_label):\n        \"\"\"\n        Replace, add (or remove) a random letter in a random chosen word with a random letter\n        Args:\n            text: a list of word tokens\n            onehot_label: onehot array indicate position of word that has already modify, so this\n            function only choose the word that do not has onehot label == 1.\n        return: a list of word tokens has one word that has been modified,\n                a list of onehot label indicate the position of words that has been modified.\n        \"\"\"\n        idx = np.random.randint(0, len(onehot_label))\n        prevent_loop = 0\n        while onehot_label[idx] == 1 or text[idx].isnumeric() or text[idx] in string.punctuation:\n            idx = np.random.randint(0, len(onehot_label))\n            prevent_loop += 1\n            if prevent_loop > 10:\n                return False, text, onehot_label\n\n        # replace, add or remove? 0 is replace, 1 is add, 2 is remove\n        coin = np.random.choice([0, 1, 2])\n        if coin == 0:\n            chosen_letter = text[idx][np.random.randint(0, len(text[idx]))]\n            replaced = self.vn_alphabet[np.random.randint(0, self.alphabet_len)]\n            try:\n                text[idx] = re.sub(chosen_letter, replaced, text[idx])\n            except:\n                return False, text, onehot_label\n        elif coin == 1:\n            chosen_letter = text[idx][np.random.randint(0, len(text[idx]))]\n            replaced = chosen_letter + self.vn_alphabet[np.random.randint(0, self.alphabet_len)]\n            try:\n                text[idx] = re.sub(chosen_letter, replaced, text[idx])\n            except:\n                return False, text, onehot_label\n        else:\n            chosen_letter = text[idx][np.random.randint(0, len(text[idx]))]\n            try:\n                text[idx] = re.sub(chosen_letter, '', text[idx])\n            except:\n                return False, text, onehot_label\n\n        onehot_label[idx] = 1\n        return True, text, onehot_label\n\n    def replace_with_homophone_word(self, text, onehot_label):\n        \"\"\"\n        Replace a candidate word (if exist in the word_couple) with its homophone. if successful, return True, else False\n        Args:\n            text: a list of word tokens\n            onehot_label: onehot array indicate position of word that has already modify, so this\n            function only choose the word that do not has onehot label == 1.\n        return: True, text, onehot_label if successful replace, else False, text, onehot_label\n        \"\"\"\n        # account for the case that the word in the text is upper case but its lowercase match the candidates list\n        candidates = []\n        for i in range(len(text)):\n            if text[i].lower() in self.all_word_candidates or text[i].lower() in self.teencode_dict.keys():\n                candidates.append((i, text[i]))\n\n        if len(candidates) == 0:\n            return False, text, onehot_label\n\n        idx = np.random.randint(0, len(candidates))\n        prevent_loop = 0\n        while onehot_label[candidates[idx][0]] == 1:\n            idx = np.random.choice(np.arange(0, len(candidates)))\n            prevent_loop += 1\n            if prevent_loop > 5:\n                return False, text, onehot_label\n\n        text[candidates[idx][0]] = self.replace_word_candidate(candidates[idx][1])\n        onehot_label[candidates[idx][0]] = 1\n        return True, text, onehot_label\n\n    def replace_with_homophone_letter(self, text, onehot_label):\n        \"\"\"\n        Replace a subword/letter with its homophones\n        Args:\n            text: a list of word tokens\n            onehot_label: onehot array indicate position of word that has already modify, so this\n            function only choose the word that do not has onehot label == 1.\n        return: True, text, onehot_label if successful replace, else False, None, None\n        \"\"\"\n        candidates = []\n        for i in range(len(text)):\n            for char in self.all_char_candidates:\n                if re.search(char, text[i]) is not None:\n                    candidates.append((i, char))\n                    break\n\n        if len(candidates) == 0:\n\n            return False, text, onehot_label\n        else:\n            idx = np.random.randint(0, len(candidates))\n            prevent_loop = 0\n            while onehot_label[candidates[idx][0]] == 1:\n                idx = np.random.randint(0, len(candidates))\n                prevent_loop += 1\n                if prevent_loop > 5:\n                    return False, text, onehot_label\n\n            replaced = self.replace_char_candidate(candidates[idx][1])\n            text[candidates[idx][0]] = re.sub(candidates[idx][1], replaced, text[candidates[idx][0]])\n\n            onehot_label[candidates[idx][0]] = 1\n            return True, text, onehot_label\n\n    def replace_with_typo_letter(self, text, onehot_label):\n        \"\"\"\n        Replace a subword/letter with its homophones\n        Args:\n            text: a list of word tokens\n            onehot_label: onehot array indicate position of word that has already modify, so this\n            function only choose the word that do not has onehot label == 1.\n        return: True, text, onehot_label if successful replace, else False, None, None\n        \"\"\"\n        # find index noise\n        idx = np.random.randint(0, len(onehot_label))\n        prevent_loop = 0\n        while onehot_label[idx] == 1 or text[idx].isnumeric() or text[idx] in string.punctuation:\n            idx = np.random.randint(0, len(onehot_label))\n            prevent_loop += 1\n            if prevent_loop > 10:\n                return False, text, onehot_label\n\n        index_noise = idx\n        onehot_label[index_noise] = 1\n\n        word_noise = text[index_noise]\n        for j in range(0, len(word_noise)):\n            char = word_noise[j]\n\n            if char in self.typo:\n                replaced = self.replace_char_candidate_typo(char)\n                word_noise = word_noise[: j] + replaced + word_noise[j + 1:]\n                text[index_noise] = word_noise\n                return True, text, onehot_label\n        return True, text, onehot_label\n\n    def add_noise(self, sentence, percent_err=0.3, num_type_err=5):\n        tokens = self.tokenizer(sentence)\n        onehot_label = [0] * len(tokens)\n\n        num_wrong = int(np.ceil(percent_err * len(tokens)))\n        num_wrong = np.random.randint(1, num_wrong + 1)\n        if np.random.rand() < 0.05:\n            num_wrong = 0\n\n        for i in range(0, num_wrong):\n            err = np.random.randint(0, num_type_err + 1)\n\n            if err == 0:\n                _, tokens, onehot_label = self.replace_with_homophone_letter(tokens, onehot_label)\n            elif err == 1:\n                _, tokens, onehot_label = self.replace_with_typo_letter(tokens, onehot_label)\n            elif err == 2:\n                _, tokens, onehot_label = self.replace_with_homophone_word(tokens, onehot_label)\n            elif err == 3:\n                _, tokens, onehot_label = self.replace_with_random_letter(tokens, onehot_label)\n            elif err == 4:\n                _, tokens, onehot_label = self.remove_diacritics(tokens, onehot_label)\n            elif err == 5:\n                _, tokens, onehot_label = self.replace_char_noaccent(tokens, onehot_label)\n            else:\n                continue\n            # print(tokens)\n        return ' '.join(tokens)","metadata":{"papermill":{"duration":0.132506,"end_time":"2022-01-02T08:20:37.572707","exception":false,"start_time":"2022-01-02T08:20:37.440201","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-16T09:30:36.669987Z","iopub.execute_input":"2022-01-16T09:30:36.670226Z","iopub.status.idle":"2022-01-16T09:30:36.769394Z","shell.execute_reply.started":"2022-01-16T09:30:36.670198Z","shell.execute_reply":"2022-01-16T09:30:36.768394Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{"papermill":{"duration":0.033363,"end_time":"2022-01-02T08:20:37.711824","exception":false,"start_time":"2022-01-02T08:20:37.678461","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Params CONFIG\n","metadata":{"papermill":{"duration":0.076068,"end_time":"2022-01-02T08:20:37.89394","exception":false,"start_time":"2022-01-02T08:20:37.817872","status":"completed"},"tags":[]}},{"cell_type":"code","source":"print(\"here\")\nMAXLEN = 40\nNGRAM = 5\nalphabets = 'aAàÀảẢãÃáÁạẠăĂằẰẳẲẵẴắẮặẶâÂầẦẩẨẫẪấẤậẬ0bBcCdDđĐeEè1ÈẻẺẽẼéÉẹẸêÊềỀểỂễỄếẾệỆfFgGhHiIìÌỉỈ2ĩĨíÍịỊjJkKlLmMnNoO3òÒỏỎõÕóÓọỌôÔồ4ỒổỔỗỖốỐộỘơƠờỜ5ởỞỡỠớỚợỢpP6qQrRsStTuUùÙủỦ7ũŨúÚụỤưƯừỪửỬữỮứỨựỰvVw8WxXyYỳỲỷỶ9ỹỸýÝỵỴzZ!\"#$%&\\'()*+,-./:;<=>?@[\\]^_`{|}~ '\n# alphabets = 'aAàÀảẢãÃáÁạẠăĂằẰẳẲẵẴắẮặẶâÂầẦẩẨẫẪấẤậẬbBcCdDđĐeEèÈẻẺẽẼéÉẹẸêÊềỀểỂễỄếẾệỆfFgGhHiIìÌỉỈĩĨíÍịỊjJkKlLmMnNoOòÒỏỎõÕóÓọỌôÔồỒổỔỗỖốỐộỘơƠờỜởỞỡỠớỚợỢpPqQrRsStTuUùÙủỦũŨúÚụỤưƯừỪửỬữỮứỨựỰvVwWxXyYỳỲỷỶỹỸýÝỵỴzZ0123456789!\"#$%&\\'()*+,-./:;<=>?@[\\]^_`{|}~ ]'\nprint(len(alphabets))\nprint(alphabets[76])\nprint(alphabets)","metadata":{"papermill":{"duration":0.043573,"end_time":"2022-01-02T08:20:37.97152","exception":false,"start_time":"2022-01-02T08:20:37.927947","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-16T09:30:36.771201Z","iopub.execute_input":"2022-01-16T09:30:36.771556Z","iopub.status.idle":"2022-01-16T09:30:36.783825Z","shell.execute_reply.started":"2022-01-16T09:30:36.771504Z","shell.execute_reply":"2022-01-16T09:30:36.782948Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"class Vocab():\n    def __init__(self, chars):\n        self.pad = 0\n        self.go = 1\n        self.eos = 2\n\n        self.chars = chars\n       \n        \n        self.i2c = {i + 3: c for i, c in enumerate(chars)}\n        \n        self.c2i = {c: i + 3 for i, c in enumerate(chars)}\n        \n        self.i2c[0] = '<pad>'\n        self.i2c[1] = '<sos>'\n        self.i2c[2] = '<eos>'\n\n        \n        \n\n    def encode(self, chars):\n        return [self.go] + [self.c2i[c] for c in chars] + [self.eos]\n\n    def decode(self, ids):\n        first = 1 if self.go in ids else 0\n        last = ids.index(self.eos) if self.eos in ids else None\n        sent = ''.join([self.i2c[i] for i in ids[first:last]])\n        return sent\n\n    def __len__(self):\n        return len(self.c2i) + 3\n\n    def batch_decode(self, arr):\n        texts = [self.decode(ids) for ids in arr]\n        return texts\n\n    def __str__(self):\n        return self.chars\n    \nvocab = Vocab(alphabets)","metadata":{"papermill":{"duration":0.045519,"end_time":"2022-01-02T08:20:38.051257","exception":false,"start_time":"2022-01-02T08:20:38.005738","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-16T09:30:36.785304Z","iopub.execute_input":"2022-01-16T09:30:36.785684Z","iopub.status.idle":"2022-01-16T09:30:36.796541Z","shell.execute_reply.started":"2022-01-16T09:30:36.785648Z","shell.execute_reply":"2022-01-16T09:30:36.795726Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"list_ngram_target = np.load('../input/5gram-nlp-project/list_5gram_nonum_train.npy')\nlist_ngram_target  = list_ngram_target [:1000000]\nlist_ngram_valid_target = np.load('../input/5gram-nlp-project/list_5gram_nonum_valid.npy')\nlist_ngram_valid_target = list_ngram_valid_target[5000:15000]\nlist_ngram_train = np.load('../input/5gram-nlp-project/train_normal_captions.npy')\nlist_ngram_valid = np.load('../input/5gram-nlp-project/valid_normal_captions.npy')\nclass SpellingDataset(Dataset):\n    def __init__(self, list_ngram, list_ngram_target, vocab, maxlen):\n        self.list_ngram = list_ngram\n        self.list_ngram_target = list_ngram_target\n        self.vocab = vocab\n        self.max_len = maxlen\n    \n    def __getitem__(self, index):\n        train_text = self.list_ngram[index]\n        train_target = self.list_ngram_target[index]\n        \n        \n        train_text_encode = self.vocab.encode(train_text)\n        train_target_encode = self.vocab.encode(train_target)\n        \n        train_text_length = len(train_text_encode)\n        train_target_length = len(train_target_encode)\n        \n        if(train_text_length < self.max_len):\n            pad_length = self.max_len-train_text_length\n            train_text_encode = np.array(train_text_encode)\n            train_text_encode = np.concatenate((train_text_encode, np.zeros(pad_length)), axis = 0)\n            \n        elif(train_text_length>= self.max_len):\n            train_text_encode = train_text_encode[0:self.max_len]\n            train_text_encode = np.array(train_text_encode)\n            \n        if(train_target_length < self.max_len):\n            pad_length = self.max_len-train_target_length\n            train_target_encode = np.array(train_target_encode)\n            train_target_encode = np.concatenate((train_target_encode, np.zeros(pad_length)), axis = 0)\n            \n        elif(train_target_length>= self.max_len):\n            train_target_encode = train_target_encode[0:self.max_len]\n            train_target_encode = np.array(train_target_encode)      \n               \n        tensor_text = torch.from_numpy(train_text_encode)\n        tensor_target = torch.from_numpy(train_target_encode)\n        return tensor_text, tensor_target\n        \n        \n    \n    def __len__(self):\n        return len(self.list_ngram)\n\nds_train = SpellingDataset(list_ngram_train, list_ngram_target, vocab, MAXLEN)\nds_valid = SpellingDataset(list_ngram_valid, list_ngram_valid_target,vocab, MAXLEN)\n# ds_test = SpellingDataset(list_ngram_test, synthesizer, vocab, MAXLEN)\ntrain_loader = DataLoader(ds_train, batch_size = 512 , shuffle=True)\nval_loader = DataLoader(ds_valid, batch_size = 1)\n# test_loader = DataLoader(ds_test, batch_size = 200)\nprint(len(train_loader), len(val_loader))\ntext, target = next(iter(train_loader))\nvalid_text, valid_target = next(iter(train_loader))\nprint(text[0])\nprint(target[0])\nprint(\"Text: \", vocab.decode(np.squeeze(text[0].detach().numpy()).tolist()))\nprint(\"Target: \", vocab.decode(np.squeeze(target[0].detach().numpy()).tolist()))\nprint(\"Text: \", vocab.decode(np.squeeze(valid_text[0].detach().numpy()).tolist()))\nprint(\"Target: \", vocab.decode(np.squeeze(valid_target[0].detach().numpy()).tolist()))\nprint(text, target)\nprint(text.size(), target.size())\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:30:36.797737Z","iopub.execute_input":"2022-01-16T09:30:36.798264Z","iopub.status.idle":"2022-01-16T09:30:37.320303Z","shell.execute_reply.started":"2022-01-16T09:30:36.798226Z","shell.execute_reply":"2022-01-16T09:30:37.319421Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers):\n        super(Encoder, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        \n        # input shape: seq_length, batchsize, embedding_dim\n        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, bidirectional=True)\n        self.fc = nn.Linear(hidden_size * 2, hidden_size)\n#     def init_hidden(self):\n#         # This is what we'll initialise our hidden state as\n#         return (torch.zeros(self.num_layers, batch_size, self.hidden_size),\n#                 torch.zeros(self.num_layers, batch_size, self.hidden_size))\n    \n    def forward(self, x):\n        # x shape: (N, seq_length) where N is batch size\n        embedding = self.embedding(x)\n        # embedding shape: (N, seq_length, embedding_size)\n        \n#         embedding = torch.transpose(embedding, 0, 1)\n#         print(\"Embedding shape\", embedding.size())\n        outputs, (hidden_state, cell_state) = self.lstm(embedding)\n#         Embedding shape torch.Size([seq_len, batch_size, embedding_size])\n#         Outputs shape torch.Size([seq_len, batch_size, hidden_dim*2])\n#         Hidden state shape torch.Size([num_layer*2, batch_size, hidden_dim])\n#         Cell state shape torch.Size([num_layer*2, batch_size, hidden_dim])\n\n#         print(\"Outputs shape\", outputs.size())\n#         print(\"Hidden state shape\", hidden_state.size())\n#         print(\"Cell state shape\", cell_state.size())\n#         print(\"Outputs shape\", outputs.size())\n        \n        hidden_state = self.fc(torch.cat((hidden_state[0,:,:], hidden_state[1,:,:]), dim=1))\n        cell_state = self.fc(torch.cat((cell_state[1,:,:], cell_state[0,:,:]), dim=1))\n        outputs = outputs[:,:,:self.hidden_size] + outputs[:,:,:self.hidden_size]\n        \n        \n        \n#         print(\"Hidden state shape\", hidden_state.size())\n#         print(\"Cell state shape\", cell_state.size())\n#         print(\"Outputs shape\", outputs.size())\n#         Hidden state shape torch.Size([batch_size, hidden_dim])\n#         Cell state shape torch.Size([batch_size, hidden_dim])\n#         Outputs shape torch.Size([seq_length, N, hidden_dim*2])\n        # outputs shape: (seq_length, N, hidden_size)\n\n        return outputs, hidden_state, cell_state","metadata":{"papermill":{"duration":0.046372,"end_time":"2022-01-02T08:20:40.705698","exception":false,"start_time":"2022-01-02T08:20:40.659326","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-16T09:30:37.322853Z","iopub.execute_input":"2022-01-16T09:30:37.323133Z","iopub.status.idle":"2022-01-16T09:30:37.333481Z","shell.execute_reply.started":"2022-01-16T09:30:37.323095Z","shell.execute_reply":"2022-01-16T09:30:37.332273Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# class Attention(nn.Module):\n#     def __init__(self, enc_hidden_dim, dec_hidden_dim):\n#         super(Attention, self).__init__()\n#         self.attn = nn.Linear((enc_hidden_dim*2) + dec_hidden_dim, dec_hidden_dim)\n#         self.V = nn.Linear(dec_hidden_dim, 1, bias = False)\n    \n#     def forward(self, hidden, encoder_outputs):\n#         batch_size = encoder_outputs.shape[1]\n#         seq_len = encoder_outputs.shape[0]\n        \n#         hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)\n        \n#         encoder_outputs = encoder_outputs.permute(1, 0, 2)\n# #         print(\"Hidden after repeat\", hidden.size())\n# #         print(\"Encoder outputs now\", encoder_outputs.size())\n#         # [batch_size, seq_len, 2 * enc_hidden_dim] output\n#         # [batch_size, seq_len, dec_hidden_dim] hidden\n#         energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n#         # energy = [batch, seq_len, dec_hidden_dim]\n# #         print(\"Energy shape\", energy.size())\n        \n#         attention = self.V(energy).squeeze(dim = 2)\n#         # attention = [batch_size, seq_len]\n# #         print(\"Attention shape\", attention.size())\n        \n#         attention_weights = F.softmax(attention, dim = 1)\n        \n#         return attention_weights\n        ","metadata":{"papermill":{"duration":0.041638,"end_time":"2022-01-02T08:20:40.781342","exception":false,"start_time":"2022-01-02T08:20:40.739704","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-16T09:30:37.334678Z","iopub.execute_input":"2022-01-16T09:30:37.335091Z","iopub.status.idle":"2022-01-16T09:30:37.346443Z","shell.execute_reply.started":"2022-01-16T09:30:37.335033Z","shell.execute_reply":"2022-01-16T09:30:37.345624Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self, enc_hidden_dim, dec_hidden_dim):\n        super(Attention, self).__init__()\n        self.attn = nn.Linear((enc_hidden_dim *2) + dec_hidden_dim, dec_hidden_dim)\n        self.V = nn.Linear(dec_hidden_dim, 1, bias = False)\n    \n    def forward(self, hidden, encoder_outputs):\n        batch_size = encoder_outputs.shape[1]\n        seq_len = encoder_outputs.shape[0]\n        hidden = hidden.unsqueeze(0)\n#         hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)\n        \n#         encoder_outputs = encoder_outputs.permute(1, 0, 2)\n#         print(\"Hidden after repeat\", hidden.size())\n#         print(\"Encoder outputs now\", encoder_outputs.size())\n        # [batch_size, seq_len, 2 * enc_hidden_dim] output\n        # [batch_size, seq_len, dec_hidden_dim] hidden\n        energy  = torch.sum(hidden*encoder_outputs, dim=2)\n#         energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n        # energy = [batch, seq_len, dec_hidden_dim]\n        \n#         attention = self.V(energy).squeeze(dim = 2)\n        attention = energy.t()\n        # attention = [batch_size, seq_len]\n\n        attention_weights = F.softmax(attention, dim = 1)\n        \n        return attention_weights\n        ","metadata":{"papermill":{"duration":0.043881,"end_time":"2022-01-02T08:20:40.859909","exception":false,"start_time":"2022-01-02T08:20:40.816028","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-16T09:30:37.347765Z","iopub.execute_input":"2022-01-16T09:30:37.348149Z","iopub.status.idle":"2022-01-16T09:30:37.358132Z","shell.execute_reply.started":"2022-01-16T09:30:37.348113Z","shell.execute_reply":"2022-01-16T09:30:37.357370Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# class Decoder(nn.Module):\n#     def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, dropout, attention):\n#         super(Decoder, self).__init__()\n#         self.dropout = nn.Dropout(dropout)\n#         self.hidden_size = hidden_size\n#         self.num_layers = num_layers\n#         self.attention = attention\n        \n\n#         self.embedding = nn.Embedding(input_size, embedding_size)\n#         self.lstm = nn.LSTM(input_size = hidden_size*2  + embedding_size, hidden_size = hidden_size, num_layers = num_layers, dropout = dropout)\n        \n#         self.fc_out = nn.Linear((hidden_size*2) + hidden_size + embedding_size, output_size)\n        \n#         self.fc = nn.Linear(hidden_size, output_size)\n    \n#     def forward(self, x, hidden_state, cell_state, encoder_outputs):        \n#         x = x.unsqueeze(0)\n# #         print(\"X shape\", x.size())\n\n#         embedding = self.dropout(self.embedding(x))\n# #         print(\"Decoder hidden state shape\", hidden_state.size())\n# #         print(\"Decoder cell state shape\", cell_state.size())\n        \n# #         print(\"Embedding decoder shape\", embedding.size())\n#         # embedding shape (batch_size, 1, embedding_size)\n        \n#         a = self.attention(hidden_state, encoder_outputs)\n#         a = a.unsqueeze(1)\n        \n#         # a shape (batch_size, 1, seq_length)\n#         encoder_outputs = encoder_outputs.permute(1, 0, 2)\n#         weighted = torch.bmm(a, encoder_outputs)\n# #         print(\"Weighted shape\", weighted.size())\n#         # weighted shape (batch_size, 1, hidden_dim*2)\n#         weighted = weighted.permute(1, 0, 2)\n#         lstm_input = torch.cat((embedding, weighted), dim=2)\n# #         print(\"LSTM input shape\", lstm_input.size())\n#         # lstm_input shape = [batch_size, 1, enc_hidden_dim * 2 + embed_dim]\n        \n\n#         output, (hidden_state, cell_state) = self.lstm(lstm_input, (hidden_state.unsqueeze(0), cell_state.unsqueeze(0)))\n        \n#         embedding = embedding.squeeze(0)\n#         output = output.squeeze(0)\n#         weighted = weighted.squeeze(0)\n        \n# #         print(\"Embedding shape\", embedding.size())\n# #         print(\"Output shape\", output.size())\n# #         print(\"Weighted\", weighted.size())\n        \n#         prediction = self.fc_out(torch.cat((output, weighted, embedding), dim = 1))\n        \n# #         print(\"Predictions shape \", prediction.size())\n#         # (batch_size, output_dim(223))\n\n#         return prediction, hidden_state.squeeze(0), cell_state.squeeze(0)","metadata":{"papermill":{"duration":0.042546,"end_time":"2022-01-02T08:20:40.936703","exception":false,"start_time":"2022-01-02T08:20:40.894157","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-16T09:30:37.361318Z","iopub.execute_input":"2022-01-16T09:30:37.361605Z","iopub.status.idle":"2022-01-16T09:30:37.369776Z","shell.execute_reply.started":"2022-01-16T09:30:37.361579Z","shell.execute_reply":"2022-01-16T09:30:37.368882Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, attention):\n        super(Decoder, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.attention = attention\n        \n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.lstm = nn.LSTM(input_size = hidden_size  + embedding_size, hidden_size = hidden_size, num_layers = num_layers)\n        \n        self.fc_out = nn.Linear((hidden_size) + hidden_size + embedding_size, output_size)\n        \n        self.fc = nn.Linear(hidden_size, output_size)\n    \n    def forward(self, x, hidden_state, cell_state, encoder_outputs):        \n        x = x.unsqueeze(0)\n#         print(\"X shape\", x.size())\n#         print(\"Decoder hidden state shape\", hidden_state.size())\n#         print(\"Decoder cell state shape\", cell_state.size())\n        embedding = self.embedding(x)\n#         print(\"Embedding decoder shape\", embedding.size())\n        # embedding shape (batch_size, 1, embedding_size)\n        \n        a = self.attention(hidden_state, encoder_outputs)\n        a = a.unsqueeze(1)\n        \n#         print(\"Shape a\", a.size())\n        \n        # a shape (batch_size, 1, seq_length)\n        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n        weighted = torch.bmm(a, encoder_outputs)\n#         print(\"Weighted shape\", weighted.size())\n        # weighted shape (batch_size, 1, hidden_dim*2)\n        weighted = weighted.permute(1, 0, 2)\n        lstm_input = torch.cat((embedding, weighted), dim=2)\n#         print(\"LSTM input shape\", lstm_input.size())\n        # lstm_input shape = [batch_size, 1, enc_hidden_dim * 2 + embed_dim]\n        \n\n        output, (hidden_state, cell_state) = self.lstm(lstm_input, (hidden_state.unsqueeze(0), cell_state.unsqueeze(0)))\n        \n        embedding = embedding.squeeze(0)\n        output = output.squeeze(0)\n        weighted = weighted.squeeze(0)\n        \n#         print(\"Embedding shape\", embedding.size())\n#         print(\"Output shape\", output.size())\n#         print(\"Weighted\", weighted.size())\n        \n        prediction = self.fc_out(torch.cat((output, weighted, embedding), dim = 1))\n        \n#         print(\"Predictions shape \", prediction.size())\n        # (batch_size, output_dim(223))\n\n        return prediction, hidden_state.squeeze(0), cell_state.squeeze(0), a.squeeze(1)","metadata":{"papermill":{"duration":0.048663,"end_time":"2022-01-02T08:20:41.019572","exception":false,"start_time":"2022-01-02T08:20:40.970909","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-16T09:30:37.371367Z","iopub.execute_input":"2022-01-16T09:30:37.371679Z","iopub.status.idle":"2022-01-16T09:30:37.385459Z","shell.execute_reply.started":"2022-01-16T09:30:37.371593Z","shell.execute_reply":"2022-01-16T09:30:37.384447Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"class Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, target_vocab_size):\n        super(Seq2Seq, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.target_vocab_size = target_vocab_size\n\n    def forward(self, source, target, teacher_force_ratio=0.5):\n        batch_size = target.shape[1]\n        target_len = target.shape[0]\n        \n#         print(\"Shape of target\", target.size())\n        outputs = torch.zeros(target_len, batch_size,  self.target_vocab_size).to(device)\n\n        encoder_outputs, hidden_state, cell_state = self.encoder(source)\n\n        # Grab the first input to the Decoder which will be <SOS> token\n#         print(\"Target\", target.size)\n#         print(\"Target shape\", target.size())\n        x = target[0,:]\n#         print(\"Shape of x\", x.size())\n\n        for t in range(1, target_len):\n            # Use previous hidden, cell as context from encoder at start\n            output, hidden_state, cell_state, _ = self.decoder(x, hidden_state, cell_state, encoder_outputs)\n            # Store next output prediction\n#             output = output.unsqueeze(1)\n#             outputs[:, t, :] = output[:, 0, :]\n            outputs[t] = output\n            \n\n            # Get the best word the Decoder predicted (index in the vocabulary)\n#             best_guess = output.argmax(2).squeeze(1)\n            best_guess = output.argmax(1)\n            \n            teacher_force = torch.rand(1).item() < teacher_force_ratio\n\n            # With probability of teacher_force_ratio we take the actual next word\n            # otherwise we take the word that the Decoder predicted it to be.\n            # Teacher Forcing is used so that the model gets used to seeing\n            # similar inputs at training and testing time, if teacher forcing is 1\n            # then inputs at test time might be completely different than what the\n            # network is used to. This was a long comment.\n#             x = target[:,t] if teacher_force else best_guess\n            x = target[t, :] if teacher_force else best_guess\n#             print(\"X target shape\", x.size())\n    \n#         outputs = outputs.transpose(0, 1)\n        return outputs","metadata":{"papermill":{"duration":0.045412,"end_time":"2022-01-02T08:20:41.098942","exception":false,"start_time":"2022-01-02T08:20:41.05353","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-16T09:30:37.387057Z","iopub.execute_input":"2022-01-16T09:30:37.387357Z","iopub.status.idle":"2022-01-16T09:30:37.398240Z","shell.execute_reply.started":"2022-01-16T09:30:37.387314Z","shell.execute_reply":"2022-01-16T09:30:37.397527Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\ndef asMinutes(s):\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\n\ndef timeSince(since, percent):\n    now = time.time()\n    s = now - since\n    es = s / (percent)\n    rs = es - s\n    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n","metadata":{"papermill":{"duration":0.044737,"end_time":"2022-01-02T08:20:41.178237","exception":false,"start_time":"2022-01-02T08:20:41.1335","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-16T09:30:37.401203Z","iopub.execute_input":"2022-01-16T09:30:37.401564Z","iopub.status.idle":"2022-01-16T09:30:37.410281Z","shell.execute_reply.started":"2022-01-16T09:30:37.401520Z","shell.execute_reply":"2022-01-16T09:30:37.409297Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"class Trainer(object):\n    def __init__(self, device, config):\n        self.config = config\n        self.start_epoch = 1\n        self.device = device\n        self.attention = Attention(self.config.hidden_size, self.config.hidden_size)\n        self.encoder = Encoder(self.config.input_size_encoder, self.config.encoder_embedding_size, self.config.hidden_size, self.config.encoder_num_layers).to(self.device)\n\n        self.decoder = Decoder(self.config.input_size_decoder,self.config.decoder_embedding_size,self.config.hidden_size,self.config.output_size,\n                               self.config.decoder_num_layers,self.attention).to(self.device)\n\n        self.model = Seq2Seq(self.encoder, self.decoder, self.config.output_size).to(self.device)\n        self.encoder_optimizer = optim.Adam(self.encoder.parameters(), lr=self.config.lr)\n        self.decoder_optimizer = optim.Adam(self.decoder.parameters(), lr=self.config.lr * self.config.decoder_learning_ratio)\n        self.criterion = nn.CrossEntropyLoss().to(device)\n        \n        self.vocab = Vocab(alphabets)\n        self.base_dir = f'{config.folder}'\n        if not os.path.exists(self.base_dir):\n            os.makedirs(self.base_dir)\n        \n    def train_loop(self, train_loader, validation_loader, train_resume=False):\n        if train_resume==True:\n            self.load_model('../input/nlp-model/model_attention_20.pth')\n        for e in range(self.start_epoch, self.start_epoch + self.config.num_epochs):\n            t = time.time()\n            calc_loss = self.train_one_epoch(self.device, train_loader, e)\n#             self.scheduler.step()\n            self.config.epoch.append(e)\n            self.config.train_loss.append(calc_loss.avg)\n            print(f'Train. Epoch: {e}, Loss: {calc_loss.avg:.5f}, time: {(time.time() - t):.5f}')\n\n            if (e==self.start_epoch + self.config.num_epochs -1):\n                t = time.time()\n                predictions, targets, calc_loss = self.valid_one_epoch(self.device, validation_loader)\n                acc_valid = self.accuracy(predictions, targets)\n                self.config.valid_acc.append(acc_valid)\n                print(f'Val. Epoch: {e}, Loss: {calc_loss.avg:.5f}, Acc valid: {acc_valid:.5f}, time: {(time.time() - t):.5f}')\n                state = {'epoch': e, 'state_dict': self.model.state_dict(),'encoder_optimizer': self.encoder_optimizer.state_dict(),\n                         'decoder_optimizer': self.decoder_optimizer.state_dict(), 'train_loss': self.config.train_loss, 'valid_acc':self.config.valid_acc}\n                self.save_model(state, f'model_attention_{e}.pth')\n            \n            \n        \n        \n    def train_one_epoch(self, device, train_loader, epoch):\n        self.model.train()\n        calc_loss = AverageMeter()\n        start = end = time.time()\n        for batch_idx, (text, target) in tqdm(enumerate(train_loader)):\n            batch_size = text.size(0)\n            # make input and target shape (seq_length, batch_size)\n            \n            text = torch.transpose(text, 0, 1)\n            text = text.to(device, dtype=torch.int64)\n            target = torch.transpose(target, 0, 1)\n            target = target.to(device, dtype=torch.int64)\n            self.encoder_optimizer.zero_grad()\n            self.decoder_optimizer.zero_grad()\n\n            # Forward prop\n            \n            #trg = [trg len, batch size]\n            #output = [trg len, batch size, output dim]\n#             with autocast():\n            output = self.model(text, target)\n\n\n    #             print(\"Output\", output)\n\n    #             print(\"target \", target)\n\n    #             print(\"Output\",output.size())\n\n    #             print(\"Target\", target.size())\n            output = output.transpose(0, 1).contiguous()\n\n    #             output_dim = output.shape[-1]\n    #             print(\"Output shape\", output.size())\n    #             print(\"Target shape\", target.size())\n    #             print(\"Output after reshape\", output.size())\n    #             print(\"Target after reshape\", target.size())\n        #         output = output[1:].reshape(-1, output.shape[2])\n        #         target = target[1:].reshape(-1)\n            output = output.reshape(-1, output.shape[2])\n            target = target.transpose(0, 1).reshape(-1)\n\n    #             print(\"Output\", output)\n\n    #             print(\"target \", target)\n\n    #             print(\"Output\",output.size())\n\n    #             print(\"Target\", target.size())\n\n                # output shape (batch_size, seq_length, vocab_length)\n                # target shape (batch_size, seq_length)           \n\n            loss = self.criterion(output, target)\n            loss_value = loss.item()\n            # Back prop\n            loss.backward()\n#             self.scaler.scale(loss).backward()\n\n            # Clip to avoid exploding gradient issues, makes sure grads are\n            # within a healthy range\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1)\n#             clip_gradient(self.encoder_optimizer, 5)\n#             clip_gradient(self.decoder_optimizer, 5)\n            # Adjust model weights\n            self.encoder_optimizer.step()\n            self.decoder_optimizer.step()\n            calc_loss.update(loss_value, batch_size)\n            if batch_idx %400==100:\n                print('Epoch: [{0}][{1}/{2}] '\n                  'Elapsed {remain:s} '\n                  .format(\n                   epoch, batch_idx, len(train_loader),\n                   remain=timeSince(start, float(batch_idx+1)/len(train_loader)),\n                   #lr=scheduler.get_lr()[0],\n                   ))\n\n        torch.cuda.empty_cache()\n        gc.collect()\n            \n        return calc_loss\n    \n    def valid_one_epoch(self, device, val_loader):\n        self.model.eval()\n        calc_loss = AverageMeter()\n        predictions = []\n        targets  =[]\n        with torch.no_grad():\n            for batch_idx, (text, targett) in tqdm(enumerate(val_loader)):\n                batch_size = text.size(0)\n                text = torch.transpose(text, 0, 1)\n                text = text.to(device, dtype=torch.int64)\n                prediction = self.correct_sentence(self.model, text, device)\n                target = self.vocab.decode(np.squeeze(targett.detach().numpy()).tolist())\n                predictions.extend(prediction)\n                targets.append(target)\n                targett = torch.transpose(targett, 0, 1)\n\n    #             text = text.to(device, dtype=torch.int64)\n                targett = targett.to(device, dtype=torch.int64)\n                output = self.model(text, targett, 0)\n\n    #             print(\"Output\", output.size())\n                output = output.transpose(0, 1).contiguous()\n                output = output.flatten(0, 1)\n                targett = targett.flatten()\n    #             print(\"Output\", output.size())\n    #             print(\"Target\", target.size())\n\n\n    #             output = output[1:].reshape(-1, output.shape[2])\n    #             target = target[1:].reshape(-1)\n\n                loss = self.criterion(output, targett)\n                loss_value = loss.item()\n\n                calc_loss.update(loss_value, batch_size)\n\n        \n        return predictions, targets, calc_loss\n\n    def correct_sentence(self, model, text, device):\n        self.model.eval()\n        with torch.no_grad():\n            encoder_outputs, hidden_state, cell_state = self.model.encoder(text)\n            translated_sentence = [[1]]\n            for _ in range(40):\n                target_input = torch.LongTensor(translated_sentence).to(device)\n                output, hidden_state, cell_state,_ = self.model.decoder(target_input[-1], hidden_state, cell_state, encoder_outputs)\n                output = output.unsqueeze(1)\n    #             print(\"Output\", output.size())\n                output = torch.softmax(output, dim=-1)\n                output = output.to('cpu')\n    #             print(\"Output\", output.size())\n                values, indices = torch.topk(output, 1)\n    #             print(\"Indices\", indices)\n                indices = indices[:, -1, 0]\n    #             print(\"Indices\", indices)\n                indices = indices.tolist()\n    #             output = output.argmax(1).tolist()\n                translated_sentence.append(indices)\n    #             print(\"Translated sentence\", translated_sentence)\n                if(indices[0]==2):\n                    break\n            translated_sentence = np.asarray(translated_sentence).T\n            translated_sentence = translated_sentence.tolist()\n    #         translated_sentence = np.squeeze(np.array(translated_sentence)).tolist()\n        return [self.vocab.decode(i) for i in translated_sentence]\n    \n    def accuracy(self, predictions, targets):\n        n = len(predictions)\n        acc = 0\n    #     print(len(predictions), len(targets))\n        for i in range(len(predictions)):\n\n    #         print(f\"Predictions {predictions}\")\n    #         print(f\"Targets {targets}\")\n            if predictions[i]==targets[i]:\n                acc+=1\n        return acc/n\n    \n    \n    def save_model(self, state, path):\n        torch.save(state, path)\n    \n    def load_model(self, path):\n        checkpoint = torch.load(path)\n        self.start_epoch = checkpoint['epoch']+1\n        self.model.load_state_dict(checkpoint['state_dict'])\n        self.encoder_optimizer.load_state_dict(checkpoint['encoder_optimizer'])\n        self.decoder_optimizer.load_state_dict(checkpoint['decoder_optimizer'])\n        self.config.train_loss = checkpoint['train_loss']\n        self.config.valid_acc = checkpoint['valid_acc']","metadata":{"papermill":{"duration":0.17341,"end_time":"2022-01-02T08:20:41.385179","exception":false,"start_time":"2022-01-02T08:20:41.211769","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-16T09:30:37.411881Z","iopub.execute_input":"2022-01-16T09:30:37.412132Z","iopub.status.idle":"2022-01-16T09:30:37.450108Z","shell.execute_reply.started":"2022-01-16T09:30:37.412100Z","shell.execute_reply":"2022-01-16T09:30:37.449335Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"class GlobalParametersTrain:\n    lr = 0.0005\n    # 0.0005 is the best\n    SRC_VOCAB_SIZE = 232\n    TARGET_VOCAB_SIZE = 232\n    num_epochs = 10\n    max_len = 40\n    PAD_IDX = 0\n    SOS_IDX = 1\n    EOS_IDX = 2 \n    encoder_embedding_size = 128\n    decoder_embedding_size = 128\n    input_size_encoder = len(alphabets)+3\n    input_size_decoder = len(alphabets)+3\n    output_size = len(alphabets)+3\n    hidden_size = 1024  # Needs to be the same for both RNN's\n    encoder_num_layers = 1\n    decoder_num_layers = 1\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    decoder_learning_ratio = 5.0\n\n    train_loss = []\n    valid_loss = []\n    valid_acc = []\n    epoch = []\n    \n    folder = './model'","metadata":{"papermill":{"duration":0.043476,"end_time":"2022-01-02T08:20:41.462773","exception":false,"start_time":"2022-01-02T08:20:41.419297","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-16T09:30:37.451363Z","iopub.execute_input":"2022-01-16T09:30:37.451618Z","iopub.status.idle":"2022-01-16T09:30:37.461409Z","shell.execute_reply.started":"2022-01-16T09:30:37.451585Z","shell.execute_reply":"2022-01-16T09:30:37.460544Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# Model hyperparameters\nconfig = GlobalParametersTrain()\n# model = Seq2SeqTransformer(SRC_VOCAB_SIZE, TARGET_VOCAB_SIZE, PAD_IDX, PAD_IDX, pes, device).to(device)\n# model = TransformerModel(config.SRC_VOCAB_SIZE, config.model_dim, config.feed_forward_dim, config.TARGET_VOCAB_SIZE, config.num_layers,\n#                             config.max_len, config.PAD_IDX, config.SOS_IDX, config.EOS_IDX).to(config.device)\ntraining = Trainer(device=config.device, config=config)\ntraining.train_loop(train_loader, val_loader, False)\n","metadata":{"papermill":{"duration":22499.904665,"end_time":"2022-01-02T14:35:41.400611","exception":false,"start_time":"2022-01-02T08:20:41.495946","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-16T09:30:37.462926Z","iopub.execute_input":"2022-01-16T09:30:37.463449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.044216,"end_time":"2022-01-02T14:35:41.871628","exception":false,"start_time":"2022-01-02T14:35:41.827412","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.044366,"end_time":"2022-01-02T14:35:42.972689","exception":false,"start_time":"2022-01-02T14:35:42.928323","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.044419,"end_time":"2022-01-02T14:35:43.061731","exception":false,"start_time":"2022-01-02T14:35:43.017312","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.046607,"end_time":"2022-01-02T14:35:43.152965","exception":false,"start_time":"2022-01-02T14:35:43.106358","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.047854,"end_time":"2022-01-02T14:35:43.249238","exception":false,"start_time":"2022-01-02T14:35:43.201384","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}