{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import itertools\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport time\nfrom tqdm.notebook import tqdm\nimport pandas as pd\nimport nltk\nfrom nltk.tokenize import word_tokenize\nimport unidecode\nimport codecs\nimport pickle\nimport string\nimport random\nfrom tqdm.notebook import tqdm\nfrom transformers import AdamW\nfrom nltk.tokenize.treebank import TreebankWordDetokenizer\nfrom torchtext.data.metrics import bleu_score\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, OneCycleLR, CosineAnnealingLR\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\nimport numpy as np\nfrom torch.jit import script, trace\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\nimport math\n# nltk.download('punkt')\n# sentence_tokenizer  =  nltk.data.load('tokenizers/punkt/english.pickle')","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:31:29.002106Z","iopub.execute_input":"2022-01-16T09:31:29.002738Z","iopub.status.idle":"2022-01-16T09:31:36.626012Z","shell.execute_reply.started":"2022-01-16T09:31:29.002620Z","shell.execute_reply":"2022-01-16T09:31:36.625246Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nset_seed(seed = 38)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:31:36.627552Z","iopub.execute_input":"2022-01-16T09:31:36.627773Z","iopub.status.idle":"2022-01-16T09:31:36.635551Z","shell.execute_reply.started":"2022-01-16T09:31:36.627747Z","shell.execute_reply":"2022-01-16T09:31:36.634522Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#borrow from https://github.com/hisiter97/Spelling_Correction_Vietnamese/blob/master/dataset/add_noise.py\nclass SynthesizeData(object):\n    \"\"\"\n    Uitils class to create artificial miss-spelled words\n    Args:\n        vocab_path: path to vocab file. Vocab file is expected to be a set of words, separate by ' ', no newline charactor.\n    \"\"\"\n\n    def __init__(self, vocab_path=\"\"):\n\n        # self.vocab = open(vocab_path, 'r', encoding = 'utf-8').read().split()\n        self.tokenizer = word_tokenize\n        self.word_couples = [['sương', 'xương'], ['sĩ', 'sỹ'], ['sẽ', 'sẻ'], ['sã', 'sả'], ['sả', 'xả'], ['sẽ', 'sẻ'],\n                             ['mùi', 'muồi'],\n                             ['chỉnh', 'chỉn'], ['sữa', 'sửa'], ['chuẩn', 'chẩn'], ['lẻ', 'lẽ'], ['chẳng', 'chẵng'],\n                             ['cổ', 'cỗ'],\n                             ['sát', 'xát'], ['cập', 'cặp'], ['truyện', 'chuyện'], ['xá', 'sá'], ['giả', 'dả'],\n                             ['đỡ', 'đở'],\n                             ['giữ', 'dữ'], ['giã', 'dã'], ['xảo', 'sảo'], ['kiểm', 'kiễm'], ['cuộc', 'cục'],\n                             ['dạng', 'dạn'],\n                             ['tản', 'tảng'], ['ngành', 'nghành'], ['nghề', 'ngề'], ['nổ', 'nỗ'], ['rảnh', 'rãnh'],\n                             ['sẵn', 'sẳn'],\n                             ['sáng', 'xán'], ['xuất', 'suất'], ['suôn', 'suông'], ['sử', 'xử'], ['sắc', 'xắc'],\n                             ['chữa', 'chửa'],\n                             ['thắn', 'thắng'], ['dỡ', 'dở'], ['trải', 'trãi'], ['trao', 'trau'], ['trung', 'chung'],\n                             ['thăm', 'tham'],\n                             ['sét', 'xét'], ['dục', 'giục'], ['tả', 'tã'], ['sông', 'xông'], ['sáo', 'xáo'],\n                             ['sang', 'xang'],\n                             ['ngã', 'ngả'], ['xuống', 'suống'], ['xuồng', 'suồng']]\n\n        self.vn_alphabet = ['a', 'ă', 'â', 'b', 'c', 'd', 'đ', 'e', 'ê', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'ô',\n                            'ơ', 'p', 'q', 'r', 's', 't', 'u', 'ư', 'v', 'x', 'y']\n        self.alphabet_len = len(self.vn_alphabet)\n        self.char_couples = [['i', 'y'], ['s', 'x'], ['gi', 'd'],\n                             ['ă', 'â'], ['ch', 'tr'], ['ng', 'n'],\n                             ['nh', 'n'], ['ngh', 'ng'], ['ục', 'uộc'], ['o', 'u'],\n                             ['ă', 'a'], ['o', 'ô'], ['ả', 'ã'], ['ổ', 'ỗ'], ['ủ', 'ũ'], ['ễ', 'ể'],\n                             ['e', 'ê'], ['à', 'ờ'], ['ằ', 'à'], ['ẩn', 'uẩn'], ['ẽ', 'ẻ'], ['ùi', 'uồi'], ['ă', 'â'],\n                             ['ở', 'ỡ'], ['ỹ', 'ỷ'], ['ỉ', 'ĩ'], ['ị', 'ỵ'],\n                             ['ấ', 'á'], ['n', 'l'], ['qu', 'w'], ['ph', 'f'], ['d', 'z'], ['c', 'k'], ['qu', 'q'],\n                             ['i', 'j'], ['gi', 'j'],\n                             ]\n\n        self.teencode_dict = {'mình': ['mk', 'mik', 'mjk'], 'vô': ['zô', 'zo', 'vo'], 'vậy': ['zậy', 'z', 'zay', 'za'],\n                              'phải': ['fải', 'fai', ], 'biết': ['bit', 'biet'],\n                              'rồi': ['rùi', 'ròi', 'r'], 'bây': ['bi', 'bay'], 'giờ': ['h', ],\n                              'không': ['k', 'ko', 'khong', 'hk', 'hong', 'hông', '0', 'kg', 'kh', ],\n                              'đi': ['di', 'dj', ], 'gì': ['j', ], 'em': ['e', ], 'được': ['dc', 'đc', ], 'tao': ['t'],\n                              'tôi': ['t'], 'chồng': ['ck'], 'vợ': ['vk']\n\n                              }\n\n        # self.typo={\"ă\":\"aw\",\"â\":\"aa\",\"á\":\"as\",\"à\":\"af\",\"ả\":\"ar\",\"ã\":\"ax\",\"ạ\":\"aj\",\"ắ\":\"aws\",\"ổ\":\"oor\",\"ỗ\":\"oox\",\"ộ\":\"ooj\",\"ơ\":\"ow\",\n        #           \"ằ\":\"awf\",\"ẳ\":\"awr\",\"ẵ\":\"awx\",\"ặ\":\"awj\",\"ó\":\"os\",\"ò\":\"of\",\"ỏ\":\"or\",\"õ\":\"ox\",\"ọ\":\"oj\",\"ô\":\"oo\",\"ố\":\"oos\",\"ồ\":\"oof\",\n        #           \"ớ\":\"ows\",\"ờ\":\"owf\",\"ở\":\"owr\",\"ỡ\":\"owx\",\"ợ\":\"owj\",\"é\":\"es\",\"è\":\"ef\",\"ẻ\":\"er\",\"ẽ\":\"ex\",\"ẹ\":\"ej\",\"ê\":\"ee\",\"ế\":\"ees\",\"ề\":\"eef\",\n        #           \"ể\":\"eer\",\"ễ\":\"eex\",\"ệ\":\"eej\",\"ú\":\"us\",\"ù\":\"uf\",\"ủ\":\"ur\",\"ũ\":\"ux\",\"ụ\":\"uj\",\"ư\":\"uw\",\"ứ\":\"uws\",\"ừ\":\"uwf\",\"ử\":\"uwr\",\"ữ\":\"uwx\",\n        #           \"ự\":\"uwj\",\"í\":\"is\",\"ì\":\"if\",\"ỉ\":\"ir\",\"ị\":\"ij\",\"ĩ\":\"ix\",\"ý\":\"ys\",\"ỳ\":\"yf\",\"ỷ\":\"yr\",\"ỵ\":\"yj\",\"đ\":\"dd\",\n        #           \"Ă\":\"Aw\",\"Â\":\"Aa\",\"Á\":\"As\",\"À\":\"Af\",\"Ả\":\"Ar\",\"Ã\":\"Ax\",\"Ạ\":\"Aj\",\"Ắ\":\"Aws\",\"Ổ\":\"Oor\",\"Ỗ\":\"Oox\",\"Ộ\":\"Ooj\",\"Ơ\":\"Ow\",\n        #           \"Ằ\":\"AWF\",\"Ẳ\":\"Awr\",\"Ẵ\":\"Awx\",\"Ặ\":\"Awj\",\"Ó\":\"Os\",\"Ò\":\"Of\",\"Ỏ\":\"Or\",\"Õ\":\"Ox\",\"Ọ\":\"Oj\",\"Ô\":\"Oo\",\"Ố\":\"Oos\",\"Ồ\":\"Oof\",\n        #           \"Ớ\":\"Ows\",\"Ờ\":\"Owf\",\"Ở\":\"Owr\",\"Ỡ\":\"Owx\",\"Ợ\":\"Owj\",\"É\":\"Es\",\"È\":\"Ef\",\"Ẻ\":\"Er\",\"Ẽ\":\"Ex\",\"Ẹ\":\"Ej\",\"Ê\":\"Ee\",\"Ế\":\"Ees\",\"Ề\":\"Eef\",\n        #           \"Ể\":\"Eer\",\"Ễ\":\"Eex\",\"Ệ\":\"Eej\",\"Ú\":\"Us\",\"Ù\":\"Uf\",\"Ủ\":\"Ur\",\"Ũ\":\"Ux\",\"Ụ\":\"Uj\",\"Ư\":\"Uw\",\"Ứ\":\"Uws\",\"Ừ\":\"Uwf\",\"Ử\":\"Uwr\",\"Ữ\":\"Uwx\",\n        #           \"Ự\":\"Uwj\",\"Í\":\"Is\",\"Ì\":\"If\",\"Ỉ\":\"Ir\",\"Ị\":\"Ij\",\"Ĩ\":\"Ix\",\"Ý\":\"Ys\",\"Ỳ\":\"Yf\",\"Ỷ\":\"Yr\",\"Ỵ\":\"Yj\",\"Đ\":\"Dd\"}\n        self.typo = {\"ă\": [\"aw\", \"a8\"], \"â\": [\"aa\", \"a6\"], \"á\": [\"as\", \"a1\"], \"à\": [\"af\", \"a2\"], \"ả\": [\"ar\", \"a3\"],\n                     \"ã\": [\"ax\", \"a4\"], \"ạ\": [\"aj\", \"a5\"], \"ắ\": [\"aws\", \"ă1\"], \"ổ\": [\"oor\", \"ô3\"], \"ỗ\": [\"oox\", \"ô4\"],\n                     \"ộ\": [\"ooj\", \"ô5\"], \"ơ\": [\"ow\", \"o7\"],\n                     \"ằ\": [\"awf\", \"ă2\"], \"ẳ\": [\"awr\", \"ă3\"], \"ẵ\": [\"awx\", \"ă4\"], \"ặ\": [\"awj\", \"ă5\"], \"ó\": [\"os\", \"o1\"],\n                     \"ò\": [\"of\", \"o2\"], \"ỏ\": [\"or\", \"o3\"], \"õ\": [\"ox\", \"o4\"], \"ọ\": [\"oj\", \"o5\"], \"ô\": [\"oo\", \"o6\"],\n                     \"ố\": [\"oos\", \"ô1\"], \"ồ\": [\"oof\", \"ô2\"],\n                     \"ớ\": [\"ows\", \"ơ1\"], \"ờ\": [\"owf\", \"ơ2\"], \"ở\": [\"owr\", \"ơ2\"], \"ỡ\": [\"owx\", \"ơ4\"], \"ợ\": [\"owj\", \"ơ5\"],\n                     \"é\": [\"es\", \"e1\"], \"è\": [\"ef\", \"e2\"], \"ẻ\": [\"er\", \"e3\"], \"ẽ\": [\"ex\", \"e4\"], \"ẹ\": [\"ej\", \"e5\"],\n                     \"ê\": [\"ee\", \"e6\"], \"ế\": [\"ees\", \"ê1\"], \"ề\": [\"eef\", \"ê2\"],\n                     \"ể\": [\"eer\", \"ê3\"], \"ễ\": [\"eex\", \"ê3\"], \"ệ\": [\"eej\", \"ê5\"], \"ú\": [\"us\", \"u1\"], \"ù\": [\"uf\", \"u2\"],\n                     \"ủ\": [\"ur\", \"u3\"], \"ũ\": [\"ux\", \"u4\"], \"ụ\": [\"uj\", \"u5\"], \"ư\": [\"uw\", \"u7\"], \"ứ\": [\"uws\", \"ư1\"],\n                     \"ừ\": [\"uwf\", \"ư2\"], \"ử\": [\"uwr\", \"ư3\"], \"ữ\": [\"uwx\", \"ư4\"],\n                     \"ự\": [\"uwj\", \"ư5\"], \"í\": [\"is\", \"i1\"], \"ì\": [\"if\", \"i2\"], \"ỉ\": [\"ir\", \"i3\"], \"ị\": [\"ij\", \"i5\"],\n                     \"ĩ\": [\"ix\", \"i4\"], \"ý\": [\"ys\", \"y1\"], \"ỳ\": [\"yf\", \"y2\"], \"ỷ\": [\"yr\", \"y3\"], \"ỵ\": [\"yj\", \"y5\"],\n                     \"đ\": [\"dd\", \"d9\"],\n                     \"Ă\": [\"Aw\", \"A8\"], \"Â\": [\"Aa\", \"A6\"], \"Á\": [\"As\", \"A1\"], \"À\": [\"Af\", \"A2\"], \"Ả\": [\"Ar\", \"A3\"],\n                     \"Ã\": [\"Ax\", \"A4\"], \"Ạ\": [\"Aj\", \"A5\"], \"Ắ\": [\"Aws\", \"Ă1\"], \"Ổ\": [\"Oor\", \"Ô3\"], \"Ỗ\": [\"Oox\", \"Ô4\"],\n                     \"Ộ\": [\"Ooj\", \"Ô5\"], \"Ơ\": [\"Ow\", \"O7\"],\n                     \"Ằ\": [\"AWF\", \"Ă2\"], \"Ẳ\": [\"Awr\", \"Ă3\"], \"Ẵ\": [\"Awx\", \"Ă4\"], \"Ặ\": [\"Awj\", \"Ă5\"], \"Ó\": [\"Os\", \"O1\"],\n                     \"Ò\": [\"Of\", \"O2\"], \"Ỏ\": [\"Or\", \"O3\"], \"Õ\": [\"Ox\", \"O4\"], \"Ọ\": [\"Oj\", \"O5\"], \"Ô\": [\"Oo\", \"O6\"],\n                     \"Ố\": [\"Oos\", \"Ô1\"], \"Ồ\": [\"Oof\", \"Ô2\"],\n                     \"Ớ\": [\"Ows\", \"Ơ1\"], \"Ờ\": [\"Owf\", \"Ơ2\"], \"Ở\": [\"Owr\", \"Ơ3\"], \"Ỡ\": [\"Owx\", \"Ơ4\"], \"Ợ\": [\"Owj\", \"Ơ5\"],\n                     \"É\": [\"Es\", \"E1\"], \"È\": [\"Ef\", \"E2\"], \"Ẻ\": [\"Er\", \"E3\"], \"Ẽ\": [\"Ex\", \"E4\"], \"Ẹ\": [\"Ej\", \"E5\"],\n                     \"Ê\": [\"Ee\", \"E6\"], \"Ế\": [\"Ees\", \"Ê1\"], \"Ề\": [\"Eef\", \"Ê2\"],\n                     \"Ể\": [\"Eer\", \"Ê3\"], \"Ễ\": [\"Eex\", \"Ê4\"], \"Ệ\": [\"Eej\", \"Ê5\"], \"Ú\": [\"Us\", \"U1\"], \"Ù\": [\"Uf\", \"U2\"],\n                     \"Ủ\": [\"Ur\", \"U3\"], \"Ũ\": [\"Ux\", \"U4\"], \"Ụ\": [\"Uj\", \"U5\"], \"Ư\": [\"Uw\", \"U7\"], \"Ứ\": [\"Uws\", \"Ư1\"],\n                     \"Ừ\": [\"Uwf\", \"Ư2\"], \"Ử\": [\"Uwr\", \"Ư3\"], \"Ữ\": [\"Uwx\", \"Ư4\"],\n                     \"Ự\": [\"Uwj\", \"Ư5\"], \"Í\": [\"Is\", \"I1\"], \"Ì\": [\"If\", \"I2\"], \"Ỉ\": [\"Ir\", \"I3\"], \"Ị\": [\"Ij\", \"I5\"],\n                     \"Ĩ\": [\"Ix\", \"I4\"], \"Ý\": [\"Ys\", \"Y1\"], \"Ỳ\": [\"Yf\", \"Y2\"], \"Ỷ\": [\"Yr\", \"Y3\"], \"Ỵ\": [\"Yj\", \"Y5\"],\n                     \"Đ\": [\"Dd\", \"D9\"]}\n        self.all_word_candidates = self.get_all_word_candidates(self.word_couples)\n        self.string_all_word_candidates = ' '.join(self.all_word_candidates)\n        self.all_char_candidates = self.get_all_char_candidates()\n        self.keyboardNeighbors = self.getKeyboardNeighbors()\n\n    def replace_teencode(self, word):\n        candidates = self.teencode_dict.get(word, None)\n        if candidates is not None:\n            chosen_one = 0\n            if len(candidates) > 1:\n                chosen_one = np.random.randint(0, len(candidates))\n            return candidates[chosen_one]\n\n    def getKeyboardNeighbors(self):\n        keyboardNeighbors = {}\n        keyboardNeighbors['a'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ă'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['â'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['á'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['à'] = \"aáàảãăắằẳẵâấầẩẫ\"\n        keyboardNeighbors['ả'] = \"aảã\"\n        keyboardNeighbors['ã'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ạ'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ắ'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ằ'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ẳ'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ặ'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ẵ'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ấ'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ầ'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ẩ'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ẫ'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ậ'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['b'] = \"bh\"\n        keyboardNeighbors['c'] = \"cgn\"\n        keyboardNeighbors['d'] = \"đctơở\"\n        keyboardNeighbors['đ'] = \"d\"\n        keyboardNeighbors['e'] = \"eéèẻẽẹêếềểễệbpg\"\n        keyboardNeighbors['é'] = \"eéèẻẽẹêếềểễệ\"\n        keyboardNeighbors['è'] = \"eéèẻẽẹêếềểễệ\"\n        keyboardNeighbors['ẻ'] = \"eéèẻẽẹêếềểễệ\"\n        keyboardNeighbors['ẽ'] = \"eéèẻẽẹêếềểễệ\"\n        keyboardNeighbors['ẹ'] = \"eéèẻẽẹêếềểễệ\"\n        keyboardNeighbors['ê'] = \"eéèẻẽẹêếềểễệá\"\n        keyboardNeighbors['ế'] = \"eéèẻẽẹêếềểễệố\"\n        keyboardNeighbors['ề'] = \"eéèẻẽẹêếềểễệ\"\n        keyboardNeighbors['ể'] = \"eéèẻẽẹêếềểễệôốồổỗộ\"\n        keyboardNeighbors['ễ'] = \"eéèẻẽẹêếềểễệ\"\n        keyboardNeighbors['ệ'] = \"eéèẻẽẹêếềểễệ\"\n        keyboardNeighbors['g'] = \"qgộ\"\n        keyboardNeighbors['h'] = \"h\"\n        keyboardNeighbors['i'] = \"iíìỉĩịat\"\n        keyboardNeighbors['í'] = \"iíìỉĩị\"\n        keyboardNeighbors['ì'] = \"iíìỉĩị\"\n        keyboardNeighbors['ỉ'] = \"iíìỉĩị\"\n        keyboardNeighbors['ĩ'] = \"iíìỉĩị\"\n        keyboardNeighbors['ị'] = \"iíìỉĩịhự\"\n        keyboardNeighbors['k'] = \"klh\"\n        keyboardNeighbors['l'] = \"ljidđ\"\n        keyboardNeighbors['m'] = \"mn\"\n        keyboardNeighbors['n'] = \"mnedư\"\n        keyboardNeighbors['o'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ó'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ò'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ỏ'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['õ'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ọ'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ô'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ố'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ồ'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ổ'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ộ'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ỗ'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ơ'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ớ'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ờ'] = \"oóòỏọõôốồổỗộơớờởợỡà\"\n        keyboardNeighbors['ở'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ợ'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ỡ'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        # keyboardNeighbors['p'] = \"op\"\n        # keyboardNeighbors['q'] = \"qọ\"\n        # keyboardNeighbors['r'] = \"rht\"\n        # keyboardNeighbors['s'] = \"s\"\n        # keyboardNeighbors['t'] = \"tp\"\n        keyboardNeighbors['u'] = \"uúùủũụưứừữửựhiaạt\"\n        keyboardNeighbors['ú'] = \"uúùủũụưứừữửự\"\n        keyboardNeighbors['ù'] = \"uúùủũụưứừữửự\"\n        keyboardNeighbors['ủ'] = \"uúùủũụưứừữửự\"\n        keyboardNeighbors['ũ'] = \"uúùủũụưứừữửự\"\n        keyboardNeighbors['ụ'] = \"uúùủũụưứừữửự\"\n        keyboardNeighbors['ư'] = \"uúùủũụưứừữửựg\"\n        keyboardNeighbors['ứ'] = \"uúùủũụưứừữửự\"\n        keyboardNeighbors['ừ'] = \"uúùủũụưứừữửự\"\n        keyboardNeighbors['ử'] = \"uúùủũụưứừữửự\"\n        keyboardNeighbors['ữ'] = \"uúùủũụưứừữửự\"\n        keyboardNeighbors['ự'] = \"uúùủũụưứừữửựg\"\n        keyboardNeighbors['v'] = \"v\"\n        keyboardNeighbors['x'] = \"x\"\n        keyboardNeighbors['y'] = \"yýỳỷỵỹụ\"\n        keyboardNeighbors['ý'] = \"yýỳỷỵỹ\"\n        keyboardNeighbors['ỳ'] = \"yýỳỷỵỹ\"\n        keyboardNeighbors['ỷ'] = \"yýỳỷỵỹ\"\n        keyboardNeighbors['ỵ'] = \"yýỳỷỵỹ\"\n        keyboardNeighbors['ỹ'] = \"yýỳỷỵỹ\"\n        # keyboardNeighbors['w'] = \"wv\"\n        # keyboardNeighbors['j'] = \"jli\"\n        # keyboardNeighbors['z'] = \"zs\"\n        # keyboardNeighbors['f'] = \"ft\"\n\n        return keyboardNeighbors\n\n    def replace_char_noaccent(self, text, onehot_label):\n\n        # find index noise\n        idx = np.random.randint(0, len(onehot_label))\n        prevent_loop = 0\n        while onehot_label[idx] == 1 or text[idx].isnumeric() or text[idx] in string.punctuation:\n            idx = np.random.randint(0, len(onehot_label))\n            prevent_loop += 1\n            if prevent_loop > 10:\n                return False, text, onehot_label\n\n        index_noise = idx\n        onehot_label[index_noise] = 1\n        word_noise = text[index_noise]\n        for id in range(0, len(word_noise)):\n            char = word_noise[id]\n\n            if char in self.keyboardNeighbors:\n                neighbors = self.keyboardNeighbors[char]\n                idx_neigh = np.random.randint(0, len(neighbors))\n                replaced = neighbors[idx_neigh]\n                word_noise = word_noise[: id] + replaced + word_noise[id + 1:]\n                text[index_noise] = word_noise\n                return True, text, onehot_label\n\n        return False, text, onehot_label\n\n    def replace_word_candidate(self, word):\n        \"\"\"\n        Return a homophone word of the input word.\n        \"\"\"\n        capital_flag = word[0].isupper()\n        word = word.lower()\n        if capital_flag and word in self.teencode_dict:\n            return self.replace_teencode(word).capitalize()\n        elif word in self.teencode_dict:\n            return self.replace_teencode(word)\n\n        for couple in self.word_couples:\n            for i in range(2):\n                if couple[i] == word:\n                    if i == 0:\n                        if capital_flag:\n                            return couple[1].capitalize()\n                        else:\n                            return couple[1]\n                    else:\n                        if capital_flag:\n                            return couple[0].capitalize()\n                        else:\n                            return couple[0]\n\n    def replace_char_candidate(self, char):\n        \"\"\"\n        return a homophone char/subword of the input char.\n        \"\"\"\n        for couple in self.char_couples:\n            for i in range(2):\n                if couple[i] == char:\n                    if i == 0:\n                        return couple[1]\n                    else:\n                        return couple[0]\n\n    def replace_char_candidate_typo(self, char):\n        \"\"\"\n        return a homophone char/subword of the input char.\n        \"\"\"\n        i = np.random.randint(0, 2)\n\n        return self.typo[char][i]\n\n    def get_all_char_candidates(self, ):\n\n        all_char_candidates = []\n        for couple in self.char_couples:\n            all_char_candidates.extend(couple)\n        return all_char_candidates\n\n    def get_all_word_candidates(self, word_couples):\n\n        all_word_candidates = []\n        for couple in self.word_couples:\n            all_word_candidates.extend(couple)\n        return all_word_candidates\n\n    def remove_diacritics(self, text, onehot_label):\n        \"\"\"\n        Replace word which has diacritics with the same word without diacritics\n        Args:\n            text: a list of word tokens\n            onehot_label: onehot array indicate position of word that has already modify, so this\n            function only choose the word that do not has onehot label == 1.\n        return: a list of word tokens has one word that its diacritics was removed,\n                a list of onehot label indicate the position of words that has been modified.\n        \"\"\"\n        idx = np.random.randint(0, len(onehot_label))\n        prevent_loop = 0\n        while onehot_label[idx] == 1 or text[idx] == unidecode.unidecode(text[idx]) or text[idx] in string.punctuation:\n            idx = np.random.randint(0, len(onehot_label))\n            prevent_loop += 1\n            if prevent_loop > 10:\n                return False, text, onehot_label\n\n        onehot_label[idx] = 1\n        text[idx] = unidecode.unidecode(text[idx])\n        return True, text, onehot_label\n\n    def replace_with_random_letter(self, text, onehot_label):\n        \"\"\"\n        Replace, add (or remove) a random letter in a random chosen word with a random letter\n        Args:\n            text: a list of word tokens\n            onehot_label: onehot array indicate position of word that has already modify, so this\n            function only choose the word that do not has onehot label == 1.\n        return: a list of word tokens has one word that has been modified,\n                a list of onehot label indicate the position of words that has been modified.\n        \"\"\"\n        idx = np.random.randint(0, len(onehot_label))\n        prevent_loop = 0\n        while onehot_label[idx] == 1 or text[idx].isnumeric() or text[idx] in string.punctuation:\n            idx = np.random.randint(0, len(onehot_label))\n            prevent_loop += 1\n            if prevent_loop > 10:\n                return False, text, onehot_label\n\n        # replace, add or remove? 0 is replace, 1 is add, 2 is remove\n        coin = np.random.choice([0, 1, 2])\n        if coin == 0:\n            chosen_letter = text[idx][np.random.randint(0, len(text[idx]))]\n            replaced = self.vn_alphabet[np.random.randint(0, self.alphabet_len)]\n            try:\n                text[idx] = re.sub(chosen_letter, replaced, text[idx])\n            except:\n                return False, text, onehot_label\n        elif coin == 1:\n            chosen_letter = text[idx][np.random.randint(0, len(text[idx]))]\n            replaced = chosen_letter + self.vn_alphabet[np.random.randint(0, self.alphabet_len)]\n            try:\n                text[idx] = re.sub(chosen_letter, replaced, text[idx])\n            except:\n                return False, text, onehot_label\n        else:\n            chosen_letter = text[idx][np.random.randint(0, len(text[idx]))]\n            try:\n                text[idx] = re.sub(chosen_letter, '', text[idx])\n            except:\n                return False, text, onehot_label\n\n        onehot_label[idx] = 1\n        return True, text, onehot_label\n\n    def replace_with_homophone_word(self, text, onehot_label):\n        \"\"\"\n        Replace a candidate word (if exist in the word_couple) with its homophone. if successful, return True, else False\n        Args:\n            text: a list of word tokens\n            onehot_label: onehot array indicate position of word that has already modify, so this\n            function only choose the word that do not has onehot label == 1.\n        return: True, text, onehot_label if successful replace, else False, text, onehot_label\n        \"\"\"\n        # account for the case that the word in the text is upper case but its lowercase match the candidates list\n        candidates = []\n        for i in range(len(text)):\n            if text[i].lower() in self.all_word_candidates or text[i].lower() in self.teencode_dict.keys():\n                candidates.append((i, text[i]))\n\n        if len(candidates) == 0:\n            return False, text, onehot_label\n\n        idx = np.random.randint(0, len(candidates))\n        prevent_loop = 0\n        while onehot_label[candidates[idx][0]] == 1:\n            idx = np.random.choice(np.arange(0, len(candidates)))\n            prevent_loop += 1\n            if prevent_loop > 5:\n                return False, text, onehot_label\n\n        text[candidates[idx][0]] = self.replace_word_candidate(candidates[idx][1])\n        onehot_label[candidates[idx][0]] = 1\n        return True, text, onehot_label\n\n    def replace_with_homophone_letter(self, text, onehot_label):\n        \"\"\"\n        Replace a subword/letter with its homophones\n        Args:\n            text: a list of word tokens\n            onehot_label: onehot array indicate position of word that has already modify, so this\n            function only choose the word that do not has onehot label == 1.\n        return: True, text, onehot_label if successful replace, else False, None, None\n        \"\"\"\n        candidates = []\n        for i in range(len(text)):\n            for char in self.all_char_candidates:\n                if re.search(char, text[i]) is not None:\n                    candidates.append((i, char))\n                    break\n\n        if len(candidates) == 0:\n\n            return False, text, onehot_label\n        else:\n            idx = np.random.randint(0, len(candidates))\n            prevent_loop = 0\n            while onehot_label[candidates[idx][0]] == 1:\n                idx = np.random.randint(0, len(candidates))\n                prevent_loop += 1\n                if prevent_loop > 5:\n                    return False, text, onehot_label\n\n            replaced = self.replace_char_candidate(candidates[idx][1])\n            text[candidates[idx][0]] = re.sub(candidates[idx][1], replaced, text[candidates[idx][0]])\n\n            onehot_label[candidates[idx][0]] = 1\n            return True, text, onehot_label\n\n    def replace_with_typo_letter(self, text, onehot_label):\n        \"\"\"\n        Replace a subword/letter with its homophones\n        Args:\n            text: a list of word tokens\n            onehot_label: onehot array indicate position of word that has already modify, so this\n            function only choose the word that do not has onehot label == 1.\n        return: True, text, onehot_label if successful replace, else False, None, None\n        \"\"\"\n        # find index noise\n        idx = np.random.randint(0, len(onehot_label))\n        prevent_loop = 0\n        while onehot_label[idx] == 1 or text[idx].isnumeric() or text[idx] in string.punctuation:\n            idx = np.random.randint(0, len(onehot_label))\n            prevent_loop += 1\n            if prevent_loop > 10:\n                return False, text, onehot_label\n\n        index_noise = idx\n        onehot_label[index_noise] = 1\n\n        word_noise = text[index_noise]\n        for j in range(0, len(word_noise)):\n            char = word_noise[j]\n\n            if char in self.typo:\n                replaced = self.replace_char_candidate_typo(char)\n                word_noise = word_noise[: j] + replaced + word_noise[j + 1:]\n                text[index_noise] = word_noise\n                return True, text, onehot_label\n        return True, text, onehot_label\n\n    def add_noise(self, sentence, percent_err=0.15, num_type_err=5):\n        tokens = self.tokenizer(sentence)\n        onehot_label = [0] * len(tokens)\n\n        num_wrong = int(np.ceil(percent_err * len(tokens)))\n        num_wrong = np.random.randint(1, num_wrong + 1)\n        if np.random.rand() < 0.05:\n            num_wrong = 0\n\n        for i in range(0, num_wrong):\n            err = np.random.randint(0, num_type_err + 1)\n\n            if err == 0:\n                _, tokens, onehot_label = self.replace_with_homophone_letter(tokens, onehot_label)\n            elif err == 1:\n                _, tokens, onehot_label = self.replace_with_typo_letter(tokens, onehot_label)\n            elif err == 2:\n                _, tokens, onehot_label = self.replace_with_homophone_word(tokens, onehot_label)\n            elif err == 3:\n                _, tokens, onehot_label = self.replace_with_random_letter(tokens, onehot_label)\n            elif err == 4:\n                _, tokens, onehot_label = self.remove_diacritics(tokens, onehot_label)\n            elif err == 5:\n                _, tokens, onehot_label = self.replace_char_noaccent(tokens, onehot_label)\n            else:\n                continue\n            # print(tokens)\n        return ' '.join(tokens)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:31:36.638381Z","iopub.execute_input":"2022-01-16T09:31:36.638614Z","iopub.status.idle":"2022-01-16T09:31:36.760951Z","shell.execute_reply.started":"2022-01-16T09:31:36.638586Z","shell.execute_reply":"2022-01-16T09:31:36.760091Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"synthesizer = SynthesizeData()\n# data = []\n# for line in content_all[0:3]:\n#     tup = synthesizer.add_noise(line)\n#     data.append(tup)\n# print(data)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:31:36.763316Z","iopub.execute_input":"2022-01-16T09:31:36.763796Z","iopub.status.idle":"2022-01-16T09:31:36.768246Z","shell.execute_reply.started":"2022-01-16T09:31:36.763749Z","shell.execute_reply":"2022-01-16T09:31:36.767450Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"# PUNCT_TO_REMOVE = string.punctuation\n# def remove_punctuation(text):\n#     \"\"\"custom function to remove the punctuation\"\"\"\n#     return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n\n# def remove_urls(text):\n#     url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n#     return url_pattern.sub(r'', text)\n\n# def remove_emoji(string):\n#     emoji_pattern = re.compile(\"[\"\n#                            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n#                            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n#                            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n#                            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n#                            u\"\\U00002702-\\U000027B0\"\n#                            u\"\\U000024C2-\\U0001F251\"\n#                            \"]+\", flags=re.UNICODE)\n#     return emoji_pattern.sub(r'', string)\n\n# def clean_numbers(x):\n#     if bool(re.search(r'\\d', x)):\n#         x = re.sub('[0-9]{5,}', '#####', x)\n#         x = re.sub('[0-9]{4}', '####', x)\n#         x = re.sub('[0-9]{3}', '###', x)\n#         x = re.sub('[0-9]{2}', '##', x)\n#     return x\n\n# def preprocessing_data(row):\n#     processed = re.sub(\n#             r'[^aAàÀảẢãÃáÁạẠăĂằẰẳẲẵẴắẮặẶâÂầẦẩẨẫẪấẤậẬbBcCdDđĐeEèÈẻẺẽẼéÉẹẸêÊềỀểỂễỄếẾệỆfFgGhHiIìÌỉỈĩĨíÍịỊjJkKlLmMnNoOòÒỏỎõÕóÓọỌôÔồỒổỔỗỖốỐộỘơƠờỜởỞỡỠớỚợỢpPqQrRsStTuUùÙủỦũŨúÚụỤưƯừỪửỬữỮứỨựỰvVwWxXyYỳỲỷỶỹỸýÝỵỴzZ0123456789!\"#$%&''()*+,-./:;<=>?@[\\]^_`{|}~ ]',\n#             \"\", row)\n#     return processed\n\n# df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:31:36.769541Z","iopub.execute_input":"2022-01-16T09:31:36.771208Z","iopub.status.idle":"2022-01-16T09:31:36.781313Z","shell.execute_reply.started":"2022-01-16T09:31:36.771162Z","shell.execute_reply":"2022-01-16T09:31:36.780611Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Params CONFIG\n","metadata":{}},{"cell_type":"code","source":"print(\"here\")\nMAXLEN = 40\nNGRAM = 5\nalphabets = 'aAàÀảẢãÃáÁạẠăĂằẰẳẲẵẴắẮặẶâÂầẦẩẨẫẪấẤậẬ0bBcCdDđĐeEè1ÈẻẺẽẼéÉẹẸêÊềỀểỂễỄếẾệỆfFgGhHiIìÌỉỈ2ĩĨíÍịỊjJkKlLmMnNoO3òÒỏỎõÕóÓọỌôÔồ4ỒổỔỗỖốỐộỘơƠờỜ5ởỞỡỠớỚợỢpP6qQrRsStTuUùÙủỦ7ũŨúÚụỤưƯừỪửỬữỮứỨựỰvVw8WxXyYỳỲỷỶ9ỹỸýÝỵỴzZ!\"#$%&\\'()*+,-./:;<=>?@[\\]^_`{|}~ '\n# alphabets = 'aAàÀảẢãÃáÁạẠăĂằẰẳẲẵẴắẮặẶâÂầẦẩẨẫẪấẤậẬbBcCdDđĐeEèÈẻẺẽẼéÉẹẸêÊềỀểỂễỄếẾệỆfFgGhHiIìÌỉỈĩĨíÍịỊjJkKlLmMnNoOòÒỏỎõÕóÓọỌôÔồỒổỔỗỖốỐộỘơƠờỜởỞỡỠớỚợỢpPqQrRsStTuUùÙủỦũŨúÚụỤưƯừỪửỬữỮứỨựỰvVwWxXyYỳỲỷỶỹỸýÝỵỴzZ0123456789!\"#$%&\\'()*+,-./:;<=>?@[\\]^_`{|}~ ]'\nprint(len(alphabets))\nprint(alphabets[76])\nprint(alphabets)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:31:36.782435Z","iopub.execute_input":"2022-01-16T09:31:36.782657Z","iopub.status.idle":"2022-01-16T09:31:36.799590Z","shell.execute_reply.started":"2022-01-16T09:31:36.782631Z","shell.execute_reply":"2022-01-16T09:31:36.798546Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# content_all = df['Content'].values.tolist()\n# content_all[0:3]","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:31:36.802150Z","iopub.execute_input":"2022-01-16T09:31:36.802689Z","iopub.status.idle":"2022-01-16T09:31:36.809983Z","shell.execute_reply.started":"2022-01-16T09:31:36.802642Z","shell.execute_reply":"2022-01-16T09:31:36.809310Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# ##https://viblo.asia/p/ung-dung-machine-translation-vao-bai-toan-them-dau-cho-tieng-viet-khong-dau-aivivn-challenge-3-3P0lP4a8lox\n# class CreateDataset():\n#     def __init__(self, csv_path='../input/kpdl-data/train_remove_noise.csv', save_path=\"./list_ngrams.npy\"):\n#         self.csv_path = csv_path\n#         self.alphabets_regex = '^[aAàÀảẢãÃáÁạẠăĂằẰẳẲẵẴắẮặẶâÂầẦẩẨẫẪấẤậẬbBcCdDđĐeEèÈẻẺẽẼéÉẹẸêÊềỀểỂễỄếẾệỆfFgGhHiIìÌỉỈĩĨíÍịỊjJkKlLmMnNoOòÒỏỎõÕóÓọỌôÔồỒổỔỗỖốỐộỘơƠờỜởỞỡỠớỚợỢpPqQrRsStTuUùÙủỦũŨúÚụỤưƯừỪửỬữỮứỨựỰvVwWxXyYỳỲỷỶỹỸýÝỵỴzZ0123456789!\"#$%&''()*+,-./:;<=>?@[\\]^_`{|}~ ]'\n#         self.save_path = save_path\n\n\n#     def processing(self):\n#         # read csv\n#         df = pd.read_csv(self.csv_path)\n\n#         # remove characters that out of vocab\n#         df['Content'] = df['Content'].apply(self.preprocessing_data)\n\n#         # extract phrases\n#         phrases = itertools.chain.from_iterable(self.extract_phrases(text) for text in df['Content'])\n#         phrases = [p.strip() for p in phrases if len(p.split()) > 1]\n\n#         # gen ngrams\n#         list_ngrams = []\n#         for p in tqdm(phrases):\n#             if not re.match(self.alphabets_regex, p.lower()):\n#                 continue\n#             if len(phrases) == 0:\n#                 continue\n\n#             for ngr in self.gen_ngrams(p, NGRAM):\n#                 if len(\" \".join(ngr)) < MAXLEN:\n#                     list_ngrams.append(\" \".join(ngr))\n#         print(\"DONE extract ngrams, total ngrams: \", len(list_ngrams))\n#         print(list_ngrams[0:30])\n\n#         # save ngrams\n#         self.save_ngrams(list_ngrams, save_path=self.save_path)\n\n#         print(\"Done create dataset - ngrams\")\n\n#     def preprocessing_data(self, row):\n#         processed = re.sub(\n#             r'[^aAàÀảẢãÃáÁạẠăĂằẰẳẲẵẴắẮặẶâÂầẦẩẨẫẪấẤậẬbBcCdDđĐeEèÈẻẺẽẼéÉẹẸêÊềỀểỂễỄếẾệỆfFgGhHiIìÌỉỈĩĨíÍịỊjJkKlLmMnNoOòÒỏỎõÕóÓọỌôÔồỒổỔỗỖốỐộỘơƠờỜởỞỡỠớỚợỢpPqQrRsStTuUùÙủỦũŨúÚụỤưƯừỪửỬữỮứỨựỰvVwWxXyYỳỲỷỶỹỸýÝỵỴzZ0123456789!\"#$%&''()*+,-./:;<=>?@[\\]^_`{|}~ ]',\n#             \"\", row)\n#         return processed\n\n#     def extract_phrases(self, text):\n#         return re.findall(r'\\w[\\w ]+', text)\n\n#     def gen_ngrams(self, text, n=5):\n#         tokens = text.split()\n\n#         if len(tokens) < n:\n#             return [tokens]\n\n#         return nltk.ngrams(text.split(), n)\n\n#     def save_ngrams(self, list_ngrams, save_path='ngrams_list.npy'):\n#         with open(save_path, 'wb') as f:\n#             np.save(f, list_ngrams)\n#         print(\"Saved dataset - ngrams\")\n\n# creater = CreateDataset()\n# creater.processing()","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:31:36.811763Z","iopub.execute_input":"2022-01-16T09:31:36.812084Z","iopub.status.idle":"2022-01-16T09:31:36.822865Z","shell.execute_reply.started":"2022-01-16T09:31:36.812027Z","shell.execute_reply":"2022-01-16T09:31:36.822166Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# encode_string1\n# encode_string1 = np.array(encode_string1)\n# print(encode_string1.shape[0])","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:31:36.823972Z","iopub.execute_input":"2022-01-16T09:31:36.826216Z","iopub.status.idle":"2022-01-16T09:31:36.839313Z","shell.execute_reply.started":"2022-01-16T09:31:36.826169Z","shell.execute_reply":"2022-01-16T09:31:36.838489Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class Vocab():\n    def __init__(self, chars):\n        self.pad = 0\n        self.go = 1\n        self.eos = 2\n\n        self.chars = chars\n       \n        \n        self.i2c = {i + 3: c for i, c in enumerate(chars)}\n        \n        self.c2i = {c: i + 3 for i, c in enumerate(chars)}\n        \n        self.i2c[0] = '<pad>'\n        self.i2c[1] = '<sos>'\n        self.i2c[2] = '<eos>'\n\n        \n        \n\n    def encode(self, chars):\n        return [self.go] + [self.c2i[c] for c in chars] + [self.eos]\n\n    def decode(self, ids):\n        first = 1 if self.go in ids else 0\n        last = ids.index(self.eos) if self.eos in ids else None\n        sent = ''.join([self.i2c[i] for i in ids[first:last]])\n        return sent\n\n    def __len__(self):\n        return len(self.c2i) + 3\n\n    def batch_decode(self, arr):\n        texts = [self.decode(ids) for ids in arr]\n        return texts\n\n    def __str__(self):\n        return self.chars\n    \nvocab = Vocab(alphabets)\nprint(vocab.i2c)\nprint(vocab.c2i)\nstring1 = 'Tôi thích đi dạo'\nencode_string1 = vocab.encode(string1)\nprint(encode_string1)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:31:36.844845Z","iopub.execute_input":"2022-01-16T09:31:36.845115Z","iopub.status.idle":"2022-01-16T09:31:36.858708Z","shell.execute_reply.started":"2022-01-16T09:31:36.845076Z","shell.execute_reply":"2022-01-16T09:31:36.857943Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"list_ngram_target = np.load('../input/5gram-nlp-project/list_5gram_nonum_train.npy')\nlist_ngram_target  = list_ngram_target [:1000000]\nlist_ngram_valid_target = np.load('../input/5gram-nlp-project/list_5gram_nonum_valid.npy')\nlist_ngram_valid_target = list_ngram_valid_target[5000:15000]\nlist_ngram_train = np.load('../input/5gram-nlp-project/train_normal_captions.npy')\nlist_ngram_valid = np.load('../input/5gram-nlp-project/valid_normal_captions.npy')\nclass SpellingDataset(Dataset):\n    def __init__(self, list_ngram, list_ngram_target, vocab, maxlen):\n        self.list_ngram = list_ngram\n        self.list_ngram_target = list_ngram_target\n        self.vocab = vocab\n        self.max_len = maxlen\n    \n    def __getitem__(self, index):\n        train_text = self.list_ngram[index]\n        train_target = self.list_ngram_target[index]\n        \n        \n        train_text_encode = self.vocab.encode(train_text)\n        train_target_encode = self.vocab.encode(train_target)\n        \n        train_text_length = len(train_text_encode)\n        train_target_length = len(train_target_encode)\n        \n        if(train_text_length < self.max_len):\n            pad_length = self.max_len-train_text_length\n            train_text_encode = np.array(train_text_encode)\n            train_text_encode = np.concatenate((train_text_encode, np.zeros(pad_length)), axis = 0)\n            \n        elif(train_text_length>= self.max_len):\n            train_text_encode = train_text_encode[0:self.max_len]\n            train_text_encode = np.array(train_text_encode)\n            \n        if(train_target_length < self.max_len):\n            pad_length = self.max_len-train_target_length\n            train_target_encode = np.array(train_target_encode)\n            train_target_encode = np.concatenate((train_target_encode, np.zeros(pad_length)), axis = 0)\n            \n        elif(train_target_length>= self.max_len):\n            train_target_encode = train_target_encode[0:self.max_len]\n            train_target_encode = np.array(train_target_encode)      \n               \n        tensor_text = torch.from_numpy(train_text_encode)\n        tensor_target = torch.from_numpy(train_target_encode)\n        return tensor_text, tensor_target\n        \n        \n    \n    def __len__(self):\n        return len(self.list_ngram)\n\nds_train = SpellingDataset(list_ngram_train, list_ngram_target, vocab, MAXLEN)\nds_valid = SpellingDataset(list_ngram_valid, list_ngram_valid_target,vocab, MAXLEN)\n# ds_test = SpellingDataset(list_ngram_test, synthesizer, vocab, MAXLEN)\ntrain_loader = DataLoader(ds_train, batch_size = 512 , shuffle=True)\nval_loader = DataLoader(ds_valid, batch_size = 1)\n# test_loader = DataLoader(ds_test, batch_size = 200)\nprint(len(train_loader), len(val_loader))\ntext, target = next(iter(train_loader))\nvalid_text, valid_target = next(iter(train_loader))\nprint(text[0])\nprint(target[0])\nprint(\"Text: \", vocab.decode(np.squeeze(text[0].detach().numpy()).tolist()))\nprint(\"Target: \", vocab.decode(np.squeeze(target[0].detach().numpy()).tolist()))\nprint(\"Text: \", vocab.decode(np.squeeze(valid_text[0].detach().numpy()).tolist()))\nprint(\"Target: \", vocab.decode(np.squeeze(valid_target[0].detach().numpy()).tolist()))\nprint(text, target)\nprint(text.size(), target.size())\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:35:37.821684Z","iopub.execute_input":"2022-01-16T09:35:37.821956Z","iopub.status.idle":"2022-01-16T09:35:38.393718Z","shell.execute_reply.started":"2022-01-16T09:35:37.821920Z","shell.execute_reply":"2022-01-16T09:35:38.392857Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# class MultiHeadAttention(nn.Module):\n#     def __init__(self, model_size, num_heads):\n#         super(MultiHeadAttention, self).__init__()\n#         self.key_size = model_size //num_heads\n#         self.heads = num_heads\n#         self.wq = nn.Linear(model_size, model_size)\n#         self.wk = nn.Linear(model_size, model_size)\n#         self.wv = nn.Linear(model_size, model_size)\n#         self.wo = nn.Linear(model_size, model_size)\n        \n    \n#     def forward(self, query, key, value, mask = None):\n#         # query len is seq_length\n#         # model_size is embed size, just different way to call\n#         # query shape (batch, query_len, model_size)\n#         # value shape (batch, value_len, model_size)\n#         query = self.wq(query)\n#         key = self.wk(key)\n#         value = self.wv(value)\n        \n#         # Originally, query has shape (batch, query_len, model_size)\n#         # We need to reshape to (batch, query_len, h, key_size)\n        \n#         batch_size = query.shape[0]\n#         query = query.reshape(batch_size, -1, self.heads, self.key_size)\n#         # In order to compute matmul, the dimensions must be transposed to (batch_size, heads, query_len, key_size)\n#         query = query.transpose(1, 2)\n\n#         # Do the same for key and value\n#         key = key.reshape(batch_size, -1, self.heads, self.key_size)\n#         key = key.transpose(1, 2)\n#         value = value.reshape(batch_size, -1, self.heads, self.key_size)\n#         value = value.transpose(1, 2)\n        \n#         # query shape(batch_size, heads, query_len, key_size)\n#         # value and key shape(batch_size, heads, value_len, key_size)\n#         score = torch.matmul(query, key.transpose(2, 3))\n#          # score will have shape of (batch_size, heads, query_len, value_len)\n#         score = score / (torch.sqrt(torch.FloatTensor([self.key_size])).to(device))\n#         if mask is not None:\n#             score = score.masked_fill(mask==0, -1e10)\n        \n#         attention = torch.softmax(score, dim=-1)\n#         # attention shape (batch_size, heads, query_len, value_len)\n#         # value shape (batch_size, heads, value, key_size)\n        \n#         context = torch.matmul(attention, value)\n#         # context shape (batch_size, heads, query_len, key_size)\n        \n#         context = context.transpose(1, 2).reshape(batch_size, -1, self.key_size * self.heads)\n#         # context shape (batch_size, query_len, heads, key_size ) -> (batch_size, query_len, model_size)\n        \n#         x = self.wo(context)\n#         # x shape (batch_size, query_len, model_size)\n#         return x, attention\n        ","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:31:39.883300Z","iopub.execute_input":"2022-01-16T09:31:39.883580Z","iopub.status.idle":"2022-01-16T09:31:39.889201Z","shell.execute_reply.started":"2022-01-16T09:31:39.883548Z","shell.execute_reply":"2022-01-16T09:31:39.888341Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# class TransformerBlock(nn.Module):\n#     def __init__(self, model_size, num_heads, dropout, forward_expansion):\n#         super(TransformerBlock, self).__init__()\n#         self.attention = MultiHeadAttention(model_size, num_heads)\n#         self.norm1 = nn.LayerNorm(model_size)\n#         self.norm2 = nn.LayerNorm(model_size)\n    \n#         self.feed_forward = nn.Sequential(nn.Linear(model_size, model_size* forward_expansion), nn.ReLU(), nn.Linear(model_size * forward_expansion, model_size))\n#         self.dropout = nn.Dropout(dropout)\n        \n#     def forward(self, query, value, key, mask):\n#         x, attention = self.attention(query, value, key, mask)\n#         x = self.dropout(self.norm1(x) + query)\n        \n#         # x shape (batch_size, query_len, model_size)\n#         forward = self.feed_forward(x)\n#         out = self.dropout(self.norm2(forward) + x)\n#         # out shape (batch_size, query_len, model_size)\n#         return out, attention\n        ","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:31:39.890637Z","iopub.execute_input":"2022-01-16T09:31:39.891119Z","iopub.status.idle":"2022-01-16T09:31:39.902567Z","shell.execute_reply.started":"2022-01-16T09:31:39.891085Z","shell.execute_reply":"2022-01-16T09:31:39.901979Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# class Encoder(nn.Module):\n#     def __init__(self, src_vocab_size, model_size, num_layers, num_heads, device, forward_expansion, dropout, pes, max_length=40):\n#         super(Encoder, self).__init__()\n#         self.model_size = model_size\n#         self.device = device\n#         self.word_embedding = nn.Embedding(src_vocab_size, model_size)\n#         self.position_embedding = nn.Embedding(max_length, model_size)\n        \n#         self.layers = nn.ModuleList([TransformerBlock(model_size, num_heads, dropout, forward_expansion) for _ in range(num_layers)])\n#         self.dropout = nn.Dropout(dropout)\n    \n#     def forward(self, x, mask):\n#         #x = [batch size, src len]\n#         #x_mask = [batch size, 1, 1, src len]\n#         batch_size, seq_len = x.shape\n#         pos = torch.arange(0, seq_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n#         # pos shape (batch_size, seq_len)\n# #         print(\"X shape\", x.size())\n#         embed_out = self.word_embedding(x)\n#         embed_out *= math.sqrt(self.model_size)\n#         embed_out += pes[:seq_len, :]\n#         x = self.dropout(embed_out)\n        \n#         # out shape (batch_size, seq_len, model_size)\n#         for layer in self.layers:\n#             x, _ = layer(x, x, x, mask)\n        \n#         return x","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:31:39.903967Z","iopub.execute_input":"2022-01-16T09:31:39.904473Z","iopub.status.idle":"2022-01-16T09:31:39.918653Z","shell.execute_reply.started":"2022-01-16T09:31:39.904439Z","shell.execute_reply":"2022-01-16T09:31:39.917793Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# class DecoderBlock(nn.Module):\n#     def __init__(self, model_size, num_heads, device, forward_expansion, dropout, pes, max_length=40):\n#         super(DecoderBlock, self).__init__()\n    \n#         self.attention = MultiHeadAttention(model_size, num_heads)\n#         self.norm = nn.LayerNorm(model_size)\n#         self.transformerBlock = TransformerBlock(model_size, num_heads, dropout,forward_expansion)\n#         self.dropout = nn.Dropout(dropout)\n    \n#     def forward(self, x, value, key, src_mask, trg_mask):\n#         #x = (batch_size, seq_len, model_size]\n#         #enc_src = (batch_size, src_len, model_size)\n#         #trg_mask = (batch size, 1, trg_len, trg_len)\n#         #src_mask = (batch size, 1, 1, src_len)\n        \n#         trg, _ = self.attention(x, x, x,  trg_mask)\n#         query = self.dropout(self.norm(trg) + x )\n#         out, attention = self.transformerBlock(query, value, key, src_mask)\n#         return out, attention","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:31:39.919746Z","iopub.execute_input":"2022-01-16T09:31:39.919964Z","iopub.status.idle":"2022-01-16T09:31:39.931594Z","shell.execute_reply.started":"2022-01-16T09:31:39.919936Z","shell.execute_reply":"2022-01-16T09:31:39.930895Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# class Decoder(nn.Module):\n#     def __init__(self, target_vocab_size, model_size, num_layers, num_heads, device, forward_expansion, dropout, pes, max_length=40):\n#         super(Decoder, self).__init__()\n#         self.model_size = model_size\n#         self.device = device\n#         self.word_embedding = nn.Embedding(target_vocab_size, model_size)\n#         self.position_embedding = nn.Embedding(max_length, model_size)\n        \n#         self.layers = nn.ModuleList([DecoderBlock(model_size, num_heads, device, forward_expansion, dropout, pes,) for _ in range(num_layers)])\n#         self.dropout = nn.Dropout(dropout)\n#         self.fc_out = nn.Linear(model_size, target_vocab_size)\n        \n#     def forward(self, x, enc_output, src_mask, target_mask):\n#         batch_size, target_length = x.shape\n# #         pos = torch.arange(0, target_length).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n#         embed_out = self.word_embedding(x)\n#         embed_out *= math.sqrt(self.model_size)\n#         embed_out += pes[:target_length, :]\n#         x = self.dropout(embed_out)\n        \n#         for layer in self.layers:\n#             x, attention = layer(x, enc_output, enc_output, src_mask, target_mask)\n        \n#         output = self.fc_out(x)\n        \n#         return output, attention\n        ","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:31:39.932752Z","iopub.execute_input":"2022-01-16T09:31:39.933063Z","iopub.status.idle":"2022-01-16T09:31:39.946083Z","shell.execute_reply.started":"2022-01-16T09:31:39.933012Z","shell.execute_reply":"2022-01-16T09:31:39.945480Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# class Seq2SeqTransformer(nn.Module):\n#     def __init__(self, src_vocab_size, target_vocab_size, src_pad_idx, target_pad_idx, pes, device, model_size = 512, num_layers = 6, forward_expansion = 4, num_heads = 8, dropout = 0.1, max_length = 40):\n#         super(Seq2SeqTransformer, self).__init__()\n    \n#         self.encoder = Encoder(src_vocab_size, model_size, num_layers, num_heads, device, forward_expansion, dropout, pes, max_length)\n#         self.decoder = Decoder(target_vocab_size, model_size, num_layers, num_heads, device, forward_expansion, dropout, pes, max_length)\n#         self.src_pad_idx = src_pad_idx\n#         self.target_pad_idx = target_pad_idx\n#         self.device = device\n    \n#     def make_src_mask(self, src):\n        \n#         #src = (batch_size, src_len)\n        \n#         src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n\n#         #src_mask = (batch_size, 1, 1, src_len)\n\n#         return src_mask.to(self.device)\n    \n#     def make_target_mask(self, target):\n        \n#         batch_size, target_len = target.shape\n#         target_pad_mask = (target != self.target_pad_idx).unsqueeze(1).unsqueeze(2)\n# #         target_mask = torch.tril(torch.ones((target_len, target_len))).expand(batch_size, 1, target_len, target_len)\n#         target_mask = torch.tril(torch.ones((target_len, target_len), device = self.device)).bool()\n#         target_mask = target_pad_mask & target_mask\n#         return target_mask.to(device)\n    \n#     def forward(self, src, target):\n#         src_mask = self.make_src_mask(src)\n#         target_mask = self.make_target_mask(target)\n#         enc_src = self.encoder(src, src_mask)\n#         out, attention = self.decoder(target, enc_src, src_mask, target_mask)\n#         return out, attention","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:31:39.947096Z","iopub.execute_input":"2022-01-16T09:31:39.947408Z","iopub.status.idle":"2022-01-16T09:31:39.959538Z","shell.execute_reply.started":"2022-01-16T09:31:39.947381Z","shell.execute_reply":"2022-01-16T09:31:39.958936Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"class PositionEncoder(nn.Module):\n    def __init__(self, max_len, emb_size, dropout=0.1):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout)\n\n        pe = torch.zeros(max_len, emb_size)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, emb_size, 2).float() * (-math.log(10000.0) / emb_size))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)\n\nclass TransformerEncoder(nn.Module):\n    def __init__(self, input_size, emb_size, hidden_size, num_layer, max_len=64):\n        super().__init__()\n        self.emb_size = emb_size\n        self.hidden_size = hidden_size\n        self.num_layer = num_layer\n        self.scale = math.sqrt(emb_size)\n\n        self.embedding = nn.Embedding(input_size, emb_size)\n        # additional length for sos and eos\n        self.pos_encoder = PositionEncoder(max_len + 10, emb_size)\n        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_size, nhead=8,\n                                                   dim_feedforward=hidden_size,\n                                                   dropout=0.1, activation='gelu')\n        encoder_norm = nn.LayerNorm(emb_size)\n        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layer, norm=encoder_norm)\n\n    def forward(self, src, src_mask):\n        src = self.embedding(src) * self.scale\n        src = self.pos_encoder(src)\n        output = self.encoder(src, src_key_padding_mask=src_mask)\n        return output\n\nclass TransformerDecoder(nn.Module):\n    def __init__(self, output_size, emb_size, hidden_size, num_layer, max_len=64):\n        super().__init__()\n        self.emb_size = emb_size\n        self.hidden_size = hidden_size\n        self.num_layer = num_layer\n        self.scale = math.sqrt(emb_size)\n\n        self.embedding = nn.Embedding(output_size, emb_size)\n        self.pos_encoder = PositionEncoder(max_len + 10, emb_size)\n        decoder_layer = nn.TransformerDecoderLayer(d_model=emb_size, nhead=8,\n                                                   dim_feedforward=hidden_size,\n                                                   dropout=0.1, activation='gelu')\n        decoder_norm = nn.LayerNorm(emb_size)\n        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layer, norm=decoder_norm)\n        self.fc = nn.Linear(emb_size, output_size)\n\n    def forward(self, trg, enc_output, sub_mask, mask):\n#         print(trg.size())\n        trg = self.embedding(trg) * self.scale\n#         print(trg.size())\n        trg = self.pos_encoder(trg)\n#         print(trg.size())\n#         print(\"Target\", trg.size())\n#         print(\"Target sub mask\", sub_mask.size())\n#         print(\"Target mask\", mask.size())\n        output = self.decoder(trg, enc_output, tgt_mask=sub_mask, tgt_key_padding_mask=mask)\n#         print(output.size())\n        output = self.fc(output)\n        return output\n\nclass TransformerModel(nn.Module):\n    def __init__(self, input_size, emb_size, hidden_size, output_size,\n                 num_layer, max_len, pad_token, sos_token, eos_token):\n        super().__init__()\n        self.encoder = TransformerEncoder(input_size, emb_size, hidden_size, num_layer, max_len)\n        self.decoder = TransformerDecoder(output_size, emb_size, hidden_size, num_layer, max_len)\n        self.pad_token = pad_token\n        self.sos_token = sos_token\n        self.eos_token = eos_token\n\n        self.encoder.apply(self.initialize_weights)\n        self.decoder.apply(self.initialize_weights)\n\n    @staticmethod\n    def initialize_weights(m):\n        if hasattr(m, 'weight') and m.weight.dim() > 1:\n            nn.init.xavier_uniform_(m.weight.data)\n\n    @staticmethod\n    def generate_mask(src, pad_token):\n        '''\n        Generate mask for tensor src\n        :param src: tensor with shape (max_src, b)\n        :param pad_token: padding token\n        :return: mask with shape (b, max_src) where pad_token is masked with 1\n        '''\n        mask = (src.t() == pad_token)\n        return mask.to(src.device)\n\n    @staticmethod\n    def generate_submask(src):\n        sz = src.size(0)\n        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n        return mask.to(src.device)\n\n    def forward(self, src, trg):\n#         print(\"Src size\", src.size())\n#         print(\"Target size\", trg.size())\n        src_mask = self.generate_mask(src, self.pad_token)\n        trg_mask = self.generate_mask(trg, self.pad_token)\n#         print(\"Src mask size\", src_mask.size())\n#         print(\"Target mask size\", trg_mask.size())\n        trg_submask = self.generate_submask(trg)\n#         print(\"Target submask size\", trg_submask.size())\n        enc_output = self.encoder(src, src_mask)\n#         print(\"Encoding_output size\", enc_output.size())\n        output = self.decoder(trg, enc_output, trg_submask, trg_mask)\n        return output\n\n    def inference(self, src, max_len, device):\n#         assert src.dim() == 1, 'Can only translate one sentence at a time!'\n#         assert src.size(0) <= max_len + 2, f'Source sentence exceeds max length: {max_len}'\n\n#         src.unsqueeze_(-1)\n        \n        src_mask = self.generate_mask(src, self.pad_token)\n        enc_output = self.encoder(src, src_mask)\n#         device = src.device\n\n        trg_list = [self.sos_token]\n        for idx in range(max_len):\n            trg = torch.tensor(trg_list, dtype=torch.long, device=device).unsqueeze(-1)\n            trg_mask = self.generate_mask(trg, self.pad_token)\n            trg_submask = self.generate_submask(trg)\n            output = self.decoder(trg, enc_output, trg_submask, trg_mask)\n            pred = torch.argmax(output.squeeze(1), dim=-1)[-1].item()\n            trg_list.append(pred)\n            if pred == self.eos_token:\n                break\n        return torch.tensor(trg_list[1:], dtype=torch.long, device=device)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:31:39.960641Z","iopub.execute_input":"2022-01-16T09:31:39.960863Z","iopub.status.idle":"2022-01-16T09:31:39.996054Z","shell.execute_reply.started":"2022-01-16T09:31:39.960835Z","shell.execute_reply":"2022-01-16T09:31:39.995075Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n        \ndef asMinutes(s):\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\n\ndef timeSince(since, percent):\n    now = time.time()\n    s = now - since\n    es = s / (percent)\n    rs = es - s\n    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:31:39.997239Z","iopub.execute_input":"2022-01-16T09:31:39.997684Z","iopub.status.idle":"2022-01-16T09:31:40.011683Z","shell.execute_reply.started":"2022-01-16T09:31:39.997631Z","shell.execute_reply":"2022-01-16T09:31:40.010753Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"class Trainer(object):\n    def __init__(self, model, device, config):\n        self.config = config\n        self.start_epoch = 1\n        self.model = model\n        self.device = device\n        self.optimizer = AdamW(self.model.parameters(), lr = self.config.lr)\n        self.scheduler = torch.optim.lr_scheduler.OneCycleLR(self.optimizer, max_lr=0.001, steps_per_epoch=len(train_loader), epochs=2)\n#         self.scheduler = config.SchedulerClass(self.optimizer, **self.config.scheduler_params)\n        self.criterion = nn.CrossEntropyLoss(ignore_index = self.config.PAD_IDX).to(device)\n        self.scaler = GradScaler()\n        self.base_dir = f'{config.folder}'\n        if not os.path.exists(self.base_dir):\n            os.makedirs(self.base_dir)\n        \n    def train_loop(self, train_loader, validation_loader, train_resume=False):\n        if train_resume==True:\n            self.load_model('../input/model-transformer/model_transformer_50.pth')\n        for e in range(self.start_epoch, self.start_epoch+self.config.num_epochs):\n            t = time.time()\n            calc_loss = self.train_one_epoch(self.device, train_loader, e)\n            self.config.epoch.append(e)\n            self.config.train_loss.append(calc_loss.avg)\n            print(f'Train. Epoch: {e}, Loss: {calc_loss.avg:.5f}, time: {(time.time() - t):.5f}')\n\n            if (e==self.start_epoch+self.config.num_epochs-1):\n                t = time.time()\n                predictions, targets, calc_loss = self.valid_one_epoch(self.device, validation_loader)\n                acc_valid = self.accuracy_valid(predictions, targets)\n                self.config.valid_acc.append(acc_valid)\n                print(f'Val. Epoch: {e}, Loss: {calc_loss.avg:.5f}, Acc valid: {acc_valid:.5f}, time: {(time.time() - t):.5f}')\n                state = {'epoch': e, 'state_dict': self.model.state_dict(),'optimizer': self.optimizer.state_dict(), \n                         'train_loss': self.config.train_loss, 'valid_acc': self.config.valid_acc}\n                self.save_model(state, f'model_transformer_{e}.pth')\n            \n        \n        \n    def train_one_epoch(self, device, train_loader, e):\n        self.model.train()\n        calc_loss = AverageMeter()\n        start = end = time.time()\n        for batch_idx, (text, target) in tqdm(enumerate(train_loader)):\n            batch_size = text.size(0)\n            text = torch.transpose(text, 0, 1)\n            target =  torch.transpose(target, 0, 1)\n            text = text.to(device, dtype=torch.int64)\n            target = target.to(device, dtype=torch.int64)\n            self.optimizer.zero_grad()\n            \n            with autocast():\n                output = self.model(text, target[:-1])\n                output = output.contiguous().reshape(-1, output.shape[2])\n                target = target[1:].reshape(-1)\n            \n#             print(\"Output\",output.size())\n            \n#             print(\"Target\", target.size())\n\n            \n#                 print(\"Output\",output.size())\n            \n#                 print(\"Target\", target.size())\n            \n                # output shape (batch_size, seq_length, vocab_length)\n                # target shape (batch_size, seq_length)           \n\n                loss = self.criterion(output, target)\n            \n            loss_value = loss.item()\n\n            # Back prop\n#             loss.backward()\n            self.scaler.scale(loss).backward()\n\n            # Clip to avoid exploding gradient issues, makes sure grads are\n            # within a healthy range\n#             torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1)\n\n            # Gradient descent step\n#             self.optimizer.step()\n            self.scaler.step(self.optimizer)\n            self.scaler.update()\n            calc_loss.update(loss_value, batch_size)\n            if batch_idx %400==100:\n                print('Epoch: [{0}][{1}/{2}] '\n                  'Elapsed {remain:s} '\n                  .format(\n                   e, batch_idx, len(train_loader),\n                   remain=timeSince(start, float(batch_idx+1)/len(train_loader)),\n                   #lr=scheduler.get_lr()[0],\n                   ))\n                \n            \n        return calc_loss\n    \n    def valid_one_epoch(self, device, val_loader):\n        self.model.eval()\n        predictions = []\n        targets  =[]\n        calc_loss = AverageMeter()\n        with torch.no_grad():\n            for batch_idx, (text, target) in tqdm(enumerate(val_loader)): \n                batch_size = text.size(0)\n                targets.append(vocab.decode(np.squeeze(target.detach().numpy()).tolist()))\n                text = torch.transpose(text, 0, 1)\n                target =  torch.transpose(target, 0, 1)\n                text = text.to(device, dtype=torch.int64)\n                target = target.to(device, dtype=torch.int64)\n                prediction = model.inference(text, 40, device)\n                prediction = prediction.detach().cpu().numpy().tolist()\n                prediction = vocab.decode(prediction)\n                predictions.append(prediction)\n                output = model(text, target[:-1])\n                \n                \n\n\n\n                output = output.contiguous().reshape(-1, output.shape[2])\n                target = target[1:].reshape(-1)\n                \n                \n                loss = self.criterion(output, target)\n                loss_value = loss.item()\n\n                calc_loss.update(loss_value, batch_size)\n                \n        return predictions, targets, calc_loss\n    \n    def accuracy_valid(self, predictions, targets):\n        n = len(predictions)\n        acc = 0\n    #     print(len(predictions), len(targets))\n        for i in range(len(predictions)):\n\n    #         print(f\"Predictions {predictions}\")\n    #         print(f\"Targets {targets}\")\n            if predictions[i]==targets[i]:\n                acc+=1\n        return acc/n\n    \n    \n    def save_model(self, state, path):\n        torch.save(state, path)\n    \n    def load_model(self, path):\n        checkpoint = torch.load(path)\n        self.start_epoch = checkpoint['epoch']+1\n        self.model.load_state_dict(checkpoint['state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer'])\n        self.config.train_loss = checkpoint['train_loss']\n        self.config.valid_acc = checkpoint['valid_acc']","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:31:40.013056Z","iopub.execute_input":"2022-01-16T09:31:40.013395Z","iopub.status.idle":"2022-01-16T09:31:40.048354Z","shell.execute_reply.started":"2022-01-16T09:31:40.013360Z","shell.execute_reply":"2022-01-16T09:31:40.047448Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"class GlobalParametersTrain:\n    lr =0.0001\n    SRC_VOCAB_SIZE = 232\n    TARGET_VOCAB_SIZE = 232\n    num_epochs = 10\n    model_dim = 256\n    feed_forward_dim = 1024\n    num_layers = 6\n    max_len = 40\n    PAD_IDX = 0\n    SOS_IDX = 1\n    EOS_IDX = 2 \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n    train_loss = []\n    valid_loss = []\n    valid_acc = []\n    epoch = []\n    \n    folder = './model'","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:31:40.049430Z","iopub.execute_input":"2022-01-16T09:31:40.049657Z","iopub.status.idle":"2022-01-16T09:31:40.056363Z","shell.execute_reply.started":"2022-01-16T09:31:40.049614Z","shell.execute_reply":"2022-01-16T09:31:40.055438Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"config = GlobalParametersTrain()\n# model = Seq2SeqTransformer(SRC_VOCAB_SIZE, TARGET_VOCAB_SIZE, PAD_IDX, PAD_IDX, pes, device).to(device)\nmodel = TransformerModel(config.SRC_VOCAB_SIZE, config.model_dim, config.feed_forward_dim, config.TARGET_VOCAB_SIZE, config.num_layers,\n                            config.max_len, config.PAD_IDX, config.SOS_IDX, config.EOS_IDX).to(config.device)\ntraining = Trainer(model.to(config.device), device=config.device, config=config)\ntraining.train_loop(train_loader, val_loader, False)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:31:40.057859Z","iopub.execute_input":"2022-01-16T09:31:40.058714Z","iopub.status.idle":"2022-01-16T09:34:37.112362Z","shell.execute_reply.started":"2022-01-16T09:31:40.058671Z","shell.execute_reply":"2022-01-16T09:34:37.111077Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}