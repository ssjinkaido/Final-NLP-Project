{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import itertools\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport time\nfrom tqdm.notebook import tqdm\nimport pandas as pd\nimport nltk\nfrom nltk.tokenize import word_tokenize\nimport unidecode\nimport codecs\nimport pickle\nimport string\nimport random\nfrom tqdm.notebook import tqdm\nfrom transformers import AdamW\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, OneCycleLR, CosineAnnealingLR\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler, Dataset\nfrom torch.cuda.amp import autocast, GradScaler\nimport numpy as np\nimport torch\nfrom torch.jit import script, trace\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\nimport math\nfrom collections import Counter","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-16T09:10:31.537507Z","iopub.execute_input":"2022-01-16T09:10:31.537993Z","iopub.status.idle":"2022-01-16T09:10:39.672917Z","shell.execute_reply.started":"2022-01-16T09:10:31.537907Z","shell.execute_reply":"2022-01-16T09:10:39.671707Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nset_seed(seed = 1)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:10:39.674469Z","iopub.execute_input":"2022-01-16T09:10:39.674726Z","iopub.status.idle":"2022-01-16T09:10:39.682538Z","shell.execute_reply.started":"2022-01-16T09:10:39.674696Z","shell.execute_reply":"2022-01-16T09:10:39.681812Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#borrow from https://github.com/hisiter97/Spelling_Correction_Vietnamese/blob/master/dataset/add_noise.py\nclass SynthesizeData(object):\n    \"\"\"\n    Uitils class to create artificial miss-spelled words\n    Args:\n        vocab_path: path to vocab file. Vocab file is expected to be a set of words, separate by ' ', no newline charactor.\n    \"\"\"\n\n    def __init__(self, vocab_path=\"\"):\n\n        # self.vocab = open(vocab_path, 'r', encoding = 'utf-8').read().split()\n        self.tokenizer = word_tokenize\n        self.word_couples = [['sương', 'xương'], ['sĩ', 'sỹ'], ['sẽ', 'sẻ'], ['sã', 'sả'], ['sả', 'xả'], ['sẽ', 'sẻ'],\n                             ['mùi', 'muồi'],\n                             ['chỉnh', 'chỉn'], ['sữa', 'sửa'], ['chuẩn', 'chẩn'], ['lẻ', 'lẽ'], ['chẳng', 'chẵng'],\n                             ['cổ', 'cỗ'],\n                             ['sát', 'xát'], ['cập', 'cặp'], ['truyện', 'chuyện'], ['xá', 'sá'], ['giả', 'dả'],\n                             ['đỡ', 'đở'],\n                             ['giữ', 'dữ'], ['giã', 'dã'], ['xảo', 'sảo'], ['kiểm', 'kiễm'], ['cuộc', 'cục'],\n                             ['dạng', 'dạn'],\n                             ['tản', 'tảng'], ['ngành', 'nghành'], ['nghề', 'ngề'], ['nổ', 'nỗ'], ['rảnh', 'rãnh'],\n                             ['sẵn', 'sẳn'],\n                             ['sáng', 'xán'], ['xuất', 'suất'], ['suôn', 'suông'], ['sử', 'xử'], ['sắc', 'xắc'],\n                             ['chữa', 'chửa'],\n                             ['thắn', 'thắng'], ['dỡ', 'dở'], ['trải', 'trãi'], ['trao', 'trau'], ['trung', 'chung'],\n                             ['thăm', 'tham'],\n                             ['sét', 'xét'], ['dục', 'giục'], ['tả', 'tã'], ['sông', 'xông'], ['sáo', 'xáo'],\n                             ['sang', 'xang'],\n                             ['ngã', 'ngả'], ['xuống', 'suống'], ['xuồng', 'suồng']]\n\n        self.vn_alphabet = ['a', 'ă', 'â', 'b', 'c', 'd', 'đ', 'e', 'ê', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'ô',\n                            'ơ', 'p', 'q', 'r', 's', 't', 'u', 'ư', 'v', 'x', 'y']\n        self.alphabet_len = len(self.vn_alphabet)\n        self.char_couples = [['i', 'y'], ['s', 'x'], ['gi', 'd'],\n                             ['ă', 'â'], ['ch', 'tr'], ['ng', 'n'],\n                             ['nh', 'n'], ['ngh', 'ng'], ['ục', 'uộc'], ['o', 'u'],\n                             ['ă', 'a'], ['o', 'ô'], ['ả', 'ã'], ['ổ', 'ỗ'], ['ủ', 'ũ'], ['ễ', 'ể'],\n                             ['e', 'ê'], ['à', 'ờ'], ['ằ', 'à'], ['ẩn', 'uẩn'], ['ẽ', 'ẻ'], ['ùi', 'uồi'], ['ă', 'â'],\n                             ['ở', 'ỡ'], ['ỹ', 'ỷ'], ['ỉ', 'ĩ'], ['ị', 'ỵ'],\n                             ['ấ', 'á'], ['n', 'l'], ['qu', 'w'], ['ph', 'f'], ['d', 'z'], ['c', 'k'], ['qu', 'q'],\n                             ['i', 'j'], ['gi', 'j'],\n                             ]\n\n        self.teencode_dict = {'mình': ['mk', 'mik', 'mjk'], 'vô': ['zô', 'zo', 'vo'], 'vậy': ['zậy', 'z', 'zay', 'za'],\n                              'phải': ['fải', 'fai', ], 'biết': ['bit', 'biet'],\n                              'rồi': ['rùi', 'ròi', 'r'], 'bây': ['bi', 'bay'], 'giờ': ['h', ],\n                              'không': ['k', 'ko', 'khong', 'hk', 'hong', 'hông', '0', 'kg', 'kh', ],\n                              'đi': ['di', 'dj', ], 'gì': ['j', ], 'em': ['e', ], 'được': ['dc', 'đc', ], 'tao': ['t'],\n                              'tôi': ['t'], 'chồng': ['ck'], 'vợ': ['vk']\n\n                              }\n\n        # self.typo={\"ă\":\"aw\",\"â\":\"aa\",\"á\":\"as\",\"à\":\"af\",\"ả\":\"ar\",\"ã\":\"ax\",\"ạ\":\"aj\",\"ắ\":\"aws\",\"ổ\":\"oor\",\"ỗ\":\"oox\",\"ộ\":\"ooj\",\"ơ\":\"ow\",\n        #           \"ằ\":\"awf\",\"ẳ\":\"awr\",\"ẵ\":\"awx\",\"ặ\":\"awj\",\"ó\":\"os\",\"ò\":\"of\",\"ỏ\":\"or\",\"õ\":\"ox\",\"ọ\":\"oj\",\"ô\":\"oo\",\"ố\":\"oos\",\"ồ\":\"oof\",\n        #           \"ớ\":\"ows\",\"ờ\":\"owf\",\"ở\":\"owr\",\"ỡ\":\"owx\",\"ợ\":\"owj\",\"é\":\"es\",\"è\":\"ef\",\"ẻ\":\"er\",\"ẽ\":\"ex\",\"ẹ\":\"ej\",\"ê\":\"ee\",\"ế\":\"ees\",\"ề\":\"eef\",\n        #           \"ể\":\"eer\",\"ễ\":\"eex\",\"ệ\":\"eej\",\"ú\":\"us\",\"ù\":\"uf\",\"ủ\":\"ur\",\"ũ\":\"ux\",\"ụ\":\"uj\",\"ư\":\"uw\",\"ứ\":\"uws\",\"ừ\":\"uwf\",\"ử\":\"uwr\",\"ữ\":\"uwx\",\n        #           \"ự\":\"uwj\",\"í\":\"is\",\"ì\":\"if\",\"ỉ\":\"ir\",\"ị\":\"ij\",\"ĩ\":\"ix\",\"ý\":\"ys\",\"ỳ\":\"yf\",\"ỷ\":\"yr\",\"ỵ\":\"yj\",\"đ\":\"dd\",\n        #           \"Ă\":\"Aw\",\"Â\":\"Aa\",\"Á\":\"As\",\"À\":\"Af\",\"Ả\":\"Ar\",\"Ã\":\"Ax\",\"Ạ\":\"Aj\",\"Ắ\":\"Aws\",\"Ổ\":\"Oor\",\"Ỗ\":\"Oox\",\"Ộ\":\"Ooj\",\"Ơ\":\"Ow\",\n        #           \"Ằ\":\"AWF\",\"Ẳ\":\"Awr\",\"Ẵ\":\"Awx\",\"Ặ\":\"Awj\",\"Ó\":\"Os\",\"Ò\":\"Of\",\"Ỏ\":\"Or\",\"Õ\":\"Ox\",\"Ọ\":\"Oj\",\"Ô\":\"Oo\",\"Ố\":\"Oos\",\"Ồ\":\"Oof\",\n        #           \"Ớ\":\"Ows\",\"Ờ\":\"Owf\",\"Ở\":\"Owr\",\"Ỡ\":\"Owx\",\"Ợ\":\"Owj\",\"É\":\"Es\",\"È\":\"Ef\",\"Ẻ\":\"Er\",\"Ẽ\":\"Ex\",\"Ẹ\":\"Ej\",\"Ê\":\"Ee\",\"Ế\":\"Ees\",\"Ề\":\"Eef\",\n        #           \"Ể\":\"Eer\",\"Ễ\":\"Eex\",\"Ệ\":\"Eej\",\"Ú\":\"Us\",\"Ù\":\"Uf\",\"Ủ\":\"Ur\",\"Ũ\":\"Ux\",\"Ụ\":\"Uj\",\"Ư\":\"Uw\",\"Ứ\":\"Uws\",\"Ừ\":\"Uwf\",\"Ử\":\"Uwr\",\"Ữ\":\"Uwx\",\n        #           \"Ự\":\"Uwj\",\"Í\":\"Is\",\"Ì\":\"If\",\"Ỉ\":\"Ir\",\"Ị\":\"Ij\",\"Ĩ\":\"Ix\",\"Ý\":\"Ys\",\"Ỳ\":\"Yf\",\"Ỷ\":\"Yr\",\"Ỵ\":\"Yj\",\"Đ\":\"Dd\"}\n        self.typo = {\"ă\": [\"aw\", \"a8\"], \"â\": [\"aa\", \"a6\"], \"á\": [\"as\", \"a1\"], \"à\": [\"af\", \"a2\"], \"ả\": [\"ar\", \"a3\"],\n                     \"ã\": [\"ax\", \"a4\"], \"ạ\": [\"aj\", \"a5\"], \"ắ\": [\"aws\", \"ă1\"], \"ổ\": [\"oor\", \"ô3\"], \"ỗ\": [\"oox\", \"ô4\"],\n                     \"ộ\": [\"ooj\", \"ô5\"], \"ơ\": [\"ow\", \"o7\"],\n                     \"ằ\": [\"awf\", \"ă2\"], \"ẳ\": [\"awr\", \"ă3\"], \"ẵ\": [\"awx\", \"ă4\"], \"ặ\": [\"awj\", \"ă5\"], \"ó\": [\"os\", \"o1\"],\n                     \"ò\": [\"of\", \"o2\"], \"ỏ\": [\"or\", \"o3\"], \"õ\": [\"ox\", \"o4\"], \"ọ\": [\"oj\", \"o5\"], \"ô\": [\"oo\", \"o6\"],\n                     \"ố\": [\"oos\", \"ô1\"], \"ồ\": [\"oof\", \"ô2\"],\n                     \"ớ\": [\"ows\", \"ơ1\"], \"ờ\": [\"owf\", \"ơ2\"], \"ở\": [\"owr\", \"ơ2\"], \"ỡ\": [\"owx\", \"ơ4\"], \"ợ\": [\"owj\", \"ơ5\"],\n                     \"é\": [\"es\", \"e1\"], \"è\": [\"ef\", \"e2\"], \"ẻ\": [\"er\", \"e3\"], \"ẽ\": [\"ex\", \"e4\"], \"ẹ\": [\"ej\", \"e5\"],\n                     \"ê\": [\"ee\", \"e6\"], \"ế\": [\"ees\", \"ê1\"], \"ề\": [\"eef\", \"ê2\"],\n                     \"ể\": [\"eer\", \"ê3\"], \"ễ\": [\"eex\", \"ê3\"], \"ệ\": [\"eej\", \"ê5\"], \"ú\": [\"us\", \"u1\"], \"ù\": [\"uf\", \"u2\"],\n                     \"ủ\": [\"ur\", \"u3\"], \"ũ\": [\"ux\", \"u4\"], \"ụ\": [\"uj\", \"u5\"], \"ư\": [\"uw\", \"u7\"], \"ứ\": [\"uws\", \"ư1\"],\n                     \"ừ\": [\"uwf\", \"ư2\"], \"ử\": [\"uwr\", \"ư3\"], \"ữ\": [\"uwx\", \"ư4\"],\n                     \"ự\": [\"uwj\", \"ư5\"], \"í\": [\"is\", \"i1\"], \"ì\": [\"if\", \"i2\"], \"ỉ\": [\"ir\", \"i3\"], \"ị\": [\"ij\", \"i5\"],\n                     \"ĩ\": [\"ix\", \"i4\"], \"ý\": [\"ys\", \"y1\"], \"ỳ\": [\"yf\", \"y2\"], \"ỷ\": [\"yr\", \"y3\"], \"ỵ\": [\"yj\", \"y5\"],\n                     \"đ\": [\"dd\", \"d9\"],\n                     \"Ă\": [\"Aw\", \"A8\"], \"Â\": [\"Aa\", \"A6\"], \"Á\": [\"As\", \"A1\"], \"À\": [\"Af\", \"A2\"], \"Ả\": [\"Ar\", \"A3\"],\n                     \"Ã\": [\"Ax\", \"A4\"], \"Ạ\": [\"Aj\", \"A5\"], \"Ắ\": [\"Aws\", \"Ă1\"], \"Ổ\": [\"Oor\", \"Ô3\"], \"Ỗ\": [\"Oox\", \"Ô4\"],\n                     \"Ộ\": [\"Ooj\", \"Ô5\"], \"Ơ\": [\"Ow\", \"O7\"],\n                     \"Ằ\": [\"AWF\", \"Ă2\"], \"Ẳ\": [\"Awr\", \"Ă3\"], \"Ẵ\": [\"Awx\", \"Ă4\"], \"Ặ\": [\"Awj\", \"Ă5\"], \"Ó\": [\"Os\", \"O1\"],\n                     \"Ò\": [\"Of\", \"O2\"], \"Ỏ\": [\"Or\", \"O3\"], \"Õ\": [\"Ox\", \"O4\"], \"Ọ\": [\"Oj\", \"O5\"], \"Ô\": [\"Oo\", \"O6\"],\n                     \"Ố\": [\"Oos\", \"Ô1\"], \"Ồ\": [\"Oof\", \"Ô2\"],\n                     \"Ớ\": [\"Ows\", \"Ơ1\"], \"Ờ\": [\"Owf\", \"Ơ2\"], \"Ở\": [\"Owr\", \"Ơ3\"], \"Ỡ\": [\"Owx\", \"Ơ4\"], \"Ợ\": [\"Owj\", \"Ơ5\"],\n                     \"É\": [\"Es\", \"E1\"], \"È\": [\"Ef\", \"E2\"], \"Ẻ\": [\"Er\", \"E3\"], \"Ẽ\": [\"Ex\", \"E4\"], \"Ẹ\": [\"Ej\", \"E5\"],\n                     \"Ê\": [\"Ee\", \"E6\"], \"Ế\": [\"Ees\", \"Ê1\"], \"Ề\": [\"Eef\", \"Ê2\"],\n                     \"Ể\": [\"Eer\", \"Ê3\"], \"Ễ\": [\"Eex\", \"Ê4\"], \"Ệ\": [\"Eej\", \"Ê5\"], \"Ú\": [\"Us\", \"U1\"], \"Ù\": [\"Uf\", \"U2\"],\n                     \"Ủ\": [\"Ur\", \"U3\"], \"Ũ\": [\"Ux\", \"U4\"], \"Ụ\": [\"Uj\", \"U5\"], \"Ư\": [\"Uw\", \"U7\"], \"Ứ\": [\"Uws\", \"Ư1\"],\n                     \"Ừ\": [\"Uwf\", \"Ư2\"], \"Ử\": [\"Uwr\", \"Ư3\"], \"Ữ\": [\"Uwx\", \"Ư4\"],\n                     \"Ự\": [\"Uwj\", \"Ư5\"], \"Í\": [\"Is\", \"I1\"], \"Ì\": [\"If\", \"I2\"], \"Ỉ\": [\"Ir\", \"I3\"], \"Ị\": [\"Ij\", \"I5\"],\n                     \"Ĩ\": [\"Ix\", \"I4\"], \"Ý\": [\"Ys\", \"Y1\"], \"Ỳ\": [\"Yf\", \"Y2\"], \"Ỷ\": [\"Yr\", \"Y3\"], \"Ỵ\": [\"Yj\", \"Y5\"],\n                     \"Đ\": [\"Dd\", \"D9\"]}\n        self.all_word_candidates = self.get_all_word_candidates(self.word_couples)\n        self.string_all_word_candidates = ' '.join(self.all_word_candidates)\n        self.all_char_candidates = self.get_all_char_candidates()\n        self.keyboardNeighbors = self.getKeyboardNeighbors()\n\n    def replace_teencode(self, word):\n        candidates = self.teencode_dict.get(word, None)\n        if candidates is not None:\n            chosen_one = 0\n            if len(candidates) > 1:\n                chosen_one = np.random.randint(0, len(candidates))\n            return candidates[chosen_one]\n\n    def getKeyboardNeighbors(self):\n        keyboardNeighbors = {}\n        keyboardNeighbors['a'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ă'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['â'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['á'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['à'] = \"aáàảãăắằẳẵâấầẩẫ\"\n        keyboardNeighbors['ả'] = \"aảã\"\n        keyboardNeighbors['ã'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ạ'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ắ'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ằ'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ẳ'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ặ'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ẵ'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ấ'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ầ'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ẩ'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ẫ'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ậ'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['b'] = \"bh\"\n        keyboardNeighbors['c'] = \"cgn\"\n        keyboardNeighbors['d'] = \"đctơở\"\n        keyboardNeighbors['đ'] = \"d\"\n        keyboardNeighbors['e'] = \"eéèẻẽẹêếềểễệbpg\"\n        keyboardNeighbors['é'] = \"eéèẻẽẹêếềểễệ\"\n        keyboardNeighbors['è'] = \"eéèẻẽẹêếềểễệ\"\n        keyboardNeighbors['ẻ'] = \"eéèẻẽẹêếềểễệ\"\n        keyboardNeighbors['ẽ'] = \"eéèẻẽẹêếềểễệ\"\n        keyboardNeighbors['ẹ'] = \"eéèẻẽẹêếềểễệ\"\n        keyboardNeighbors['ê'] = \"eéèẻẽẹêếềểễệá\"\n        keyboardNeighbors['ế'] = \"eéèẻẽẹêếềểễệố\"\n        keyboardNeighbors['ề'] = \"eéèẻẽẹêếềểễệ\"\n        keyboardNeighbors['ể'] = \"eéèẻẽẹêếềểễệôốồổỗộ\"\n        keyboardNeighbors['ễ'] = \"eéèẻẽẹêếềểễệ\"\n        keyboardNeighbors['ệ'] = \"eéèẻẽẹêếềểễệ\"\n        keyboardNeighbors['g'] = \"qgộ\"\n        keyboardNeighbors['h'] = \"h\"\n        keyboardNeighbors['i'] = \"iíìỉĩịat\"\n        keyboardNeighbors['í'] = \"iíìỉĩị\"\n        keyboardNeighbors['ì'] = \"iíìỉĩị\"\n        keyboardNeighbors['ỉ'] = \"iíìỉĩị\"\n        keyboardNeighbors['ĩ'] = \"iíìỉĩị\"\n        keyboardNeighbors['ị'] = \"iíìỉĩịhự\"\n        keyboardNeighbors['k'] = \"klh\"\n        keyboardNeighbors['l'] = \"ljidđ\"\n        keyboardNeighbors['m'] = \"mn\"\n        keyboardNeighbors['n'] = \"mnedư\"\n        keyboardNeighbors['o'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ó'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ò'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ỏ'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['õ'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ọ'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ô'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ố'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ồ'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ổ'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ộ'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ỗ'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ơ'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ớ'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ờ'] = \"oóòỏọõôốồổỗộơớờởợỡà\"\n        keyboardNeighbors['ở'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ợ'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ỡ'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        # keyboardNeighbors['p'] = \"op\"\n        # keyboardNeighbors['q'] = \"qọ\"\n        # keyboardNeighbors['r'] = \"rht\"\n        # keyboardNeighbors['s'] = \"s\"\n        # keyboardNeighbors['t'] = \"tp\"\n        keyboardNeighbors['u'] = \"uúùủũụưứừữửựhiaạt\"\n        keyboardNeighbors['ú'] = \"uúùủũụưứừữửự\"\n        keyboardNeighbors['ù'] = \"uúùủũụưứừữửự\"\n        keyboardNeighbors['ủ'] = \"uúùủũụưứừữửự\"\n        keyboardNeighbors['ũ'] = \"uúùủũụưứừữửự\"\n        keyboardNeighbors['ụ'] = \"uúùủũụưứừữửự\"\n        keyboardNeighbors['ư'] = \"uúùủũụưứừữửựg\"\n        keyboardNeighbors['ứ'] = \"uúùủũụưứừữửự\"\n        keyboardNeighbors['ừ'] = \"uúùủũụưứừữửự\"\n        keyboardNeighbors['ử'] = \"uúùủũụưứừữửự\"\n        keyboardNeighbors['ữ'] = \"uúùủũụưứừữửự\"\n        keyboardNeighbors['ự'] = \"uúùủũụưứừữửựg\"\n        keyboardNeighbors['v'] = \"v\"\n        keyboardNeighbors['x'] = \"x\"\n        keyboardNeighbors['y'] = \"yýỳỷỵỹụ\"\n        keyboardNeighbors['ý'] = \"yýỳỷỵỹ\"\n        keyboardNeighbors['ỳ'] = \"yýỳỷỵỹ\"\n        keyboardNeighbors['ỷ'] = \"yýỳỷỵỹ\"\n        keyboardNeighbors['ỵ'] = \"yýỳỷỵỹ\"\n        keyboardNeighbors['ỹ'] = \"yýỳỷỵỹ\"\n        # keyboardNeighbors['w'] = \"wv\"\n        # keyboardNeighbors['j'] = \"jli\"\n        # keyboardNeighbors['z'] = \"zs\"\n        # keyboardNeighbors['f'] = \"ft\"\n\n        return keyboardNeighbors\n\n    def replace_char_noaccent(self, text, onehot_label):\n\n        # find index noise\n        idx = np.random.randint(0, len(onehot_label))\n        prevent_loop = 0\n        while onehot_label[idx] == 1 or text[idx].isnumeric() or text[idx] in string.punctuation:\n            idx = np.random.randint(0, len(onehot_label))\n            prevent_loop += 1\n            if prevent_loop > 10:\n                return False, text, onehot_label\n\n        index_noise = idx\n        onehot_label[index_noise] = 1\n        word_noise = text[index_noise]\n        for id in range(0, len(word_noise)):\n            char = word_noise[id]\n\n            if char in self.keyboardNeighbors:\n                neighbors = self.keyboardNeighbors[char]\n                idx_neigh = np.random.randint(0, len(neighbors))\n                replaced = neighbors[idx_neigh]\n                word_noise = word_noise[: id] + replaced + word_noise[id + 1:]\n                text[index_noise] = word_noise\n                return True, text, onehot_label\n\n        return False, text, onehot_label\n\n    def replace_word_candidate(self, word):\n        \"\"\"\n        Return a homophone word of the input word.\n        \"\"\"\n        capital_flag = word[0].isupper()\n        word = word.lower()\n        if capital_flag and word in self.teencode_dict:\n            return self.replace_teencode(word).capitalize()\n        elif word in self.teencode_dict:\n            return self.replace_teencode(word)\n\n        for couple in self.word_couples:\n            for i in range(2):\n                if couple[i] == word:\n                    if i == 0:\n                        if capital_flag:\n                            return couple[1].capitalize()\n                        else:\n                            return couple[1]\n                    else:\n                        if capital_flag:\n                            return couple[0].capitalize()\n                        else:\n                            return couple[0]\n\n    def replace_char_candidate(self, char):\n        \"\"\"\n        return a homophone char/subword of the input char.\n        \"\"\"\n        for couple in self.char_couples:\n            for i in range(2):\n                if couple[i] == char:\n                    if i == 0:\n                        return couple[1]\n                    else:\n                        return couple[0]\n\n    def replace_char_candidate_typo(self, char):\n        \"\"\"\n        return a homophone char/subword of the input char.\n        \"\"\"\n        i = np.random.randint(0, 2)\n\n        return self.typo[char][i]\n\n    def get_all_char_candidates(self, ):\n\n        all_char_candidates = []\n        for couple in self.char_couples:\n            all_char_candidates.extend(couple)\n        return all_char_candidates\n\n    def get_all_word_candidates(self, word_couples):\n\n        all_word_candidates = []\n        for couple in self.word_couples:\n            all_word_candidates.extend(couple)\n        return all_word_candidates\n\n    def remove_diacritics(self, text, onehot_label):\n        \"\"\"\n        Replace word which has diacritics with the same word without diacritics\n        Args:\n            text: a list of word tokens\n            onehot_label: onehot array indicate position of word that has already modify, so this\n            function only choose the word that do not has onehot label == 1.\n        return: a list of word tokens has one word that its diacritics was removed,\n                a list of onehot label indicate the position of words that has been modified.\n        \"\"\"\n        idx = np.random.randint(0, len(onehot_label))\n        prevent_loop = 0\n        while onehot_label[idx] == 1 or text[idx] == unidecode.unidecode(text[idx]) or text[idx] in string.punctuation:\n            idx = np.random.randint(0, len(onehot_label))\n            prevent_loop += 1\n            if prevent_loop > 10:\n                return False, text, onehot_label\n\n        onehot_label[idx] = 1\n        text[idx] = unidecode.unidecode(text[idx])\n        return True, text, onehot_label\n\n    def replace_with_random_letter(self, text, onehot_label):\n        \"\"\"\n        Replace, add (or remove) a random letter in a random chosen word with a random letter\n        Args:\n            text: a list of word tokens\n            onehot_label: onehot array indicate position of word that has already modify, so this\n            function only choose the word that do not has onehot label == 1.\n        return: a list of word tokens has one word that has been modified,\n                a list of onehot label indicate the position of words that has been modified.\n        \"\"\"\n        idx = np.random.randint(0, len(onehot_label))\n        prevent_loop = 0\n        while onehot_label[idx] == 1 or text[idx].isnumeric() or text[idx] in string.punctuation:\n            idx = np.random.randint(0, len(onehot_label))\n            prevent_loop += 1\n            if prevent_loop > 10:\n                return False, text, onehot_label\n\n        # replace, add or remove? 0 is replace, 1 is add, 2 is remove\n        coin = np.random.choice([0, 1, 2])\n        if coin == 0:\n            chosen_letter = text[idx][np.random.randint(0, len(text[idx]))]\n            replaced = self.vn_alphabet[np.random.randint(0, self.alphabet_len)]\n            try:\n                text[idx] = re.sub(chosen_letter, replaced, text[idx])\n            except:\n                return False, text, onehot_label\n        elif coin == 1:\n            chosen_letter = text[idx][np.random.randint(0, len(text[idx]))]\n            replaced = chosen_letter + self.vn_alphabet[np.random.randint(0, self.alphabet_len)]\n            try:\n                text[idx] = re.sub(chosen_letter, replaced, text[idx])\n            except:\n                return False, text, onehot_label\n        else:\n            chosen_letter = text[idx][np.random.randint(0, len(text[idx]))]\n            try:\n                text[idx] = re.sub(chosen_letter, '', text[idx])\n            except:\n                return False, text, onehot_label\n\n        onehot_label[idx] = 1\n        return True, text, onehot_label\n\n    def replace_with_homophone_word(self, text, onehot_label):\n        \"\"\"\n        Replace a candidate word (if exist in the word_couple) with its homophone. if successful, return True, else False\n        Args:\n            text: a list of word tokens\n            onehot_label: onehot array indicate position of word that has already modify, so this\n            function only choose the word that do not has onehot label == 1.\n        return: True, text, onehot_label if successful replace, else False, text, onehot_label\n        \"\"\"\n        # account for the case that the word in the text is upper case but its lowercase match the candidates list\n        candidates = []\n        for i in range(len(text)):\n            if text[i].lower() in self.all_word_candidates or text[i].lower() in self.teencode_dict.keys():\n                candidates.append((i, text[i]))\n\n        if len(candidates) == 0:\n            return False, text, onehot_label\n\n        idx = np.random.randint(0, len(candidates))\n        prevent_loop = 0\n        while onehot_label[candidates[idx][0]] == 1:\n            idx = np.random.choice(np.arange(0, len(candidates)))\n            prevent_loop += 1\n            if prevent_loop > 5:\n                return False, text, onehot_label\n\n        text[candidates[idx][0]] = self.replace_word_candidate(candidates[idx][1])\n        onehot_label[candidates[idx][0]] = 1\n        return True, text, onehot_label\n\n    def replace_with_homophone_letter(self, text, onehot_label):\n        \"\"\"\n        Replace a subword/letter with its homophones\n        Args:\n            text: a list of word tokens\n            onehot_label: onehot array indicate position of word that has already modify, so this\n            function only choose the word that do not has onehot label == 1.\n        return: True, text, onehot_label if successful replace, else False, None, None\n        \"\"\"\n        candidates = []\n        for i in range(len(text)):\n            for char in self.all_char_candidates:\n                if re.search(char, text[i]) is not None:\n                    candidates.append((i, char))\n                    break\n\n        if len(candidates) == 0:\n\n            return False, text, onehot_label\n        else:\n            idx = np.random.randint(0, len(candidates))\n            prevent_loop = 0\n            while onehot_label[candidates[idx][0]] == 1:\n                idx = np.random.randint(0, len(candidates))\n                prevent_loop += 1\n                if prevent_loop > 5:\n                    return False, text, onehot_label\n\n            replaced = self.replace_char_candidate(candidates[idx][1])\n            text[candidates[idx][0]] = re.sub(candidates[idx][1], replaced, text[candidates[idx][0]])\n\n            onehot_label[candidates[idx][0]] = 1\n            return True, text, onehot_label\n\n    def replace_with_typo_letter(self, text, onehot_label):\n        \"\"\"\n        Replace a subword/letter with its homophones\n        Args:\n            text: a list of word tokens\n            onehot_label: onehot array indicate position of word that has already modify, so this\n            function only choose the word that do not has onehot label == 1.\n        return: True, text, onehot_label if successful replace, else False, None, None\n        \"\"\"\n        # find index noise\n        idx = np.random.randint(0, len(onehot_label))\n        prevent_loop = 0\n        while onehot_label[idx] == 1 or text[idx].isnumeric() or text[idx] in string.punctuation:\n            idx = np.random.randint(0, len(onehot_label))\n            prevent_loop += 1\n            if prevent_loop > 10:\n                return False, text, onehot_label\n\n        index_noise = idx\n        onehot_label[index_noise] = 1\n\n        word_noise = text[index_noise]\n        for j in range(0, len(word_noise)):\n            char = word_noise[j]\n\n            if char in self.typo:\n                replaced = self.replace_char_candidate_typo(char)\n                word_noise = word_noise[: j] + replaced + word_noise[j + 1:]\n                text[index_noise] = word_noise\n                return True, text, onehot_label\n        return True, text, onehot_label\n\n    def add_noise(self, sentence, percent_err=0.15, num_type_err=5):\n        tokens = self.tokenizer(sentence)\n        assert len(tokens) < 6\n        onehot_label = [0] * len(tokens)\n\n        num_wrong = int(np.ceil(percent_err * len(tokens)))\n        num_wrong = np.random.randint(1, num_wrong + 1)\n        if np.random.rand() < 0.05:\n            num_wrong = 0\n\n        for i in range(0, num_wrong):\n            err = np.random.randint(0, num_type_err + 1)\n\n            if err == 0:\n                _, tokens, onehot_label = self.replace_with_homophone_letter(tokens, onehot_label)\n            elif err == 1:\n                _, tokens, onehot_label = self.replace_with_typo_letter(tokens, onehot_label)\n            elif err == 2:\n                _, tokens, onehot_label = self.replace_with_homophone_word(tokens, onehot_label)\n            elif err == 3:\n                _, tokens, onehot_label = self.replace_with_random_letter(tokens, onehot_label)\n            elif err == 4:\n                _, tokens, onehot_label = self.remove_diacritics(tokens, onehot_label)\n            elif err == 5:\n                _, tokens, onehot_label = self.replace_char_noaccent(tokens, onehot_label)\n            else:\n                continue\n        return ' '.join(tokens), onehot_label","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:10:39.684983Z","iopub.execute_input":"2022-01-16T09:10:39.685456Z","iopub.status.idle":"2022-01-16T09:10:39.775279Z","shell.execute_reply.started":"2022-01-16T09:10:39.685410Z","shell.execute_reply":"2022-01-16T09:10:39.774277Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class Vocab(object):\n    def __init__(self):\n        self.word2idx = {}\n        self.idx2word ={}\n        self.idx = 0\n        self.go = 1\n        self.eos = 2\n\n    def add_word(self, word):\n        if not word in self.word2idx:\n            self.word2idx[word] = self.idx\n            self.idx2word[self.idx] = word\n            self.idx +=1\n\n    def __len__(self):\n        return len(self.word2idx)\n    \n    def encode(self, chars):\n        encode_sent = []\n        encode_sent.append(self.go)\n        for word in word_tokenize(chars):\n            if not word in self.word2idx:\n                encode_sent.append(self.word2idx['<unk>'])\n            else:\n                encode_sent.append(self.word2idx[word])\n        encode_sent.append(self.eos)        \n        while len(encode_sent)<7:\n            encode_sent.append(0)\n        \n        return encode_sent\n\n\n    def __call__(self, word):\n        if not word in self.word2idx:\n            return self.word2idx['<unk>']\n        return self.word2idx[word]","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:10:39.777209Z","iopub.execute_input":"2022-01-16T09:10:39.777960Z","iopub.status.idle":"2022-01-16T09:10:39.793191Z","shell.execute_reply.started":"2022-01-16T09:10:39.777914Z","shell.execute_reply":"2022-01-16T09:10:39.792414Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"synthesizer = SynthesizeData()\nlist_ngram_train = np.load('../input/5gram-nlp-project/list_5gram_nonum_train.npy')\nlist_ngram_train  = list_ngram_train [:1000000]\nlist_ngram_validate = np.load('../input/5gram-nlp-project/list_5gram_nonum_valid.npy')\nlist_ngram_validate  = list_ngram_validate [5000:15000]\ntrain_normal_captions = []\ntrain_lowercase_captions = []\ntrain_onehot_labels = []\ndef build_vocab(all_clean_captions):\n    counter = Counter()\n    for caption in tqdm(all_clean_captions):\n        caption, onehot_label = synthesizer.add_noise(caption)\n        train_normal_captions.append(caption)\n        train_onehot_labels.append(onehot_label)\n        train_lowercase_captions.append(caption.lower())\n        \nbuild_vocab(list_ngram_train)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:10:39.794137Z","iopub.execute_input":"2022-01-16T09:10:39.794924Z","iopub.status.idle":"2022-01-16T09:13:04.094505Z","shell.execute_reply.started":"2022-01-16T09:10:39.794889Z","shell.execute_reply":"2022-01-16T09:13:04.093591Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"valid_normal_captions = []\nvalid_onehot_labels = []\nvalid_lowercase_captions = []\ndef build_validation(all_clean_captions):\n    counter = Counter()\n    for caption in tqdm(all_clean_captions):\n        caption, onehot_label = synthesizer.add_noise(caption)\n        valid_normal_captions.append(caption)\n        valid_onehot_labels.append(onehot_label)\n        valid_lowercase_captions.append(caption.lower())\n\nbuild_validation(list_ngram_validate)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:13:04.095791Z","iopub.execute_input":"2022-01-16T09:13:04.096079Z","iopub.status.idle":"2022-01-16T09:13:05.581553Z","shell.execute_reply.started":"2022-01-16T09:13:04.096048Z","shell.execute_reply":"2022-01-16T09:13:05.580903Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"print(train_normal_captions[0:20])\nprint(train_onehot_labels[0:20])\nprint(train_lowercase_captions[0:20])\n","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:13:05.582364Z","iopub.execute_input":"2022-01-16T09:13:05.582597Z","iopub.status.idle":"2022-01-16T09:13:05.589661Z","shell.execute_reply.started":"2022-01-16T09:13:05.582567Z","shell.execute_reply":"2022-01-16T09:13:05.588652Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(valid_normal_captions[0:20])\nprint(valid_onehot_labels[0:20])\nprint(valid_lowercase_captions[0:20])","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:13:05.590815Z","iopub.execute_input":"2022-01-16T09:13:05.591017Z","iopub.status.idle":"2022-01-16T09:13:05.600578Z","shell.execute_reply.started":"2022-01-16T09:13:05.590994Z","shell.execute_reply":"2022-01-16T09:13:05.599813Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_ngrams(list_ngrams, save_path):\n    with open(save_path, 'wb') as f:\n        np.save(f, list_ngrams)\n    print(\"Saved dataset - ngrams\")\n    \nsave_ngrams(train_lowercase_captions, 'train_lowercase_captions.npy')\nsave_ngrams(valid_lowercase_captions, 'valid_lowercase_captions.npy')\nsave_ngrams(train_normal_captions, 'train_normal_captions.npy')\nsave_ngrams(valid_normal_captions, 'valid_normal_captions.npy')\nsave_ngrams(train_onehot_labels, 'train_onehot_labels.npy')\nsave_ngrams(valid_onehot_labels, 'valid_onehot_labels.npy')","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:13:05.602341Z","iopub.execute_input":"2022-01-16T09:13:05.602643Z","iopub.status.idle":"2022-01-16T09:13:07.590097Z","shell.execute_reply.started":"2022-01-16T09:13:05.602601Z","shell.execute_reply":"2022-01-16T09:13:07.589254Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# train_captions = np.load('./train_captions.npy')\n# print(train_captions[0:20])","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:13:07.592847Z","iopub.execute_input":"2022-01-16T09:13:07.593066Z","iopub.status.idle":"2022-01-16T09:13:07.596738Z","shell.execute_reply.started":"2022-01-16T09:13:07.593041Z","shell.execute_reply":"2022-01-16T09:13:07.595897Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# valid_captions = np.load('./valid_captions.npy')\n# print(train_captions[0:20])","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:13:07.598056Z","iopub.execute_input":"2022-01-16T09:13:07.598365Z","iopub.status.idle":"2022-01-16T09:13:07.609109Z","shell.execute_reply.started":"2022-01-16T09:13:07.598327Z","shell.execute_reply":"2022-01-16T09:13:07.608074Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# train_onehot_labels = np.load('./train_onehot_labels.npy', allow_pickle=True)\n# for i in tqdm(train_onehot_labels):\n#     if len(i)>5:\n#         print(\"Yes\")","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:13:07.610981Z","iopub.execute_input":"2022-01-16T09:13:07.611346Z","iopub.status.idle":"2022-01-16T09:13:07.618946Z","shell.execute_reply.started":"2022-01-16T09:13:07.611271Z","shell.execute_reply":"2022-01-16T09:13:07.618349Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# valid_onehot_labels = np.load('./valid_onehot_labels.npy', allow_pickle=True)\n# for i in tqdm(valid_onehot_labels):\n#     if len(i)>5:\n#         print(\"Yes\")","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:13:07.619767Z","iopub.execute_input":"2022-01-16T09:13:07.619983Z","iopub.status.idle":"2022-01-16T09:13:07.630091Z","shell.execute_reply.started":"2022-01-16T09:13:07.619948Z","shell.execute_reply":"2022-01-16T09:13:07.629477Z"},"trusted":true},"execution_count":13,"outputs":[]}]}