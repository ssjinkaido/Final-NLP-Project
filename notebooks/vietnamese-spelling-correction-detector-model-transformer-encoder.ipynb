{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import itertools\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport time\nfrom tqdm.notebook import tqdm\nimport pandas as pd\nimport nltk\nfrom nltk.tokenize import word_tokenize\nimport unidecode\nimport codecs\nimport pickle\nimport string\nimport random\nfrom tqdm.notebook import tqdm\nfrom transformers import AdamW\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, OneCycleLR, CosineAnnealingLR\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler, Dataset\nfrom torch.cuda.amp import autocast, GradScaler\nimport numpy as np\nimport torch\nfrom torch.jit import script, trace\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\nimport math\nfrom collections import Counter\n# nltk.download('punkt')\n# sentence_tokenizer  =  nltk.data.load('tokenizers/punkt/english.pickle')","metadata":{"papermill":{"duration":8.453995,"end_time":"2022-01-11T07:29:57.678228","exception":false,"start_time":"2022-01-11T07:29:49.224233","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-18T13:13:45.807939Z","iopub.execute_input":"2022-01-18T13:13:45.808225Z","iopub.status.idle":"2022-01-18T13:13:45.817215Z","shell.execute_reply.started":"2022-01-18T13:13:45.808193Z","shell.execute_reply":"2022-01-18T13:13:45.816432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nset_seed(seed = 1)","metadata":{"papermill":{"duration":0.051872,"end_time":"2022-01-11T07:29:57.770088","exception":false,"start_time":"2022-01-11T07:29:57.718216","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-18T13:13:45.867921Z","iopub.execute_input":"2022-01-18T13:13:45.868453Z","iopub.status.idle":"2022-01-18T13:13:45.874206Z","shell.execute_reply.started":"2022-01-18T13:13:45.868424Z","shell.execute_reply":"2022-01-18T13:13:45.873327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df= pd.read_csv('../input/kpdl-data/train_remove_noise.csv')\n# df.head()","metadata":{"papermill":{"duration":0.046693,"end_time":"2022-01-11T07:29:57.855442","exception":false,"start_time":"2022-01-11T07:29:57.808749","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-18T13:13:45.87633Z","iopub.execute_input":"2022-01-18T13:13:45.877268Z","iopub.status.idle":"2022-01-18T13:13:45.882957Z","shell.execute_reply.started":"2022-01-18T13:13:45.877224Z","shell.execute_reply":"2022-01-18T13:13:45.882118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(len(df))","metadata":{"papermill":{"duration":0.048053,"end_time":"2022-01-11T07:29:57.943462","exception":false,"start_time":"2022-01-11T07:29:57.895409","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-18T13:13:45.884249Z","iopub.execute_input":"2022-01-18T13:13:45.886513Z","iopub.status.idle":"2022-01-18T13:13:45.89056Z","shell.execute_reply.started":"2022-01-18T13:13:45.88647Z","shell.execute_reply":"2022-01-18T13:13:45.889806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def create_stopwordlist():\n#     f = codecs.open('/kaggle/input/vietnamese-stopwords/vietnamese-stopwords.txt', encoding='utf-8')\n#     data = []\n#     null_data = []\n#     for i, line in enumerate(f):\n#         line = repr(line)\n#         line = line[1:len(line)-3]\n#         data.append(line)\n#     return data","metadata":{"papermill":{"duration":0.047412,"end_time":"2022-01-11T07:29:58.030125","exception":false,"start_time":"2022-01-11T07:29:57.982713","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-18T13:13:45.892928Z","iopub.execute_input":"2022-01-18T13:13:45.893843Z","iopub.status.idle":"2022-01-18T13:13:45.898155Z","shell.execute_reply.started":"2022-01-18T13:13:45.8938Z","shell.execute_reply":"2022-01-18T13:13:45.897387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# vn_stopwords = create_stopwordlist()\n# print(f\"Length of Vietnamese stopwords {len(vn_stopwords)}\")","metadata":{"papermill":{"duration":0.047141,"end_time":"2022-01-11T07:29:58.116657","exception":false,"start_time":"2022-01-11T07:29:58.069516","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-18T13:13:45.929004Z","iopub.execute_input":"2022-01-18T13:13:45.929224Z","iopub.status.idle":"2022-01-18T13:13:45.932123Z","shell.execute_reply.started":"2022-01-18T13:13:45.929199Z","shell.execute_reply":"2022-01-18T13:13:45.931419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df['content_length'] =df['Content'].apply(lambda x: len(x))\n# df['word_counts'] = df['Content'].apply(lambda x: len(x.split()))\n# df.head()","metadata":{"papermill":{"duration":0.049391,"end_time":"2022-01-11T07:29:58.206579","exception":false,"start_time":"2022-01-11T07:29:58.157188","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-18T13:13:45.9342Z","iopub.execute_input":"2022-01-18T13:13:45.934792Z","iopub.status.idle":"2022-01-18T13:13:45.941185Z","shell.execute_reply.started":"2022-01-18T13:13:45.93475Z","shell.execute_reply":"2022-01-18T13:13:45.940389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fig, axes = plt.subplots(2, figsize = (16,10))\n# plt.title(\"Word counts and content length distribution\")\n# sns.distplot(df['content_length'], ax = axes[0]);\n# sns.distplot(df['word_counts'], ax = axes[1])\n# plt.show()","metadata":{"papermill":{"duration":0.059907,"end_time":"2022-01-11T07:29:58.305509","exception":false,"start_time":"2022-01-11T07:29:58.245602","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-18T13:13:45.942675Z","iopub.execute_input":"2022-01-18T13:13:45.943325Z","iopub.status.idle":"2022-01-18T13:13:45.94853Z","shell.execute_reply.started":"2022-01-18T13:13:45.943222Z","shell.execute_reply":"2022-01-18T13:13:45.94769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import logging\n# def init_logger():\n#     logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n#                         datefmt='%m/%d/%Y %H:%M:%S',\n#                         level=logging.INFO)\n# init_logger()\n# logger = logging.getLogger(__name__)","metadata":{"papermill":{"duration":0.04784,"end_time":"2022-01-11T07:29:58.39245","exception":false,"start_time":"2022-01-11T07:29:58.34461","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-18T13:13:45.950092Z","iopub.execute_input":"2022-01-18T13:13:45.950431Z","iopub.status.idle":"2022-01-18T13:13:45.958315Z","shell.execute_reply.started":"2022-01-18T13:13:45.950358Z","shell.execute_reply":"2022-01-18T13:13:45.957557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#borrow from https://github.com/hisiter97/Spelling_Correction_Vietnamese/blob/master/dataset/add_noise.py\nclass SynthesizeData(object):\n    \"\"\"\n    Uitils class to create artificial miss-spelled words\n    Args:\n        vocab_path: path to vocab file. Vocab file is expected to be a set of words, separate by ' ', no newline charactor.\n    \"\"\"\n\n    def __init__(self, vocab_path=\"\"):\n\n        # self.vocab = open(vocab_path, 'r', encoding = 'utf-8').read().split()\n        self.tokenizer = word_tokenize\n        self.word_couples = [['sương', 'xương'], ['sĩ', 'sỹ'], ['sẽ', 'sẻ'], ['sã', 'sả'], ['sả', 'xả'], ['sẽ', 'sẻ'],\n                             ['mùi', 'muồi'],\n                             ['chỉnh', 'chỉn'], ['sữa', 'sửa'], ['chuẩn', 'chẩn'], ['lẻ', 'lẽ'], ['chẳng', 'chẵng'],\n                             ['cổ', 'cỗ'],\n                             ['sát', 'xát'], ['cập', 'cặp'], ['truyện', 'chuyện'], ['xá', 'sá'], ['giả', 'dả'],\n                             ['đỡ', 'đở'],\n                             ['giữ', 'dữ'], ['giã', 'dã'], ['xảo', 'sảo'], ['kiểm', 'kiễm'], ['cuộc', 'cục'],\n                             ['dạng', 'dạn'],\n                             ['tản', 'tảng'], ['ngành', 'nghành'], ['nghề', 'ngề'], ['nổ', 'nỗ'], ['rảnh', 'rãnh'],\n                             ['sẵn', 'sẳn'],\n                             ['sáng', 'xán'], ['xuất', 'suất'], ['suôn', 'suông'], ['sử', 'xử'], ['sắc', 'xắc'],\n                             ['chữa', 'chửa'],\n                             ['thắn', 'thắng'], ['dỡ', 'dở'], ['trải', 'trãi'], ['trao', 'trau'], ['trung', 'chung'],\n                             ['thăm', 'tham'],\n                             ['sét', 'xét'], ['dục', 'giục'], ['tả', 'tã'], ['sông', 'xông'], ['sáo', 'xáo'],\n                             ['sang', 'xang'],\n                             ['ngã', 'ngả'], ['xuống', 'suống'], ['xuồng', 'suồng']]\n\n        self.vn_alphabet = ['a', 'ă', 'â', 'b', 'c', 'd', 'đ', 'e', 'ê', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'ô',\n                            'ơ', 'p', 'q', 'r', 's', 't', 'u', 'ư', 'v', 'x', 'y']\n        self.alphabet_len = len(self.vn_alphabet)\n        self.char_couples = [['i', 'y'], ['s', 'x'], ['gi', 'd'],\n                             ['ă', 'â'], ['ch', 'tr'], ['ng', 'n'],\n                             ['nh', 'n'], ['ngh', 'ng'], ['ục', 'uộc'], ['o', 'u'],\n                             ['ă', 'a'], ['o', 'ô'], ['ả', 'ã'], ['ổ', 'ỗ'], ['ủ', 'ũ'], ['ễ', 'ể'],\n                             ['e', 'ê'], ['à', 'ờ'], ['ằ', 'à'], ['ẩn', 'uẩn'], ['ẽ', 'ẻ'], ['ùi', 'uồi'], ['ă', 'â'],\n                             ['ở', 'ỡ'], ['ỹ', 'ỷ'], ['ỉ', 'ĩ'], ['ị', 'ỵ'],\n                             ['ấ', 'á'], ['n', 'l'], ['qu', 'w'], ['ph', 'f'], ['d', 'z'], ['c', 'k'], ['qu', 'q'],\n                             ['i', 'j'], ['gi', 'j'],\n                             ]\n\n        self.teencode_dict = {'mình': ['mk', 'mik', 'mjk'], 'vô': ['zô', 'zo', 'vo'], 'vậy': ['zậy', 'z', 'zay', 'za'],\n                              'phải': ['fải', 'fai', ], 'biết': ['bit', 'biet'],\n                              'rồi': ['rùi', 'ròi', 'r'], 'bây': ['bi', 'bay'], 'giờ': ['h', ],\n                              'không': ['k', 'ko', 'khong', 'hk', 'hong', 'hông', '0', 'kg', 'kh', ],\n                              'đi': ['di', 'dj', ], 'gì': ['j', ], 'em': ['e', ], 'được': ['dc', 'đc', ], 'tao': ['t'],\n                              'tôi': ['t'], 'chồng': ['ck'], 'vợ': ['vk']\n\n                              }\n\n        # self.typo={\"ă\":\"aw\",\"â\":\"aa\",\"á\":\"as\",\"à\":\"af\",\"ả\":\"ar\",\"ã\":\"ax\",\"ạ\":\"aj\",\"ắ\":\"aws\",\"ổ\":\"oor\",\"ỗ\":\"oox\",\"ộ\":\"ooj\",\"ơ\":\"ow\",\n        #           \"ằ\":\"awf\",\"ẳ\":\"awr\",\"ẵ\":\"awx\",\"ặ\":\"awj\",\"ó\":\"os\",\"ò\":\"of\",\"ỏ\":\"or\",\"õ\":\"ox\",\"ọ\":\"oj\",\"ô\":\"oo\",\"ố\":\"oos\",\"ồ\":\"oof\",\n        #           \"ớ\":\"ows\",\"ờ\":\"owf\",\"ở\":\"owr\",\"ỡ\":\"owx\",\"ợ\":\"owj\",\"é\":\"es\",\"è\":\"ef\",\"ẻ\":\"er\",\"ẽ\":\"ex\",\"ẹ\":\"ej\",\"ê\":\"ee\",\"ế\":\"ees\",\"ề\":\"eef\",\n        #           \"ể\":\"eer\",\"ễ\":\"eex\",\"ệ\":\"eej\",\"ú\":\"us\",\"ù\":\"uf\",\"ủ\":\"ur\",\"ũ\":\"ux\",\"ụ\":\"uj\",\"ư\":\"uw\",\"ứ\":\"uws\",\"ừ\":\"uwf\",\"ử\":\"uwr\",\"ữ\":\"uwx\",\n        #           \"ự\":\"uwj\",\"í\":\"is\",\"ì\":\"if\",\"ỉ\":\"ir\",\"ị\":\"ij\",\"ĩ\":\"ix\",\"ý\":\"ys\",\"ỳ\":\"yf\",\"ỷ\":\"yr\",\"ỵ\":\"yj\",\"đ\":\"dd\",\n        #           \"Ă\":\"Aw\",\"Â\":\"Aa\",\"Á\":\"As\",\"À\":\"Af\",\"Ả\":\"Ar\",\"Ã\":\"Ax\",\"Ạ\":\"Aj\",\"Ắ\":\"Aws\",\"Ổ\":\"Oor\",\"Ỗ\":\"Oox\",\"Ộ\":\"Ooj\",\"Ơ\":\"Ow\",\n        #           \"Ằ\":\"AWF\",\"Ẳ\":\"Awr\",\"Ẵ\":\"Awx\",\"Ặ\":\"Awj\",\"Ó\":\"Os\",\"Ò\":\"Of\",\"Ỏ\":\"Or\",\"Õ\":\"Ox\",\"Ọ\":\"Oj\",\"Ô\":\"Oo\",\"Ố\":\"Oos\",\"Ồ\":\"Oof\",\n        #           \"Ớ\":\"Ows\",\"Ờ\":\"Owf\",\"Ở\":\"Owr\",\"Ỡ\":\"Owx\",\"Ợ\":\"Owj\",\"É\":\"Es\",\"È\":\"Ef\",\"Ẻ\":\"Er\",\"Ẽ\":\"Ex\",\"Ẹ\":\"Ej\",\"Ê\":\"Ee\",\"Ế\":\"Ees\",\"Ề\":\"Eef\",\n        #           \"Ể\":\"Eer\",\"Ễ\":\"Eex\",\"Ệ\":\"Eej\",\"Ú\":\"Us\",\"Ù\":\"Uf\",\"Ủ\":\"Ur\",\"Ũ\":\"Ux\",\"Ụ\":\"Uj\",\"Ư\":\"Uw\",\"Ứ\":\"Uws\",\"Ừ\":\"Uwf\",\"Ử\":\"Uwr\",\"Ữ\":\"Uwx\",\n        #           \"Ự\":\"Uwj\",\"Í\":\"Is\",\"Ì\":\"If\",\"Ỉ\":\"Ir\",\"Ị\":\"Ij\",\"Ĩ\":\"Ix\",\"Ý\":\"Ys\",\"Ỳ\":\"Yf\",\"Ỷ\":\"Yr\",\"Ỵ\":\"Yj\",\"Đ\":\"Dd\"}\n        self.typo = {\"ă\": [\"aw\", \"a8\"], \"â\": [\"aa\", \"a6\"], \"á\": [\"as\", \"a1\"], \"à\": [\"af\", \"a2\"], \"ả\": [\"ar\", \"a3\"],\n                     \"ã\": [\"ax\", \"a4\"], \"ạ\": [\"aj\", \"a5\"], \"ắ\": [\"aws\", \"ă1\"], \"ổ\": [\"oor\", \"ô3\"], \"ỗ\": [\"oox\", \"ô4\"],\n                     \"ộ\": [\"ooj\", \"ô5\"], \"ơ\": [\"ow\", \"o7\"],\n                     \"ằ\": [\"awf\", \"ă2\"], \"ẳ\": [\"awr\", \"ă3\"], \"ẵ\": [\"awx\", \"ă4\"], \"ặ\": [\"awj\", \"ă5\"], \"ó\": [\"os\", \"o1\"],\n                     \"ò\": [\"of\", \"o2\"], \"ỏ\": [\"or\", \"o3\"], \"õ\": [\"ox\", \"o4\"], \"ọ\": [\"oj\", \"o5\"], \"ô\": [\"oo\", \"o6\"],\n                     \"ố\": [\"oos\", \"ô1\"], \"ồ\": [\"oof\", \"ô2\"],\n                     \"ớ\": [\"ows\", \"ơ1\"], \"ờ\": [\"owf\", \"ơ2\"], \"ở\": [\"owr\", \"ơ2\"], \"ỡ\": [\"owx\", \"ơ4\"], \"ợ\": [\"owj\", \"ơ5\"],\n                     \"é\": [\"es\", \"e1\"], \"è\": [\"ef\", \"e2\"], \"ẻ\": [\"er\", \"e3\"], \"ẽ\": [\"ex\", \"e4\"], \"ẹ\": [\"ej\", \"e5\"],\n                     \"ê\": [\"ee\", \"e6\"], \"ế\": [\"ees\", \"ê1\"], \"ề\": [\"eef\", \"ê2\"],\n                     \"ể\": [\"eer\", \"ê3\"], \"ễ\": [\"eex\", \"ê3\"], \"ệ\": [\"eej\", \"ê5\"], \"ú\": [\"us\", \"u1\"], \"ù\": [\"uf\", \"u2\"],\n                     \"ủ\": [\"ur\", \"u3\"], \"ũ\": [\"ux\", \"u4\"], \"ụ\": [\"uj\", \"u5\"], \"ư\": [\"uw\", \"u7\"], \"ứ\": [\"uws\", \"ư1\"],\n                     \"ừ\": [\"uwf\", \"ư2\"], \"ử\": [\"uwr\", \"ư3\"], \"ữ\": [\"uwx\", \"ư4\"],\n                     \"ự\": [\"uwj\", \"ư5\"], \"í\": [\"is\", \"i1\"], \"ì\": [\"if\", \"i2\"], \"ỉ\": [\"ir\", \"i3\"], \"ị\": [\"ij\", \"i5\"],\n                     \"ĩ\": [\"ix\", \"i4\"], \"ý\": [\"ys\", \"y1\"], \"ỳ\": [\"yf\", \"y2\"], \"ỷ\": [\"yr\", \"y3\"], \"ỵ\": [\"yj\", \"y5\"],\n                     \"đ\": [\"dd\", \"d9\"],\n                     \"Ă\": [\"Aw\", \"A8\"], \"Â\": [\"Aa\", \"A6\"], \"Á\": [\"As\", \"A1\"], \"À\": [\"Af\", \"A2\"], \"Ả\": [\"Ar\", \"A3\"],\n                     \"Ã\": [\"Ax\", \"A4\"], \"Ạ\": [\"Aj\", \"A5\"], \"Ắ\": [\"Aws\", \"Ă1\"], \"Ổ\": [\"Oor\", \"Ô3\"], \"Ỗ\": [\"Oox\", \"Ô4\"],\n                     \"Ộ\": [\"Ooj\", \"Ô5\"], \"Ơ\": [\"Ow\", \"O7\"],\n                     \"Ằ\": [\"AWF\", \"Ă2\"], \"Ẳ\": [\"Awr\", \"Ă3\"], \"Ẵ\": [\"Awx\", \"Ă4\"], \"Ặ\": [\"Awj\", \"Ă5\"], \"Ó\": [\"Os\", \"O1\"],\n                     \"Ò\": [\"Of\", \"O2\"], \"Ỏ\": [\"Or\", \"O3\"], \"Õ\": [\"Ox\", \"O4\"], \"Ọ\": [\"Oj\", \"O5\"], \"Ô\": [\"Oo\", \"O6\"],\n                     \"Ố\": [\"Oos\", \"Ô1\"], \"Ồ\": [\"Oof\", \"Ô2\"],\n                     \"Ớ\": [\"Ows\", \"Ơ1\"], \"Ờ\": [\"Owf\", \"Ơ2\"], \"Ở\": [\"Owr\", \"Ơ3\"], \"Ỡ\": [\"Owx\", \"Ơ4\"], \"Ợ\": [\"Owj\", \"Ơ5\"],\n                     \"É\": [\"Es\", \"E1\"], \"È\": [\"Ef\", \"E2\"], \"Ẻ\": [\"Er\", \"E3\"], \"Ẽ\": [\"Ex\", \"E4\"], \"Ẹ\": [\"Ej\", \"E5\"],\n                     \"Ê\": [\"Ee\", \"E6\"], \"Ế\": [\"Ees\", \"Ê1\"], \"Ề\": [\"Eef\", \"Ê2\"],\n                     \"Ể\": [\"Eer\", \"Ê3\"], \"Ễ\": [\"Eex\", \"Ê4\"], \"Ệ\": [\"Eej\", \"Ê5\"], \"Ú\": [\"Us\", \"U1\"], \"Ù\": [\"Uf\", \"U2\"],\n                     \"Ủ\": [\"Ur\", \"U3\"], \"Ũ\": [\"Ux\", \"U4\"], \"Ụ\": [\"Uj\", \"U5\"], \"Ư\": [\"Uw\", \"U7\"], \"Ứ\": [\"Uws\", \"Ư1\"],\n                     \"Ừ\": [\"Uwf\", \"Ư2\"], \"Ử\": [\"Uwr\", \"Ư3\"], \"Ữ\": [\"Uwx\", \"Ư4\"],\n                     \"Ự\": [\"Uwj\", \"Ư5\"], \"Í\": [\"Is\", \"I1\"], \"Ì\": [\"If\", \"I2\"], \"Ỉ\": [\"Ir\", \"I3\"], \"Ị\": [\"Ij\", \"I5\"],\n                     \"Ĩ\": [\"Ix\", \"I4\"], \"Ý\": [\"Ys\", \"Y1\"], \"Ỳ\": [\"Yf\", \"Y2\"], \"Ỷ\": [\"Yr\", \"Y3\"], \"Ỵ\": [\"Yj\", \"Y5\"],\n                     \"Đ\": [\"Dd\", \"D9\"]}\n        self.all_word_candidates = self.get_all_word_candidates(self.word_couples)\n        self.string_all_word_candidates = ' '.join(self.all_word_candidates)\n        self.all_char_candidates = self.get_all_char_candidates()\n        self.keyboardNeighbors = self.getKeyboardNeighbors()\n\n    def replace_teencode(self, word):\n        candidates = self.teencode_dict.get(word, None)\n        if candidates is not None:\n            chosen_one = 0\n            if len(candidates) > 1:\n                chosen_one = np.random.randint(0, len(candidates))\n            return candidates[chosen_one]\n\n    def getKeyboardNeighbors(self):\n        keyboardNeighbors = {}\n        keyboardNeighbors['a'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ă'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['â'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['á'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['à'] = \"aáàảãăắằẳẵâấầẩẫ\"\n        keyboardNeighbors['ả'] = \"aảã\"\n        keyboardNeighbors['ã'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ạ'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ắ'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ằ'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ẳ'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ặ'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ẵ'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ấ'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ầ'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ẩ'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ẫ'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['ậ'] = \"aáàảãạăắằẳẵặâấầẩẫậ\"\n        keyboardNeighbors['b'] = \"bh\"\n        keyboardNeighbors['c'] = \"cgn\"\n        keyboardNeighbors['d'] = \"đctơở\"\n        keyboardNeighbors['đ'] = \"d\"\n        keyboardNeighbors['e'] = \"eéèẻẽẹêếềểễệbpg\"\n        keyboardNeighbors['é'] = \"eéèẻẽẹêếềểễệ\"\n        keyboardNeighbors['è'] = \"eéèẻẽẹêếềểễệ\"\n        keyboardNeighbors['ẻ'] = \"eéèẻẽẹêếềểễệ\"\n        keyboardNeighbors['ẽ'] = \"eéèẻẽẹêếềểễệ\"\n        keyboardNeighbors['ẹ'] = \"eéèẻẽẹêếềểễệ\"\n        keyboardNeighbors['ê'] = \"eéèẻẽẹêếềểễệá\"\n        keyboardNeighbors['ế'] = \"eéèẻẽẹêếềểễệố\"\n        keyboardNeighbors['ề'] = \"eéèẻẽẹêếềểễệ\"\n        keyboardNeighbors['ể'] = \"eéèẻẽẹêếềểễệôốồổỗộ\"\n        keyboardNeighbors['ễ'] = \"eéèẻẽẹêếềểễệ\"\n        keyboardNeighbors['ệ'] = \"eéèẻẽẹêếềểễệ\"\n        keyboardNeighbors['g'] = \"qgộ\"\n        keyboardNeighbors['h'] = \"h\"\n        keyboardNeighbors['i'] = \"iíìỉĩịat\"\n        keyboardNeighbors['í'] = \"iíìỉĩị\"\n        keyboardNeighbors['ì'] = \"iíìỉĩị\"\n        keyboardNeighbors['ỉ'] = \"iíìỉĩị\"\n        keyboardNeighbors['ĩ'] = \"iíìỉĩị\"\n        keyboardNeighbors['ị'] = \"iíìỉĩịhự\"\n        keyboardNeighbors['k'] = \"klh\"\n        keyboardNeighbors['l'] = \"ljidđ\"\n        keyboardNeighbors['m'] = \"mn\"\n        keyboardNeighbors['n'] = \"mnedư\"\n        keyboardNeighbors['o'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ó'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ò'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ỏ'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['õ'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ọ'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ô'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ố'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ồ'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ổ'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ộ'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ỗ'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ơ'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ớ'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ờ'] = \"oóòỏọõôốồổỗộơớờởợỡà\"\n        keyboardNeighbors['ở'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ợ'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        keyboardNeighbors['ỡ'] = \"oóòỏọõôốồổỗộơớờởợỡ\"\n        # keyboardNeighbors['p'] = \"op\"\n        # keyboardNeighbors['q'] = \"qọ\"\n        # keyboardNeighbors['r'] = \"rht\"\n        # keyboardNeighbors['s'] = \"s\"\n        # keyboardNeighbors['t'] = \"tp\"\n        keyboardNeighbors['u'] = \"uúùủũụưứừữửựhiaạt\"\n        keyboardNeighbors['ú'] = \"uúùủũụưứừữửự\"\n        keyboardNeighbors['ù'] = \"uúùủũụưứừữửự\"\n        keyboardNeighbors['ủ'] = \"uúùủũụưứừữửự\"\n        keyboardNeighbors['ũ'] = \"uúùủũụưứừữửự\"\n        keyboardNeighbors['ụ'] = \"uúùủũụưứừữửự\"\n        keyboardNeighbors['ư'] = \"uúùủũụưứừữửựg\"\n        keyboardNeighbors['ứ'] = \"uúùủũụưứừữửự\"\n        keyboardNeighbors['ừ'] = \"uúùủũụưứừữửự\"\n        keyboardNeighbors['ử'] = \"uúùủũụưứừữửự\"\n        keyboardNeighbors['ữ'] = \"uúùủũụưứừữửự\"\n        keyboardNeighbors['ự'] = \"uúùủũụưứừữửựg\"\n        keyboardNeighbors['v'] = \"v\"\n        keyboardNeighbors['x'] = \"x\"\n        keyboardNeighbors['y'] = \"yýỳỷỵỹụ\"\n        keyboardNeighbors['ý'] = \"yýỳỷỵỹ\"\n        keyboardNeighbors['ỳ'] = \"yýỳỷỵỹ\"\n        keyboardNeighbors['ỷ'] = \"yýỳỷỵỹ\"\n        keyboardNeighbors['ỵ'] = \"yýỳỷỵỹ\"\n        keyboardNeighbors['ỹ'] = \"yýỳỷỵỹ\"\n        # keyboardNeighbors['w'] = \"wv\"\n        # keyboardNeighbors['j'] = \"jli\"\n        # keyboardNeighbors['z'] = \"zs\"\n        # keyboardNeighbors['f'] = \"ft\"\n\n        return keyboardNeighbors\n\n    def replace_char_noaccent(self, text, onehot_label):\n\n        # find index noise\n        idx = np.random.randint(0, len(onehot_label))\n        prevent_loop = 0\n        while onehot_label[idx] == 1 or text[idx].isnumeric() or text[idx] in string.punctuation:\n            idx = np.random.randint(0, len(onehot_label))\n            prevent_loop += 1\n            if prevent_loop > 10:\n                return False, text, onehot_label\n\n        index_noise = idx\n        onehot_label[index_noise] = 1\n        word_noise = text[index_noise]\n        for id in range(0, len(word_noise)):\n            char = word_noise[id]\n\n            if char in self.keyboardNeighbors:\n                neighbors = self.keyboardNeighbors[char]\n                idx_neigh = np.random.randint(0, len(neighbors))\n                replaced = neighbors[idx_neigh]\n                word_noise = word_noise[: id] + replaced + word_noise[id + 1:]\n                text[index_noise] = word_noise\n                return True, text, onehot_label\n\n        return False, text, onehot_label\n\n    def replace_word_candidate(self, word):\n        \"\"\"\n        Return a homophone word of the input word.\n        \"\"\"\n        capital_flag = word[0].isupper()\n        word = word.lower()\n        if capital_flag and word in self.teencode_dict:\n            return self.replace_teencode(word).capitalize()\n        elif word in self.teencode_dict:\n            return self.replace_teencode(word)\n\n        for couple in self.word_couples:\n            for i in range(2):\n                if couple[i] == word:\n                    if i == 0:\n                        if capital_flag:\n                            return couple[1].capitalize()\n                        else:\n                            return couple[1]\n                    else:\n                        if capital_flag:\n                            return couple[0].capitalize()\n                        else:\n                            return couple[0]\n\n    def replace_char_candidate(self, char):\n        \"\"\"\n        return a homophone char/subword of the input char.\n        \"\"\"\n        for couple in self.char_couples:\n            for i in range(2):\n                if couple[i] == char:\n                    if i == 0:\n                        return couple[1]\n                    else:\n                        return couple[0]\n\n    def replace_char_candidate_typo(self, char):\n        \"\"\"\n        return a homophone char/subword of the input char.\n        \"\"\"\n        i = np.random.randint(0, 2)\n\n        return self.typo[char][i]\n\n    def get_all_char_candidates(self, ):\n\n        all_char_candidates = []\n        for couple in self.char_couples:\n            all_char_candidates.extend(couple)\n        return all_char_candidates\n\n    def get_all_word_candidates(self, word_couples):\n\n        all_word_candidates = []\n        for couple in self.word_couples:\n            all_word_candidates.extend(couple)\n        return all_word_candidates\n\n    def remove_diacritics(self, text, onehot_label):\n        \"\"\"\n        Replace word which has diacritics with the same word without diacritics\n        Args:\n            text: a list of word tokens\n            onehot_label: onehot array indicate position of word that has already modify, so this\n            function only choose the word that do not has onehot label == 1.\n        return: a list of word tokens has one word that its diacritics was removed,\n                a list of onehot label indicate the position of words that has been modified.\n        \"\"\"\n        idx = np.random.randint(0, len(onehot_label))\n        prevent_loop = 0\n        while onehot_label[idx] == 1 or text[idx] == unidecode.unidecode(text[idx]) or text[idx] in string.punctuation:\n            idx = np.random.randint(0, len(onehot_label))\n            prevent_loop += 1\n            if prevent_loop > 10:\n                return False, text, onehot_label\n\n        onehot_label[idx] = 1\n        text[idx] = unidecode.unidecode(text[idx])\n        return True, text, onehot_label\n\n    def replace_with_random_letter(self, text, onehot_label):\n        \"\"\"\n        Replace, add (or remove) a random letter in a random chosen word with a random letter\n        Args:\n            text: a list of word tokens\n            onehot_label: onehot array indicate position of word that has already modify, so this\n            function only choose the word that do not has onehot label == 1.\n        return: a list of word tokens has one word that has been modified,\n                a list of onehot label indicate the position of words that has been modified.\n        \"\"\"\n        idx = np.random.randint(0, len(onehot_label))\n        prevent_loop = 0\n        while onehot_label[idx] == 1 or text[idx].isnumeric() or text[idx] in string.punctuation:\n            idx = np.random.randint(0, len(onehot_label))\n            prevent_loop += 1\n            if prevent_loop > 10:\n                return False, text, onehot_label\n\n        # replace, add or remove? 0 is replace, 1 is add, 2 is remove\n        coin = np.random.choice([0, 1, 2])\n        if coin == 0:\n            chosen_letter = text[idx][np.random.randint(0, len(text[idx]))]\n            replaced = self.vn_alphabet[np.random.randint(0, self.alphabet_len)]\n            try:\n                text[idx] = re.sub(chosen_letter, replaced, text[idx])\n            except:\n                return False, text, onehot_label\n        elif coin == 1:\n            chosen_letter = text[idx][np.random.randint(0, len(text[idx]))]\n            replaced = chosen_letter + self.vn_alphabet[np.random.randint(0, self.alphabet_len)]\n            try:\n                text[idx] = re.sub(chosen_letter, replaced, text[idx])\n            except:\n                return False, text, onehot_label\n        else:\n            chosen_letter = text[idx][np.random.randint(0, len(text[idx]))]\n            try:\n                text[idx] = re.sub(chosen_letter, '', text[idx])\n            except:\n                return False, text, onehot_label\n\n        onehot_label[idx] = 1\n        return True, text, onehot_label\n\n    def replace_with_homophone_word(self, text, onehot_label):\n        \"\"\"\n        Replace a candidate word (if exist in the word_couple) with its homophone. if successful, return True, else False\n        Args:\n            text: a list of word tokens\n            onehot_label: onehot array indicate position of word that has already modify, so this\n            function only choose the word that do not has onehot label == 1.\n        return: True, text, onehot_label if successful replace, else False, text, onehot_label\n        \"\"\"\n        # account for the case that the word in the text is upper case but its lowercase match the candidates list\n        candidates = []\n        for i in range(len(text)):\n            if text[i].lower() in self.all_word_candidates or text[i].lower() in self.teencode_dict.keys():\n                candidates.append((i, text[i]))\n\n        if len(candidates) == 0:\n            return False, text, onehot_label\n\n        idx = np.random.randint(0, len(candidates))\n        prevent_loop = 0\n        while onehot_label[candidates[idx][0]] == 1:\n            idx = np.random.choice(np.arange(0, len(candidates)))\n            prevent_loop += 1\n            if prevent_loop > 5:\n                return False, text, onehot_label\n\n        text[candidates[idx][0]] = self.replace_word_candidate(candidates[idx][1])\n        onehot_label[candidates[idx][0]] = 1\n        return True, text, onehot_label\n\n    def replace_with_homophone_letter(self, text, onehot_label):\n        \"\"\"\n        Replace a subword/letter with its homophones\n        Args:\n            text: a list of word tokens\n            onehot_label: onehot array indicate position of word that has already modify, so this\n            function only choose the word that do not has onehot label == 1.\n        return: True, text, onehot_label if successful replace, else False, None, None\n        \"\"\"\n        candidates = []\n        for i in range(len(text)):\n            for char in self.all_char_candidates:\n                if re.search(char, text[i]) is not None:\n                    candidates.append((i, char))\n                    break\n\n        if len(candidates) == 0:\n\n            return False, text, onehot_label\n        else:\n            idx = np.random.randint(0, len(candidates))\n            prevent_loop = 0\n            while onehot_label[candidates[idx][0]] == 1:\n                idx = np.random.randint(0, len(candidates))\n                prevent_loop += 1\n                if prevent_loop > 5:\n                    return False, text, onehot_label\n\n            replaced = self.replace_char_candidate(candidates[idx][1])\n            text[candidates[idx][0]] = re.sub(candidates[idx][1], replaced, text[candidates[idx][0]])\n\n            onehot_label[candidates[idx][0]] = 1\n            return True, text, onehot_label\n\n    def replace_with_typo_letter(self, text, onehot_label):\n        \"\"\"\n        Replace a subword/letter with its homophones\n        Args:\n            text: a list of word tokens\n            onehot_label: onehot array indicate position of word that has already modify, so this\n            function only choose the word that do not has onehot label == 1.\n        return: True, text, onehot_label if successful replace, else False, None, None\n        \"\"\"\n        # find index noise\n        idx = np.random.randint(0, len(onehot_label))\n        prevent_loop = 0\n        while onehot_label[idx] == 1 or text[idx].isnumeric() or text[idx] in string.punctuation:\n            idx = np.random.randint(0, len(onehot_label))\n            prevent_loop += 1\n            if prevent_loop > 10:\n                return False, text, onehot_label\n\n        index_noise = idx\n        onehot_label[index_noise] = 1\n\n        word_noise = text[index_noise]\n        for j in range(0, len(word_noise)):\n            char = word_noise[j]\n\n            if char in self.typo:\n                replaced = self.replace_char_candidate_typo(char)\n                word_noise = word_noise[: j] + replaced + word_noise[j + 1:]\n                text[index_noise] = word_noise\n                return True, text, onehot_label\n        return True, text, onehot_label\n\n    def add_noise(self, sentence, percent_err=0.15, num_type_err=5):\n        tokens = self.tokenizer(sentence)\n        onehot_label = [0] * len(tokens)\n\n        num_wrong = int(np.ceil(percent_err * len(tokens)))\n        num_wrong = np.random.randint(1, num_wrong + 1)\n        if np.random.rand() < 0.05:\n            num_wrong = 0\n\n        for i in range(0, num_wrong):\n            err = np.random.randint(0, num_type_err + 1)\n\n            if err == 0:\n                _, tokens, onehot_label = self.replace_with_homophone_letter(tokens, onehot_label)\n            elif err == 1:\n                _, tokens, onehot_label = self.replace_with_typo_letter(tokens, onehot_label)\n            elif err == 2:\n                _, tokens, onehot_label = self.replace_with_homophone_word(tokens, onehot_label)\n            elif err == 3:\n                _, tokens, onehot_label = self.replace_with_random_letter(tokens, onehot_label)\n            elif err == 4:\n                _, tokens, onehot_label = self.remove_diacritics(tokens, onehot_label)\n            elif err == 5:\n                _, tokens, onehot_label = self.replace_char_noaccent(tokens, onehot_label)\n            else:\n                continue\n        return ' '.join(tokens), onehot_label","metadata":{"papermill":{"duration":0.156022,"end_time":"2022-01-11T07:29:58.586555","exception":false,"start_time":"2022-01-11T07:29:58.430533","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-18T13:13:46.315639Z","iopub.execute_input":"2022-01-18T13:13:46.316506Z","iopub.status.idle":"2022-01-18T13:13:46.483623Z","shell.execute_reply.started":"2022-01-18T13:13:46.316462Z","shell.execute_reply":"2022-01-18T13:13:46.482986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"synthesizer = SynthesizeData()\n","metadata":{"papermill":{"duration":0.046834,"end_time":"2022-01-11T07:29:58.673372","exception":false,"start_time":"2022-01-11T07:29:58.626538","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-18T13:13:46.485139Z","iopub.execute_input":"2022-01-18T13:13:46.485522Z","iopub.status.idle":"2022-01-18T13:13:46.489043Z","shell.execute_reply.started":"2022-01-18T13:13:46.485486Z","shell.execute_reply":"2022-01-18T13:13:46.488335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{"papermill":{"duration":0.038824,"end_time":"2022-01-11T07:29:58.752656","exception":false,"start_time":"2022-01-11T07:29:58.713832","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Params CONFIG\n","metadata":{"papermill":{"duration":0.038502,"end_time":"2022-01-11T07:29:58.919357","exception":false,"start_time":"2022-01-11T07:29:58.880855","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class Vocab(object):\n    def __init__(self):\n        self.word2idx = {}\n        self.idx2word ={}\n        self.idx = 0\n        self.go = 1\n        self.eos = 2\n\n    def add_word(self, word):\n        if not word in self.word2idx:\n            self.word2idx[word] = self.idx\n            self.idx2word[self.idx] = word\n            self.idx +=1\n\n    def __len__(self):\n        return len(self.word2idx)\n    \n    def encode(self, chars):\n        encode_sent = []\n        encode_sent.append(self.go)\n        for word in word_tokenize(chars):\n            if not word in self.word2idx:\n                encode_sent.append(self.word2idx['<unk>'])\n            else:\n                encode_sent.append(self.word2idx[word])\n        encode_sent.append(self.eos)        \n        while len(encode_sent)<10:\n            encode_sent.append(0)\n        \n        return encode_sent\n\n\n    def __call__(self, word):\n        if not word in self.word2idx:\n            return self.word2idx['<unk>']\n        return self.word2idx[word]","metadata":{"execution":{"iopub.status.busy":"2022-01-18T13:13:46.490469Z","iopub.execute_input":"2022-01-18T13:13:46.490942Z","iopub.status.idle":"2022-01-18T13:13:46.502462Z","shell.execute_reply.started":"2022-01-18T13:13:46.490906Z","shell.execute_reply":"2022-01-18T13:13:46.501877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_captions = np.load('../input/5gram-nlp-project/train_normal_captions.npy')\nvalid_captions = np.load('../input/5gram-nlp-project/valid_normal_captions.npy')\ncaptions = []\nonehot_labels = []\ndef build_vocab(all_clean_captions):\n    counter = Counter()\n    for caption in tqdm(all_clean_captions):\n        captions.append(caption)\n        tokens = word_tokenize(caption)\n        counter.update(tokens)\n\n    words = [word for word, cnt in counter.items() if cnt>=2]\n    vocab = Vocab()\n    vocab.add_word('<pad>') # 0\n    vocab.add_word('<start>') # 1\n    vocab.add_word('<end>') # 2\n    vocab.add_word('<unk>') # 3\n    for i, word in enumerate(words):\n        vocab.add_word(word)\n    return vocab\n\nvocab = build_vocab(train_captions)\nprint(f\"Length of vocab {len(vocab)}\")","metadata":{"execution":{"iopub.status.busy":"2022-01-18T13:13:46.504798Z","iopub.execute_input":"2022-01-18T13:13:46.505339Z","iopub.status.idle":"2022-01-18T13:15:39.762877Z","shell.execute_reply.started":"2022-01-18T13:13:46.505301Z","shell.execute_reply":"2022-01-18T13:15:39.762054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_onehot_labels = np.load('../input/5gram-nlp-project/train_onehot_labels.npy', allow_pickle=True)\nvalid_onehot_labels = np.load('../input/5gram-nlp-project/valid_onehot_labels.npy', allow_pickle=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T13:15:39.76433Z","iopub.execute_input":"2022-01-18T13:15:39.764611Z","iopub.status.idle":"2022-01-18T13:15:41.471561Z","shell.execute_reply.started":"2022-01-18T13:15:39.764574Z","shell.execute_reply":"2022-01-18T13:15:41.47075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(onehot_labels[0:20])","metadata":{"execution":{"iopub.status.busy":"2022-01-18T13:15:41.472726Z","iopub.execute_input":"2022-01-18T13:15:41.472987Z","iopub.status.idle":"2022-01-18T13:15:41.477321Z","shell.execute_reply.started":"2022-01-18T13:15:41.472952Z","shell.execute_reply":"2022-01-18T13:15:41.476601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_captions[0:20])","metadata":{"execution":{"iopub.status.busy":"2022-01-18T13:15:41.478737Z","iopub.execute_input":"2022-01-18T13:15:41.479428Z","iopub.status.idle":"2022-01-18T13:15:41.491458Z","shell.execute_reply.started":"2022-01-18T13:15:41.479389Z","shell.execute_reply":"2022-01-18T13:15:41.490602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(valid_onehot_labels[0:20])","metadata":{"execution":{"iopub.status.busy":"2022-01-18T13:15:41.492913Z","iopub.execute_input":"2022-01-18T13:15:41.493583Z","iopub.status.idle":"2022-01-18T13:15:41.500203Z","shell.execute_reply.started":"2022-01-18T13:15:41.493545Z","shell.execute_reply":"2022-01-18T13:15:41.499341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(valid_captions[0:20])","metadata":{"execution":{"iopub.status.busy":"2022-01-18T13:15:41.501883Z","iopub.execute_input":"2022-01-18T13:15:41.502325Z","iopub.status.idle":"2022-01-18T13:15:41.508978Z","shell.execute_reply.started":"2022-01-18T13:15:41.502273Z","shell.execute_reply":"2022-01-18T13:15:41.50802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SpellingDataset(Dataset):\n    def __init__(self, list_ngram, onehot_labels,  src_vocab):\n        self.list_ngram = list_ngram\n        self.onehot_labels = onehot_labels\n        self.src_vocab = src_vocab\n    \n    def __getitem__(self, index):\n        train_text = self.list_ngram[index]\n        train_oh_label = self.onehot_labels[index]\n#         print(train_text)\n#         print(train_oh_label)\n        train_oh_label.insert(0, 0)\n        while len(train_oh_label) < 10:\n            train_oh_label.append(0)\n        if len(train_oh_label)>10:\n            train_oh_label = train_oh_label[:10]\n#         print(\"After\",train_oh_label)\n#         print(len(train_oh_label))\n        \n        \n        train_text = self.src_vocab.encode(train_text)\n        tensor_text = torch.tensor(train_text, dtype = torch.long)\n        train_oh_label = torch.tensor(train_oh_label, dtype = torch.long)\n        return tensor_text, train_oh_label\n        \n        \n    \n    def __len__(self):\n        return len(self.list_ngram)\n\nds_train = SpellingDataset(train_captions, train_onehot_labels, vocab)\nds_valid = SpellingDataset(valid_captions, valid_onehot_labels, vocab)\n# ds_test = SpellingDataset(list_ngram_test, synthesizer, vocab, MAXLEN)\ntrain_loader = DataLoader(ds_train, batch_size = 512, shuffle=True)\nval_loader = DataLoader(ds_valid, batch_size = 1)\n# test_loader = DataLoader(ds_test, batch_size = 200)\n# print(len(train_loader))\ntext, train_oh_label= next(iter(train_loader))\nvalid_text, valid_oh_label= next(iter(val_loader))\nprint(text[0])\nprint(text.size())\nprint(text)\nprint(train_oh_label.size())\nprint(train_oh_label[0])\nprint(train_oh_label[1])\nprint(train_oh_label[2])\nprint(valid_text.size(), valid_oh_label.size())","metadata":{"papermill":{"duration":2.697167,"end_time":"2022-01-11T07:30:02.114802","exception":false,"start_time":"2022-01-11T07:29:59.417635","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-18T13:15:41.512075Z","iopub.execute_input":"2022-01-18T13:15:41.512637Z","iopub.status.idle":"2022-01-18T13:15:41.889752Z","shell.execute_reply.started":"2022-01-18T13:15:41.51259Z","shell.execute_reply":"2022-01-18T13:15:41.889044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PositionEncoder(nn.Module):\n    def __init__(self, max_len, emb_size, dropout=0.1):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout)\n\n        pe = torch.zeros(max_len, emb_size)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, emb_size, 2).float() * (-math.log(10000.0) / emb_size))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)\n\nclass TransformerEncoder(nn.Module):\n    def __init__(self, input_size, emb_size, hidden_size, num_layer, max_len=7):\n        super().__init__()\n        self.emb_size = emb_size\n        self.hidden_size = hidden_size\n        self.num_layer = num_layer\n        self.scale = math.sqrt(emb_size)\n\n        self.embedding = nn.Embedding(input_size, emb_size)\n        # additional length for sos and eos\n        self.pos_encoder = PositionEncoder(max_len + 1, emb_size)\n        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_size, nhead=8,\n                                                   dim_feedforward=hidden_size,\n                                                   dropout=0.1, activation='gelu')\n        encoder_norm = nn.LayerNorm(emb_size)\n        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layer, norm=encoder_norm)\n\n    def forward(self, src):\n        src = self.embedding(src) * self.scale\n        src = self.pos_encoder(src)\n        output = self.encoder(src)\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-01-18T13:15:41.891139Z","iopub.execute_input":"2022-01-18T13:15:41.891423Z","iopub.status.idle":"2022-01-18T13:15:41.902777Z","shell.execute_reply.started":"2022-01-18T13:15:41.891386Z","shell.execute_reply":"2022-01-18T13:15:41.902116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TransformerModel(nn.Module):\n    def __init__(self, input_size, emb_size, hidden_size, output_size,\n                 num_layer, max_len, pad_token, sos_token, eos_token):\n        super().__init__()\n        self.encoder = TransformerEncoder(input_size, emb_size, hidden_size, num_layer, max_len)\n        self.fc = nn.Linear(emb_size, output_size)\n        self.pad_token = pad_token\n        self.sigmoid = nn.Sigmoid()\n    \n    @staticmethod\n    def generate_mask(src, pad_token):\n        '''\n        Generate mask for tensor src\n        :param src: tensor with shape (max_src, b)\n        :param pad_token: padding token\n        :return: mask with shape (b, max_src) where pad_token is masked with 1\n        '''\n        mask = (src.t() == pad_token)\n        return mask.to(src.device)\n    \n    def forward(self, src):\n        src_mask = self.generate_mask(src, self.pad_token)\n        enc_output = self.encoder(src)\n        enc_output = self.fc(enc_output)\n        output = enc_output.transpose(0,1)\n        output = self.sigmoid(output)\n        \n        return output","metadata":{"execution":{"iopub.status.busy":"2022-01-18T13:15:41.90403Z","iopub.execute_input":"2022-01-18T13:15:41.904444Z","iopub.status.idle":"2022-01-18T13:15:41.914829Z","shell.execute_reply.started":"2022-01-18T13:15:41.904408Z","shell.execute_reply":"2022-01-18T13:15:41.914111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n        \ndef asMinutes(s):\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\n\ndef timeSince(since, percent):\n    now = time.time()\n    s = now - since\n    es = s / (percent)\n    rs = es - s\n    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))","metadata":{"papermill":{"duration":0.054743,"end_time":"2022-01-11T07:30:03.08739","exception":false,"start_time":"2022-01-11T07:30:03.032647","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-18T13:15:41.916039Z","iopub.execute_input":"2022-01-18T13:15:41.916309Z","iopub.status.idle":"2022-01-18T13:15:41.927086Z","shell.execute_reply.started":"2022-01-18T13:15:41.916275Z","shell.execute_reply":"2022-01-18T13:15:41.926423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Trainer(object):\n    def __init__(self, model, device, config):\n        self.config = config\n        self.start_epoch = 1\n        self.model = model\n        self.device = device\n        self.model.to(self.device)\n        self.optimizer = AdamW(self.model.parameters(), lr = self.config.lr)\n        self.scheduler = torch.optim.lr_scheduler.OneCycleLR(self.optimizer, max_lr=0.001, steps_per_epoch=len(train_loader), epochs=2)\n        self.criterion = nn.BCELoss()\n        self.scaler = GradScaler()\n        self.base_dir = f'{config.folder}'\n        if not os.path.exists(self.base_dir):\n            os.makedirs(self.base_dir)\n        \n    def train_loop(self, train_resume=False):\n        for e in range(self.start_epoch, self.start_epoch+self.config.num_epochs):\n            t = time.time()\n            calc_loss = self.train_one_epoch(self.device)\n            self.config.epoch.append(e)\n            self.config.train_loss.append(calc_loss.avg)\n            print(f'Train. Epoch: {e}, Loss: {calc_loss.avg:.5f}, time: {(time.time() - t):.5f}')\n\n            t = time.time()\n            detector_preds, detector_labels = self.valid_one_epoch(self.device)\n#             print(detector_preds, detector_labels)\n            precision = precision_score(detector_labels,detector_preds , average = 'micro')\n            recall = recall_score(detector_labels,detector_preds , average = 'micro' )\n            f1 = f1_score(detector_labels,detector_preds, average = 'micro' )\n            acc = accuracy_score(detector_labels,detector_preds)\n            print(f\"Precision {precision}, recall {recall}, f1 {f1}, acc {acc}\") \n            state = {'epoch': e, 'state_dict': self.model.state_dict(),'optimizer': self.optimizer.state_dict(), \n                         'precision': precision, 'f1': f1, 'recall':recall, 'acc':acc}\n            self.save_model(state, f'model_detector_transformer_{e}.pth')\n            \n        \n        \n    def train_one_epoch(self, device):\n        self.model.train()\n        calc_loss = AverageMeter()\n        for batch in tqdm(train_loader):\n            batch = tuple(t.to(device) for t in batch)\n            text = batch[0]\n            train_oh_label = batch[1]\n            batch_size = text.size(0)\n            output = self.model(text.transpose(0,1))\n            output = output.squeeze(-1)\n            loss = self.criterion(output, train_oh_label.float())\n\n\n            loss_value = loss.item()\n\n            # Back prop\n            loss.backward()\n#             self.scaler.scale(loss).backward()\n\n            # Clip to avoid exploding gradient issues, makes sure grads are\n            # within a healthy range\n#             torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1)\n\n            # Gradient descent step\n            self.optimizer.step()\n            self.optimizer.zero_grad()\n#             self.scaler.step(self.optimizer)\n#             self.scaler.update()\n            calc_loss.update(loss_value, batch_size)\n    \n\n            \n        return calc_loss\n    \n    def valid_one_epoch(self, device):\n        self.model.eval()\n        preds = []\n\n        labels = []\n        calc_loss = AverageMeter()\n        with torch.no_grad():\n            for batch in tqdm(val_loader):\n                batch = tuple(t.to(device) for t in batch)\n                target = batch[0]\n                target_oh_labels = batch[1]\n                batch_size = target.size(0)\n#                 target = target.to(device)\n#                 target_oh_label = target_oh_label.to(device)\n                \n                output = self.model(target.transpose(0,1))\n#                 loss = self.criterion(output.squeeze(-1), target_oh_labels.float())\n#                 loss_value = loss.item()\n                pred = (output.detach().cpu().numpy()  > 0.5 ).astype(int).squeeze(-1)  \n                preds.extend(pred)\n                labels.extend(target_oh_labels.detach().cpu().numpy())\n\n#                 calc_loss.update(loss_value, batch_size)\n                \n        return preds, labels\n    \n    def accuracy_valid(self, predictions, targets):\n        n = len(predictions)\n        acc = 0\n    #     print(len(predictions), len(targets))\n        for i in range(len(predictions)):\n\n    #         print(f\"Predictions {predictions}\")\n    #         print(f\"Targets {targets}\")\n            if predictions[i]==targets[i]:\n                acc+=1\n        return acc/n\n    \n    \n    def save_model(self, state, path):\n        torch.save(state, path)\n    \n    def load_model(self, path):\n        checkpoint = torch.load(path)\n        self.start_epoch = checkpoint['epoch']+1\n        self.model.load_state_dict(checkpoint['state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer'])\n        self.config.train_loss = checkpoint['train_loss']\n        self.config.valid_acc = checkpoint['acc_valid']","metadata":{"papermill":{"duration":0.078445,"end_time":"2022-01-11T07:30:03.251864","exception":false,"start_time":"2022-01-11T07:30:03.173419","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-18T13:15:41.928505Z","iopub.execute_input":"2022-01-18T13:15:41.928787Z","iopub.status.idle":"2022-01-18T13:15:41.953106Z","shell.execute_reply.started":"2022-01-18T13:15:41.928755Z","shell.execute_reply":"2022-01-18T13:15:41.952419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GlobalParametersTrain:\n    lr =0.0001\n    src_vocab_size = len(vocab)\n    target_dim = 1\n    num_epochs = 1\n    model_dim = 512\n    feed_forward_dim = 2048\n    num_layers = 8\n    max_len = 40\n    PAD_IDX = 0\n    SOS_IDX = 1\n    EOS_IDX = 2 \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n    train_loss = []\n    valid_loss = []\n    valid_acc = []\n    epoch = []\n    \n    folder = './model'\n","metadata":{"papermill":{"duration":0.106169,"end_time":"2022-01-11T07:30:03.40005","exception":false,"start_time":"2022-01-11T07:30:03.293881","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-18T13:15:41.954307Z","iopub.execute_input":"2022-01-18T13:15:41.954738Z","iopub.status.idle":"2022-01-18T13:15:41.963538Z","shell.execute_reply.started":"2022-01-18T13:15:41.954703Z","shell.execute_reply":"2022-01-18T13:15:41.962824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = GlobalParametersTrain()\nmodel = TransformerModel(config.src_vocab_size, config.model_dim, config.feed_forward_dim, config.target_dim, config.num_layers,\n                            config.max_len, config.PAD_IDX, config.SOS_IDX, config.EOS_IDX).to(config.device)\ntraining = Trainer(model, device=config.device, config=config)\ntraining.train_loop(False)\n","metadata":{"papermill":{"duration":24599.122607,"end_time":"2022-01-11T14:20:02.563659","exception":false,"start_time":"2022-01-11T07:30:03.441052","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-18T13:15:41.964901Z","iopub.execute_input":"2022-01-18T13:15:41.965162Z","iopub.status.idle":"2022-01-18T13:24:06.360045Z","shell.execute_reply.started":"2022-01-18T13:15:41.965128Z","shell.execute_reply":"2022-01-18T13:24:06.359105Z"},"trusted":true},"execution_count":null,"outputs":[]}]}